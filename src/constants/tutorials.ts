export const mockTutorials = [
  {
    "id": "ff9e5d7d-005c-466c-8e32-b30f30a1eb5b",
    "title": "Preparar o ferramental e ambiente",
    "description": "Preparar o ambiente de desenvolvimento: instalação do Terraform, configuração do AWS CLI e organização da estrutura de arquivos do projeto.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/ff9e5d7d-005c-466c-8e32-b30f30a1eb5b",
    "markdown": "A Infraestrutura como Código (IaC) é o processo de gerenciar sua infraestrutura (servidores, redes, serviços em nuvem etc.) usando configuração declarativa em vez de procedimentos manuais. O Terraform, criado pela HashiCorp, é uma das principais ferramentas de IaC disponíveis. Ele permite **construir, mudar e versionar** a infraestrutura de forma segura e eficiente, suportando desde componentes de baixo nível (como instâncias EC2, buckets S3, redes) até componentes de alto nível (como DNS e recursos de SaaS) em vários provedores de nuvem.\n\nAntes de começarmos a usar o Terraform, precisamos preparar o ambiente de trabalho com as seguintes etapas:\n\n- **1. Criar uma conta na AWS:** Se ainda não tiver, [crie uma conta AWS](/blog/3a60fae5-2431-4b18-8763-75815bfad331) (a maioria dos serviços oferece um free tier). Em ambientes de produção, é recomendável **não usar as credenciais root** da conta AWS para automação. Em vez disso, [crie um usuário IAM específico para o Terraform com as permissões necessárias](/blog/206b42e6-6209-46e2-a86f-b4a4a815d6df). Para simplificar este tutorial, você pode atribuir temporariamente permissões administrativas completas a esse usuário IAM (não é boa prática em cenários reais, mas facilita o aprendizado inicial).\n    \n- **2. Gerar credenciais de acesso (Access Key ID e Secret Access Key):** Após criar o usuário IAM, gere um par de chaves de acesso (Access Key ID e Secret Access Key) para ele. A Access Key ID funciona como um nome de usuário e a Secret Access Key como uma senha para acesso programático à AWS. **Importante:** Salve essas credenciais em local seguro, pois a chave secreta só é exibida no momento da criação e não poderá ser recuperada depois. A AWS permite no máximo duas chaves por usuário, e recomenda usar credenciais temporárias quando possível.\n    \n- **3. Instalar o AWS CLI (opcional, mas recomendado):** O AWS CLI facilita configurar credenciais e testar acesso. [Baixe e instale a AWS CLI (v2) para seu sistema operacional](/blog/3e3b870b-8fbb-4b1b-8998-6ab4e233b7f4). Em seguida, execute **aws configure** no terminal e insira a **Access Key ID**, **Secret Access Key**, região padrão (por exemplo, **us-east-1**) e formato de saída (por exemplo, **json**) quando solicitado. Isso salvará suas credenciais em **~/.aws/credentials** sob o perfil \"default\". O Terraform posteriormente poderá detectar essas credenciais automaticamente neste arquivo.\n    \n- **4. Criar um par de chaves SSH (para acesso a instâncias EC2):** Caso planeje criar instâncias EC2, será necessário um par de chaves SSH para conectá-las. No console AWS, navegue até _EC2 > Network & Security > Key Pairs_ e clique em **Create key pair**. Informe um nome (ex: \"terraform-key\") e salve o arquivo PEM baixado (este é a chave privada) em local seguro. Essa chave será usada para acesso SSH às máquinas e pelo Terraform para provisionamento remoto. _(Observação: você também pode usar um par de chaves existente caso já tenha um.)_\n    \n- **5. Instalar o Terraform CLI:** A HashiCorp distribui o [Terraform como um binário executável único](/blog/32a6aac2-1188-4448-b838-e33fd0908a92). Baixe a versão mais recente do Terraform no site oficial ([https://developer.hashicorp.com/terraform/install](https://developer.hashicorp.com/terraform/install)). Se estiver em **Windows**, baixe o zip, extraia o **terraform.exe** e coloque-o em uma pasta (por exemplo, **C:\\Terraform**). No **Linux ou macOS**, baixe o pacote, descompacte o binário e mova-o para um diretório do PATH do sistema (como **/usr/local/bin/**). Após isso, verifique a instalação abrindo um terminal e executando:\n    \n\n```bash\nterraform -version\n```\n\nEsse comando deve exibir a versão do Terraform instalada. Por exemplo:\n\n```plaintext\nTerraform v1.12.1\non linux_amd64\n```\n\nSe o comando for reconhecido (mesmo que sua versão seja diferente), o Terraform CLI está instalado corretamente. Se não, verifique se o diretório do binário está no PATH ou repita a instalação.\n\nAgora, com a ferramenta instalada e as credenciais configuradas, temos o ambiente pronto para criar nossa primeira infraestrutura com Terraform. Nos tutoriais seguintes, construiremos passo a passo uma configuração Terraform, usando a AWS como provedor de nuvem escolhido."
  },
  {
    "id": "723dc4aa-284e-4cb9-9fc5-bc2fb61fb68f",
    "title": "Funções Básicas",
    "description": "Introduzir a construção de recursos utilizando HCL (HashiCorp Configuration Language). Vamos criar uma VPC, subnets e uma instância EC2 como exemplo.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Fundamentos",
      "VPC",
      "Subnet",
      "EC2",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/723dc4aa-284e-4cb9-9fc5-bc2fb61fb68f",
    "markdown": "Com o ambiente configurado, vamos criar uma infraestrutura simples na AWS usando o Terraform, para entender suas funções básicas. Os **passos básicos** de uso do Terraform são: **escrever a configuração**, **inicializar os plugins/provedores**, **planejar as mudanças** e **aplicar (provisionar) a infraestrutura**. Vamos detalhar cada passo criando um pequeno exemplo de recurso AWS.\n\n**1. Escrever a configuração Terraform (provider e resource):**\n\nCrie uma pasta de projeto (por exemplo, **terraform-projeto1**) e dentro dele um arquivo chamado **main.tf**. Arquivos **.tf** contêm a configuração em HCL (HashiCorp Configuration Language). Vamos começar definindo o provedor AWS e um recurso simples (uma instância EC2) neste arquivo:\n\n```hcl\n# main.tf\n\n# Define o provedor AWS e a região onde os recursos serão criados\nprovider \"aws\" {\n  region = \"us-east-1\"        # Região AWS (ex: us-east-1)\n}\n\n# Recurso: Instância EC2\nresource \"aws_instance\" \"exemplo\" {\n  ami           = \"ami-0fc5d935ebf8bc3bc\"   # ID da imagem AMI para o servidor (Ubuntu 20.04 LTS us-east-1)\n  instance_type = \"t2.micro\"               # Tipo da instância (t2.micro é elegível ao free tier)\n  \n  key_name      = \"terraform-key\"          # Nome do par de chaves SSH para acesso (criado no passo anterior)\n  \n  tags = {\n    Name = \"ServidorExemplo\"              # Tag Name para identificar a instância no console AWS\n  }\n}\n```\n\n**Sobre o código acima:** No bloco **provider \"aws\"**, especificamos qual provedor de nuvem usar e configuramos opções como região. O Terraform usa _providers_ (provedores) para interagir com APIs de diferentes plataformas cloud. Para usar a AWS, utilizamos o provedor AWS, que sabe gerenciar recursos da AWS. A região **us-east-1** foi escolhida aqui apenas como exemplo; você pode ajustá-la para outra região AWS de preferência.\n\nNo bloco **resource \"aws_instance\" \"exemplo\"**, estamos declarando que desejamos criar um recurso do tipo AWS EC2 Instance. Cada recurso em Terraform é identificado pelo **tipo** e um **nome interno** (no caso, tipo **aws_instance** e nome **exemplo**). Esses identificadores servem para o Terraform mapear e referenciar o recurso. Dentro do recurso, definimos os **atributos** necessários:\n\n- **ami**: a Amazon Machine Image usada para instanciação. Aqui usamos uma AMI pública de Ubuntu Server 20.04 em us-east-1 (cada região tem IDs próprios; esta AMI ID é usada como exemplo e pode estar desatualizada no futuro).\n- **instance_type**: o tamanho da instância (t2.micro).\n- **key_name**: o nome do par de chaves para habilitar acesso SSH à instância. Certifique-se de usar o nome do Key Pair que você criou anteriormente na AWS.\n- **tags**: bloco opcional para etiquetar o recurso. Colocamos uma tag “Name” para identificar a instância pelo nome no console.\n\n_Observação:_ Não especificamos credenciais aqui no código. O provedor AWS irá procurar credenciais de forma padrão (primeiro em variáveis de ambiente **AWS_ACCESS_KEY_ID** e **AWS_SECRET_ACCESS_KEY**, depois no arquivo de credenciais **~/.aws/credentials** perfil default, etc.). Como configuramos o AWS CLI no Tutorial 1, o Terraform deverá encontrar as credenciais automaticamente no perfil _default_.\n\n> _Dica:_ Em projetos reais, é recomendado **fixar a versão do provedor** para evitar atualizações inesperadas. Isso pode ser feito com um bloco **terraform { required_providers { ... } }** especificando a fonte e versão do provedor AWS. Aqui omitimos para simplificar, usando a versão mais recente disponível no momento da inicialização.\n\n**2. Inicializar o projeto Terraform:**\n\nAbra um terminal na pasta do projeto (**terraform-projeto1**) e execute o comando abaixo para inicializar o diretório de trabalho:\n\n```bash\nterraform init\n```\n\nEsse comando realiza a **inicialização** do Terraform no diretório atual. Ele irá baixar o plugin do provedor AWS necessário (caso ainda não esteja em cache) e configurar o ambiente local. Você deverá ver mensagens indicando o download do provider AWS. O Terraform analisa a configuração em busca de referências a provedores e instala automaticamente os plugins necessários. Após o **init**, será criada uma pasta oculta **.terraform/** com os plugins baixados, e um arquivo **terraform.lock.hcl** com as versões usadas.\n\n**3. Revisar o plano de execução (terraform plan):**\n\nAntes de aplicar as mudanças, é uma boa prática visualizar o **plano de execução**. Execute:\n\n```bash\nterraform plan\n```\n\nO Terraform irá ler a configuração e comparar com o estado atual da infraestrutura (no primeiro uso, não há nada provisionado ainda). Como o recurso **aws_instance.exemplo** não existe na AWS, o plano mostrará que esse recurso será **criado**. A saída do **plan** lista as ações que serão tomadas sem efetuar mudanças reais ainda. Você deve ver algo como:\n\n```plaintext\nPlan: 1 to add, 0 to change, 0 to destroy.\n```\n\nIsso indica que 1 recurso será adicionado. O comando **terraform plan** permite revisar se a infraestrutura que será criada corresponde ao desejado, servindo como uma etapa de validação antes da execução real.\n\n**4. Aplicar a configuração (terraform apply):**\n\nEstando satisfeito com o plano, podemos provisionar de fato a infraestrutura:\n\n```bash\nterraform apply\n```\n\nO Terraform mostrará novamente o plano e perguntará **Do you want to perform these actions?** para confirmarmos. Responda **yes** para prosseguir. (Você pode adicionar **-auto-approve** para pular a confirmação, mas por segurança não faremos isso agora). O comando **apply** então **executa as ações propostas**, criando os recursos na AWS. Isso incluirá chamar a API da AWS para iniciar a instância EC2 com as características definidas.\n\nSe tudo ocorrer bem, após alguns segundos/minutos, a saída irá indicar **Apply complete! Resources: 1 added, 0 changed, 0 destroyed.**. Você também poderá ver informações sobre o recurso criado, como o ID da instância EC2, IP público, etc.\n\nAgora sua instância EC2 está criada na AWS. Você pode confirmar acessando o console AWS EC2 na região escolhida e verificando se há uma instância em execução com a tag Name \"ServidorExemplo\". Parabéns, você provisionou sua primeira infraestrutura com Terraform! 🎉\n\n**5. Limpar recursos (terraform destroy):**\n\nComo boa prática, vamos **destruir** o recurso criado para evitar custos desnecessários e manter o ambiente limpo. O Terraform facilita isso com:\n\n```bash\nterraform destroy\n```\n\nSemelhante ao apply, ele mostrará um plano (desta vez indicando que o recurso será destruído) e pedirá confirmação. Digite **yes** para confirmar. O Terraform então chamará a API da AWS para terminar a instância EC2. Ao final, a mensagem deve indicar que o recurso foi removido: **Destroy complete! Resources: 1 destroyed.**. O comando **terraform destroy** **deprovisiona todos os objetos gerenciados pela configuração Terraform**, ou seja, remove tudo que foi criado pelo apply, seguindo o estado conhecido. Use-o com cautela, especialmente em ambientes não-desejáveis de perder – ele é útil em dev/testes, mas em produção normalmente você gerenciaria remoções de forma mais controlada.\n\n> **Recapitulando os comandos principais:**\n> \n> - **terraform init** – inicializa o ambiente (baixar provedores, módulos).\n> - **terraform plan** – mostra as mudanças que serão feitas (fase de revisão).\n> - **terraform apply** – aplica as mudanças, provisionando ou atualizando a infra.\n> - **terraform destroy** – destrói os recursos provisionados pelo Terraform.\n> \n> Esses comandos formam o núcleo do workflow do Terraform. Em execução típica: você altera os arquivos **.tf** (config), roda **plan** para verificar e então **apply** para efetivar.\n\nCom esses passos básicos, aprendemos a preparar uma configuração Terraform e criar recursos na nuvem AWS de forma declarativa. No próximo tutorial, vamos tornar nossa configuração mais flexível usando **variáveis** de entrada e **outputs**."
  },
  {
    "id": "c0b2f47b-7dfb-440c-8276-8690d2a4a1bf",
    "title": "Variáveis e Outputs no Terraform",
    "description": "Parametrizar nossa infraestrutura usando variáveis de entrada e exibir resultados com outputs. Dessa forma, o código se torna mais flexível e reutilizável.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Fundamentos",
      "Variable",
      "Output",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/c0b2f47b-7dfb-440c-8276-8690d2a4a1bf",
    "markdown": "Ao criar configurações Terraform, é comum evitar valores fixos (hardcodes) para torná-las reutilizáveis e parametrizáveis. **Variáveis de entrada** permitem customizar aspectos da infraestrutura sem alterar o código, enquanto **outputs** expõem valores úteis da infraestrutura provisionada. Vamos adicionar variáveis e outputs ao nosso exemplo.\n\n**1. Definindo variáveis de entrada:**\n\nNo Terraform, variáveis de entrada são declaradas usando o bloco **variable**. Elas podem ter um tipo, um valor padrão, e uma descrição. Vamos modificar nossa configuração para usar variáveis:\n\n- Crie um arquivo separado para variáveis, por exemplo **variables.tf** (opcional, apenas organização). Dentro dele, declare:\n\n```hcl\n# variables.tf\n\nvariable \"regiao\" {\n  description = \"Região AWS onde criar os recursos\"\n  default     = \"us-east-1\"\n}\n\nvariable \"ami_id\" {\n  description = \"ID da imagem AMI para a instância EC2\"\n  default     = \"ami-0fc5d935ebf8bc3bc\"\n}\n\nvariable \"instancia_tipo\" {\n  description = \"Tipo da instância EC2\"\n  default     = \"t2.micro\"\n}\n\nvariable \"nome_chave\" {\n  description = \"Nome do par de chaves SSH para acesso à instância\"\n  default     = \"terraform-key\"\n}\n\nvariable \"nome_instancia\" {\n  description = \"Nome (Tag) da instância EC2\"\n  default     = \"ServidorExemplo\"\n}\n```\n\nAqui definimos 5 variáveis com valores padrão. O uso de **default** torna a variável **opcional**, pois caso nenhuma valor seja fornecido, o Terraform usa o default especificado. Por exemplo, definimos um AMI padrão e tipo de instância padrão. Em cenários reais, você poderia omitir o default de algumas variáveis importantes para forçar o usuário a fornecer (nesse caso, o Terraform pediria valor no **terraform apply** caso não fosse passado nenhum).\n\n- Atualize o arquivo **main.tf** para usar essas variáveis em vez de valores fixos:\n\n```hcl\n# main.tf (atualizado para usar variáveis)\n\nprovider \"aws\" {\n  region = var.regiao     # usa a variável \"regiao\"\n}\n\nresource \"aws_instance\" \"exemplo\" {\n  ami           = var.ami_id          # usa a variável \"ami_id\"\n  instance_type = var.instancia_tipo  # usa a variável \"instancia_tipo\"\n  key_name      = var.nome_chave      # usa a variável \"nome_chave\"\n\n  tags = {\n    Name = var.nome_instancia        # usa a variável \"nome_instancia\"\n  }\n}\n```\n\nNote o prefixo **var.** para referenciar o valor de uma variável dentro do código Terraform. Estamos interpolando as variáveis nas configurações. Essa abordagem torna nosso código mais genérico e fácil de reutilizar. Por exemplo, se quisermos lançar a instância em outra região ou com outro tipo, basta fornecer valores diferentes às variáveis, sem editar o código em si.\n\n> _Comparação com programação tradicional:_ Se você está familiarizado com funções, pode imaginar as **variáveis de entrada como parâmetros de função** e os **outputs como retorno**. Variáveis permitem passar argumentos para a “função” (módulo Terraform) sem precisar alterar seu interior.\n\n**2. Fornecendo valores para variáveis:**\n\nHá diversas maneiras de definir os valores de variáveis no Terraform:\n\n- **Defaults no código:** como fizemos acima, garante um valor padrão.\n- **Arquivo .tfvars:** você pode criar um arquivo (ex: **terraform.tfvars**) listando variáveis e valores, que o Terraform carrega automaticamente.\n- **Linha de comando:** usar a opção **-var \"nome=valor\"** no **terraform apply/plan** para setar diretamente.\n- **Variáveis de ambiente:** exportar **TF_VAR_nome** no shell para cada variável.\n\nPor agora, podemos confiar nos valores default. Se quiser testar diferentes valores, crie um arquivo **terraform.tfvars** no projeto com conteúdo, por exemplo:\n\n```hcl\nregiao = \"us-west-2\"\nnome_instancia = \"ServidorOeste\"\n```\n\nIsso mudaria a região para Oregon (us-west-2) e o nome da instância. Ao rodar **terraform apply**, o Terraform automaticamente lê esse arquivo e substitui nos defaults das variáveis correspondentes.\n\n**3. Declarando outputs:**\n\nOutputs são saídas que o Terraform pode exibir após o apply, ou que podem ser consumidos por outros módulos. Eles são úteis para mostrar informações importantes da infraestrutura provisionada (IP da máquina, URL gerada, ID de recurso, etc). Vamos adicionar um output para ilustrar:\n\nNo arquivo **outputs.tf** (pode criar um novo arquivo), declare, por exemplo:\n\n```hcl\n# outputs.tf\n\noutput \"ip_publico\" {\n  description = \"IP público da instância EC2 criada\"\n  value       = aws_instance.exemplo.public_ip\n}\n```\n\nExplicação:\n\n- O bloco **output** nomeia uma saída (**ip_publico**) e define seu valor. Aqui usamos a referência **aws_instance.exemplo.public_ip** – ou seja, estamos acessando o atributo **public_ip** do recurso que criamos. O Terraform, ao concluir o apply, capturará esse valor do estado e exibirá para nós.\n- Adicionamos uma descrição para clareza (opcional, mas útil).\n\nVocê pode adicionar múltiplos outputs se quiser. Por exemplo, poderíamos expor também o ID da instância ou o DNS público:\n\n```hcl\noutput \"id_instancia\" {\n  value = aws_instance.exemplo.id\n}\n```\n\n**4. Aplicar a configuração com variáveis e outputs:**\n\nSalve os arquivos modificados/novos e execute novamente o ciclo:\n\n```bash\nterraform init        # (não necessário se já feito antes e não se adicionou novos providers)\nterraform plan\nterraform apply\n```\n\nNo **plan**, agora o Terraform deve mostrar nenhuma mudança se você não alterou valores (pois já criamos a instância no tutorial 2). Porém, note que adicionamos outputs – e outputs não alteram infraestrutura, são apenas informativos. Assim, possivelmente o **plan** indicará **0 to add, 0 to change, 0 to destroy** (nenhum recurso mudou).\n\nApós um **terraform apply** (que reconhecerá que nada precisa ser criado/destroído), o Terraform exibirá os valores de output. Por exemplo, deverá aparecer algo como:\n\n```plaintext\nOutputs:\n\nid_instancia = \"i-0bb...123\"\nip_publico = \"3.85.XX.XX\"\n```\n\nAgora temos o IP público facilmente visível, sem precisar ir ao console AWS. O output também fica acessível via comando CLI **terraform output** a qualquer momento após um apply, ou pode ser utilizado por outros módulos Terraform se esta configuração fosse consumida como módulo. Em resumo, **outputs expõem informações da infraestrutura** para o usuário ou para outros componentes, de forma semelhante a valores de retorno de uma função.\n\n**5. Refinando tipos e validações (opcional):**\n\nNas variáveis, podemos exigir tipos específicos. Por padrão, se não definirmos **type**, qualquer tipo é aceito (string, number, bool, lista, etc.). No exemplo acima, todas as variáveis acabaram sendo strings. Poderíamos definir explicitamente, por exemplo:\n\n```hcl\nvariable \"instancia_tipo\" {\n  type        = string\n  default     = \"t2.micro\"\n  description = \"Tipo da instância EC2\"\n}\n```\n\nO Terraform suporta tipos complexos como **list(string)**, **map(number)**, **object({...})** etc., o que permite estruturas de dados mais ricas. Também podemos adicionar restrições (validation rules) e marcar variáveis como **sensitive** (para que valores não apareçam em logs).\n\nNo contexto deste tutorial, não aprofundaremos nessas opções, mas saiba que elas existem para tornar módulos robustos e seguros contra entradas inválidas.\n\n**Resumo:** As **variáveis de entrada** tornam o código Terraform flexível e reutilizável, evitando a necessidade de editar o código para cada contexto. Já os **outputs** fornecem um jeito organizado de obter dados resultantes da infraestrutura criada. Juntos, eles facilitam o uso de um mesmo conjunto de configurações em diferentes ambientes (dev, hml, prod, cada um com parâmetros diferentes) sem duplicação de código.\n\nNo próximo tutorial, entenderemos como o Terraform gerencia o **state (estado)** da infraestrutura e porque ele é crucial no funcionamento da ferramenta.\n"
  },
  {
    "id": "f82224ce-d909-4fea-9bcc-8a353b1b05f7",
    "title": "Terraform State (Estado do Terraform)",
    "description": "Compreender como o Terraform gerencia o estado dos recursos, a importância do arquivo de state e como trabalhar com backends remotos.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Backend",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/f82224ce-d909-4fea-9bcc-8a353b1b05f7",
    "markdown": "O **Terraform State** é o coração do funcionamento do Terraform. Toda vez que aplicamos uma configuração, o Terraform **armazena informações sobre os recursos provisionados** em um arquivo de estado. Esse arquivo atua como a fonte da verdade (_single source of truth_) que o Terraform usa para acompanhar o que foi criado e mapear para os recursos reais na nuvem.\n\nVamos entender o state por partes:\n\n**1. O que é o arquivo de estado?**\n\nApós executar **terraform apply** pela primeira vez, você verá que um arquivo chamado **terraform.tfstate** foi gerado no diretório do projeto. Esse arquivo (no formato JSON) contém a representação da infraestrutura que o Terraform gerenciou. Ele inclui todos os recursos e seus atributos conhecidos. Por exemplo, para nossa instância EC2, o state armazenará seu ID (**i-...**), IP, AMI usada, tags, etc. O state **mapeia os recursos definidos no código com os objetos reais na AWS**.\n\n- O Terraform usa esse arquivo em execuções futuras para saber o que já existe. Quando rodamos **terraform plan** ou **apply** subsequente, ele carrega o estado e compara com a configuração desejada para determinar quais alterações são necessárias (adição, mudança ou destruição de recursos).\n- **Não edite manualmente** o arquivo de estado. Alterações manuais podem corromper o mapeamento e levar a inconsistências. O Terraform oferece comandos próprios para manipular o estado se necessário (como **terraform state rm**, **mv**, etc.), mas essa é uma funcionalidade avançada para casos especiais.\n\nPara visualizar o conteúdo do state de forma legível, você pode usar o comando:\n\n```bash\nterraform show terraform.tfstate\n```\n\nque exibirá os recursos e atributos atuais conforme registrados. Por exemplo, uma seção do state para nossa instância EC2 pode parecer assim (resumido):\n\n```json\n{\n    \"resources\": [{\n        \"type\": \"aws_instance\",\n        \"name\": \"exemplo\",\n        \"provider\": \"provider[\\\"registry.terraform.io/hashicorp/aws\\\"]\",\n        \"instances\": [{\n            \"attributes\": {\n                \"id\": \"i-0bb...123\",\n                \"ami\": \"ami-0fc5d935ebf8bc3bc\",\n                \"instance_type\": \"t2.micro\",\n                \"public_ip\": \"3.85.xx.xx\",\n                \"tags\": {\n                    \"Name\": \"ServidorExemplo\"\n                },\n                // ... outros atributos ...\n            }\n        }]\n    }]\n}\n```\n\nIsso mostra o recurso **aws_instance.exemplo** com seus atributos atuais. O Terraform compara esses valores com o que está nos arquivos **.tf** para detectar diferenças.\n\n**2. Importância do state:**\n\n- **Fonte de verdade:** O state é considerado pela ferramenta como o estado real conhecido da sua infra. Se alguém modificar manualmente a infraestrutura fora do Terraform (ex: mudar uma configuração da instância via console AWS), o state do Terraform ficará desatualizado em relação ao mundo real. É possível então que um **terraform plan** indique mudanças (pois ao refrescar ele percebe divergências). Portanto, mantenha a disciplina de usar o Terraform para mudar recursos, ou se alterar externamente, sincronize (via **terraform refresh** ou import) para evitar surpresas.\n- **Performance:** Com o state, o Terraform não precisa consultar a API de todos os recursos toda vez – ele já sabe o que existe e só consulta mudanças relevantes, tornando as operações mais eficientes em infraestruturas grandes.\n- **Tracking metadata e dependências:** O state guarda metadados e permite ao Terraform entender as dependências entre recursos. Por exemplo, se um recurso depende de outro (implicitamente por referência ou explicitamente via **depends_on**), o Terraform usa o state para orquestrar a ordem de criação/remoção corretamente.\n\n**3. Backend do state (armazenamento local vs remoto):**\n\nPor padrão, o Terraform salva o arquivo **terraform.tfstate** localmente (no mesmo diretório). Isso funciona bem para uso individual ou ambientes de teste. Porém, em contextos colaborativos ou de equipe, **é problemático compartilhar o state via arquivo local**, pois apenas uma pessoa teria as informações atualizadas. Além disso, se dois membros aplicarem mudanças ao mesmo tempo usando estados separados, podem ocorrer conflitos e infra inconsistente.\n\nPara resolver isso, o Terraform suporta **backends remotos** para o state:\n\n- Você pode configurar o Terraform para armazenar o state em um local remoto centralizado, como um bucket S3 na AWS, um armazenamento no Azure, Terraform Cloud/Enterprise, entre outros. Por exemplo, usando um backend S3, todos os membros da equipe apontam para o mesmo arquivo de state na nuvem. Assim, quando alguém faz um apply, o arquivo remoto é atualizado e outro usuário obtém essas atualizações automaticamente no próximo run.\n- Backends remotos frequentemente suportam **bloqueio de estado (state locking)** para prevenir condições de corrida. No caso do AWS S3, normalmente configura-se um bloqueio usando DynamoDB junto com o bucket S3 (o Terraform adquire um lock no DynamoDB durante operações para que dois processos não alterem o state simultaneamente).\n\nConfigurar backend: É feito adicionando um bloco **backend** dentro do bloco **terraform {}**. Exemplo de configuração para S3 (sintaxe ilustrativa):\n\n```hcl\nterraform {\n  backend \"s3\" {\n    bucket = \"meu-bucket-terraform-state\"\n    key    = \"dev/state.tfstate\"    # caminho/nome do arquivo no bucket\n    region = \"us-east-1\"\n    dynamodb_table = \"terraform-locks\"  # tabela DynamoDB para lock (já deve existir)\n    encrypt = true\n  }\n}\n```\n\nApós configurar, rodamos **terraform init** novamente para migrar o state local para o remoto. A partir daí, o state reside no S3.\n\n_Para propósitos deste tutorial, manteremos o backend local default._ Mas fique ciente de que em projetos profissionais quase sempre usa-se backends remotos para **compartilhar o state** de forma segura.\n\n**4. Comandos úteis de state:** O Terraform oferece subcomandos para inspecionar ou modificar o state:\n\n- **terraform state list** – lista todos os recursos presentes no estado atual (útil para ver o que Terraform está gerenciando).\n- **terraform state show <RESOURCE>** – mostra os atributos específicos de um recurso do state.\n- **terraform import** – permite importar um recurso criado manualmente na AWS para dentro do state do Terraform (avançado, para adoção gradual de Terraform em infra existente).\n- **terraform state mv** / **rm** – para mover ou remover manualmente entradas do state (casos especiais de refatoração).\n\nEm geral, manipular state manualmente requer cautela. A necessidade de fazer isso indica frequentemente que queremos reorganizar configurações ou corrigir algo fora do comum. Nas operações cotidianas, **o Terraform gerencia o state automaticamente**.\n\n**5. Protegendo o state:** O arquivo state contém informações sensíveis – por exemplo, dados de recursos podem incluir IPs, IDs, e possivelmente segredos (se você gerencia senhas/keys via Terraform, elas podem aparecer no state em texto plano). Portanto:\n\n- Adicione o **terraform.tfstate** (e o diretório **.terraform/**) ao **.gitignore** caso use Git, **não commit** esse arquivo em controle de versão.\n- Ao usar backends remotos, restrinja o acesso ao bucket ou armazenamento para apenas as pessoas/processos necessários.\n- Considere ativar criptografia no backend remoto (ex: S3 com SSE ou usar Vault/Terraform Cloud que criptografam automaticamente).\n\n**Resumo:** O **state** do Terraform é o mecanismo que permite que ele saiba o que está acontecendo na sua infraestrutura. Ele **mapeia** as configurações declarativas aos recursos reais e registra o estado atual deles. Esse conceito possibilita planos precisos e aplicações idempotentes. Entender e gerenciar o state (especialmente em equipe, usando backends remotos e locks) é fundamental para um uso seguro do Terraform em ambientes profissionais.\n\nTendo coberto o state, seguimos em frente para recursos mais avançados. No próximo tutorial, veremos os **Provisioners**, que permitem executar ações adicionais (scripts/comandos) durante a criação ou destruição de recursos."
  },
  {
    "id": "1ca38694-1270-4b9f-b46f-e16b2847a9db",
    "title": "Provisioners (Executando Scripts e Comandos)",
    "description": "Ensinar como executar comandos e scripts durante a criação dos recursos, utilizando provisioners como remote-exec e local-exec.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Fundamentos",
      "Provisioner",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/1ca38694-1270-4b9f-b46f-e16b2847a9db",
    "markdown": "Normalmente, o Terraform se limita a declarar infraestrutura (instâncias, redes, buckets etc.). Mas e se quisermos executar alguma configuração extra, como rodar um script dentro de uma máquina criada ou executar um comando local após criar um recurso? Para esses casos, o Terraform oferece os **Provisioners**.\n\nProvisioners são trechos de configuração que permitem executar scripts _depois_ que um recurso é criado ou _antes/depois_ de ser destruído. Eles podem agir **localmente** (na máquina onde o Terraform está sendo rodado) ou **remotamente** (dentro de uma instância provisionada, via SSH/WinRM). É uma feature de \"configuração pós-provisionamento\".\n\n**⚠️ Aviso:** Provisioners devem ser usados como **último recurso**. A HashiCorp recomenda, quando possível, usar métodos nativos de provisionamento (por exemplo, _user data_ no EC2, scripts de inicialização, ou ferramentas de gerência de configuração como Ansible, Chef) em vez de provisioners do Terraform. Isso porque provisioners podem introduzir dependências implícitas e falhas não determinísticas (se um script falha, o Terraform marca o recurso como \"tainted\"). Ainda assim, são úteis em casos específicos e vamos demonstrá-los aqui na criação de uma instância EC2.\n\n**1. Provisioner remote-exec (executando comandos na instância):**\n\nVamos estender nossa configuração da instância EC2 para instalar o Nginx automaticamente quando a máquina subir. Usaremos o provisioner **remote-exec** para isso.\n\nNo recurso **aws_instance.exemplo**, adicione dentro do bloco resource:\n\n```hcl\nresource \"aws_instance\" \"exemplo\" {\n  # ... outros atributos (ami, instance_type, etc) ...\n\n  provisioner \"remote-exec\" {\n    inline = [\n      \"sudo apt-get update -y\",\n      \"sudo apt-get install -y nginx\"\n    ]\n\n    # Configurações de conexão SSH para a instância\n    connection {\n      type        = \"ssh\"\n      host        = self.public_ip         # IP público da instância\n      user        = \"ubuntu\"               # Usuário SSH (Ubuntu AMI usa 'ubuntu'; Amazon Linux usaria 'ec2-user')\n      private_key = file(\"~/.ssh/terraform-key.pem\")\n    }\n  }\n}\n```\n\n**Entendendo o código:**\n\n- Definimos um **provisioner \"remote-exec\"** dentro do recurso EC2. Isso informa que, após criar a instância, o Terraform deve **se conectar via SSH** a ela e executar alguns comandos.\n- A opção **inline** nos permite especificar uma lista de comandos shell que serão executados sequencialmente na máquina remota. Aqui usamos dois comandos: update dos pacotes e instalação do nginx (considerando que a AMI é Ubuntu/Debian). Poderíamos alternativamente usar **script = \"caminho/para/script.sh\"** para enviar e executar um script pronto.\n- O bloco **connection** especifica _como_ se conectar na instância:\n    - **host = self.public_ip** significa usar o IP público da própria instância (**self** refere-se ao recurso em contexto). Poderíamos usar **aws_instance.exemplo.public_ip** igualmente.\n    - **user** é o usuário SSH apropriado. **Atenção:** varia conforme a AMI. Para Ubuntu usamos **ubuntu**, para Amazon Linux seria **ec2-user**, para RedHat **ec2-user** ou **root**, etc. Certifique-se de usar o correto ou você terá falha de autenticação.\n    - **private_key** usamos a função built-in **file()** para ler o conteúdo de um arquivo de chave privada. Aqui apontamos para o arquivo PEM do par de chaves \"terraform-key\" que baixamos no Tutorial 1. Ajuste o caminho conforme onde sua chave está armazenada. Alternativamente, se seu agente SSH estiver carregado com a chave, poderíamos usar **agent = true** em vez de especificar a chave diretamente.\n\nQuando rodarmos **terraform apply** com esse provisioner adicionado, o fluxo será:\n\n1. Terraform cria a instância EC2.\n2. Quando a instância estiver pronta, o Terraform tentará conectar via SSH (pode esperar alguns segundos até a porta SSH responder).\n3. Executará os comandos **sudo apt-get update -y** e depois **sudo apt-get install -y nginx**.\n4. Se tudo executar com sucesso, o provisioner termina com sucesso; caso algum comando retorne erro (status diferente de 0), o Terraform marcará o recurso como _tainted_ (com falha) e você poderia tentar aplicar novamente após corrigir o script.\n\nApós a conclusão, você poderia testar acessando o IP público da instância no navegador (porta 80) para ver a página padrão do Nginx, verificando que a instalação ocorreu.\n\n**2. Provisioner local-exec (executando comando localmente):**\n\nO **local-exec** roda um comando no **máquina local** (onde o Terraform está rodando). Por exemplo, podemos usar para acionar alguma ferramenta ou script após criar um recurso. Um caso de uso comum: chamar o AWS CLI local para algo não suportado diretamente pelo Terraform, ou simplesmente exibir uma mensagem customizada.\n\nExemplo simples: adicionar um provisioner local-exec que só imprime uma mensagem:\n\n```hcl\nresource \"aws_instance\" \"exemplo\" {\n  # ... atributos e remote-exec ...\n  \n  provisioner \"local-exec\" {\n    command = \"echo 'Instância ${self.id} criada com IP ${self.public_ip}'\"\n  }\n}\n```\n\nAqui usamos **${self.id}** e **${self.public_ip}** para inserir o ID e IP da instância na mensagem (essa sintaxe dentro de strings é para interpolação de expressões do Terraform). Esse comando será executado localmente, então você verá essa mensagem no console do Terraform durante o apply. O **local-exec** roda assim que o recurso é criado e não depende de conexão SSH.\n\n**3. Provisioners em ações de destruição:**\n\nTambém existem provisioners que executam em destruição de recurso, usando a sintaxe **provisioner \"local-exec\" { when = destroy ... }** por exemplo. Isso poderia, por exemplo, rodar um script de backup antes de apagar um servidor, ou limpar algum registro. Só mencionando para referência – não usuaremos no exemplo atual.\n\n**4. Boas práticas com provisioners:**\n\n- Tente manter os scripts idempotentes. Se você aplicar novamente o Terraform, o provisioner **remote-exec** não será reexecutado a menos que o recurso em si precise ser recriado (ou se você forçar via **taint**). Então, idealmente, o script deveria poder rodar múltiplas vezes sem problemas (no nosso exemplo, reinstalar nginx repetidamente não quebra nada, apt-get handle corretamente).\n- Se a configuração da instância é complexa (vários pacotes, confs), considere usar _user data_ do EC2. Você pode fornecer **user_data** no recurso **aws_instance** com um script shell que a AWS executa na inicialização. Isso muitas vezes substitui a necessidade de um **remote-exec**, com a vantagem do script ficar registrado no ec2 e executar mesmo sem o Terraform conectado.\n- _Debug:_ erros em provisioners podem ser difíceis de diagnosticar de primeira. Use **terraform apply -auto-approve -parallelism=1** se precisar ver a saída em ordem (embora para um recurso único não importe). O Terraform mostrará o stdout/stderr dos comandos provisionados. Em caso de falha, ele marca o recurso com erro; você pode usar **terraform taint <resource>** para forçar recriação e tentar novamente após ajustar o script.\n\n**5. Alternativa: null_resource + triggers:**\n\nUma estratégia para executar ações sem atrelar a um recurso específico é usar o recurso especial **null_resource**. Um **null_resource** não provisiona nada na nuvem, mas você pode anexar provisioners a ele e usar o argumento **triggers** para definir dependências arbitrárias. Por exemplo:\n\n```hcl\nresource \"null_resource\" \"exec_depois\" {\n  triggers = {\n    instancia_id = aws_instance.exemplo.id\n  }\n\n  provisioner \"local-exec\" {\n    command = \"echo Instância ${trigger.instancia_id} pronta.\"\n  }\n}\n```\n\nAqui, o **null_resource** terá como _trigger_ o ID da instância EC2. Sempre que esse ID mudar (ou seja, a instância for recriada), o Terraform recria esse null_resource, acionando o provisioner. Isso garante que o comando rode após a instância ser (re)criada. Essa técnica é útil para sequenciar provisioners de forma controlada. Contudo, para iniciantes, entender que isso existe é suficiente.\n\n**Resumo:** Os **Provisioners** dão flexibilidade para executar ações complementares à criação de recursos:\n\n- **remote-exec** para configurar dentro de máquinas (via SSH/WinRM).\n- **local-exec** para executar coisas localmente (scripts, notificações, etc.). Eles devem ser usados com critério, pois introduzem considerações de ordem de execução e possíveis instabilidades. No entanto, são muito poderosos em cenários onde precisamos integrar Terraform com processos de configuração.\n\nNo nosso exemplo, vimos como instalar automaticamente um servidor web em uma instância criada. Nos próximos tutoriais, focaremos em como estruturar melhor nosso código Terraform, primeiro com **Módulos** para reuso e organização, e depois explorando **Meta-argumentos** e **Funções/Expressões** avançadas da linguagem."
  },
  {
    "id": "d60862af-f596-493e-aa07-46d1ebbff335",
    "title": "Módulos no Terraform",
    "description": "Ensinar a criação e utilização de módulos no Terraform para modularizar a infraestrutura e melhorar a manutenção do código.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Fundamentos",
      "Modules",
      "AWS",
      "VPC"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/d60862af-f596-493e-aa07-46d1ebbff335",
    "markdown": "Conforme sua infraestrutura cresce, manter tudo em um único conjunto de arquivos pode ficar difícil. O Terraform permite **organizar e reutilizar código** através de **Módulos**. Um módulo nada mais é do que um conjunto de configurações Terraform em uma pasta, que pode ser referenciado por outras configurações. Qualquer pasta com arquivos **.tf** pode ser considerada um módulo (inclusive a pasta raiz do seu projeto, chamada de _root module_).\n\n**1. Por que usar módulos?**\n\n- **Reutilização:** Escreva um módulo uma vez e use-o em vários lugares. Por exemplo, você pode criar um módulo que representa um servidor web com certas configurações (instância + segurança + attach de volume, etc.) e então reutilizá-lo para lançar múltiplos servidores iguais em vez de duplicar código.\n- **Organização:** Separar a infraestrutura em componentes lógicos. Por exemplo, um módulo para _networking_, outro para _servidores_, outro para _banco de dados_, facilita a manutenção.\n- **Abstração:** Você pode encapsular detalhes complexos dentro de um módulo e expor apenas variáveis e outputs necessários, simplificando o uso para outros (ou equipe).\n\nEm suma, módulos tornam a configuração **componível e reutilizável** – princípios importantes conforme os projetos escalam.\n\n**2. Criando um módulo local:**\n\nVamos criar um módulo simples a partir do que já temos. Suponha que queiramos reutilizar a configuração da instância EC2 (com suas variáveis) para criar várias instâncias semelhantes. Podemos transformar nossa configuração em um módulo genérico de _instância EC2_.\n\n- Crie uma pasta chamada **modules/instancia_ec2**. Dentro dela, crie três arquivos: **main.tf**, **variables.tf** e **outputs.tf** (estrutura comum para módulos).\n\n**modules/instancia_ec2/main.tf:**\n\n```hcl\n# Recurso EC2 definido dentro do módulo\nresource \"aws_instance\" \"this\" {\n  ami           = var.ami_id\n  instance_type = var.instance_type\n  key_name      = var.key_name\n\n  tags = {\n    Name = var.instance_name\n  }\n}\n```\n\n**modules/instancia_ec2/variables.tf:**\n\n```hcl\nvariable \"ami_id\" {\n  description = \"ID da AMI da instância\"\n}\n\nvariable \"instance_type\" {\n  description = \"Tipo da instância\"\n  default     = \"t2.micro\"\n}\n\nvariable \"key_name\" {\n  description = \"Nome do par de chaves SSH\"\n}\n\nvariable \"instance_name\" {\n  description = \"Valor da tag Name da instância\"\n}\n```\n\n**modules/instancia_ec2/outputs.tf:**\n\n```hcl\noutput \"instance_id\" {\n  description = \"ID da instância criada\"\n  value       = aws_instance.this.id\n}\n\noutput \"public_ip\" {\n  description = \"IP público da instância\"\n  value       = aws_instance.this.public_ip\n}\n```\n\nExplicação:\n\n- Dentro do módulo, usamos o recurso **aws_instance.this** (optamos pelo nome interno \"this\" por convenção, já que o módulo pode ser instanciado várias vezes, assim cada instância do módulo terá seu próprio **aws_instance.this** separado).\n- Variáveis do módulo: definimos **ami_id**, **instance_type**, **key_name** e **instance_name** como entradas do módulo. Note que não colocamos **default** para **ami_id**, **key_name** e **instance_name** – isso os torna obrigatórios, forçando quem chamar o módulo a fornecer esses valores (o tipo default any se nenhum type é especificado, aqui inferimos string para estes).\n- Outputs do módulo: repassamos o ID e IP público para quem usar o módulo, caso queira acessar.\n\nTemos então um módulo que cria 1 instância EC2 de acordo com os parâmetros fornecidos.\n\n**3. Usando (chamando) um módulo:**\n\nNo arquivo principal do nosso projeto (por exemplo, **main.tf** na raiz), vamos **chamar o módulo** para criar instâncias. Podemos substituir nosso recurso explícito anterior por chamadas de módulo. Exemplo:\n\n```hcl\nmodule \"web1\" {\n  source          = \"./modules/instancia_ec2\"\n  ami_id          = \"ami-0fc5d935ebf8bc3bc\"\n  instance_type   = \"t2.micro\"\n  key_name        = \"terraform-key\"\n  instance_name   = \"WebServer-1\"\n}\n\nmodule \"web2\" {\n  source          = \"./modules/instancia_ec2\"\n  ami_id          = \"ami-0fc5d935ebf8bc3bc\"\n  instance_type   = \"t2.micro\"\n  key_name        = \"terraform-key\"\n  instance_name   = \"WebServer-2\"\n}\n```\n\nCada bloco **module \"<nome>\" { ... }** instanciará nosso módulo. Os atributos dentro do bloco correspondem às variáveis do módulo que declaramos:\n\n- **source** indica onde está o código do módulo. No caso, usamos um path relativo apontando para a pasta do módulo local. Poderia ser um endereço de repositório Git, registro público, etc., se fosse um módulo externo.\n- Fornecemos valores para as variáveis **ami_id**, **instance_type**, etc. (nesse caso usamos os mesmos para ambos, exceto o **instance_name** que diferenciamos para fins de identificação).\n\nAgora, quando rodarmos **terraform apply**, o Terraform vai entrar na pasta do módulo, ler o **main.tf** dele e criar os recursos definidos ali, para cada instância do módulo.\n\nTeremos então 2 instâncias EC2 criadas, com nomes \"WebServer-1\" e \"WebServer-2\". Os outputs definidos no módulo também ficam disponíveis. Você pode acessá-los via **module.web1.public_ip** por exemplo, em um output no root module, ou via o comando output se você expôs no root. Se quiséssemos, poderíamos no root module criar outputs para consolidar, ex:\n\n```hcl\noutput \"ips_dos_webs\" {\n  value = [ module.web1.public_ip, module.web2.public_ip ]\n}\n```\n\nAssim, após o apply, veríamos a lista de IPs das duas instâncias.\n\n**4. Módulos do Terraform Registry (reutilização compartilhada):**\n\nAlém de módulos locais, o Terraform possui um **Registry público** (registro) onde a comunidade e a HashiCorp publicam módulos reutilizáveis. Em vez de escrever tudo do zero, você pode usar módulos prontos. Por exemplo, módulos populares:\n\n- **terraform-aws-vpc** que cria uma VPC completa (subnets, gateways, etc.).\n- **terraform-aws-ec2-instance** para instâncias com configurações pré-determinadas.\n- Muitos outros para bancos de dados, redes, Kubernetes, etc.\n\nPara usar um módulo do registry, o **source** do módulo usa uma notação como **terraform-aws-modules/ec2-instance/aws** (nome do módulo e provedor). O Terraform baixará automaticamente o módulo do registry no init. Por exemplo:\n\n```hcl\nmodule \"servers\" {\n  source  = \"terraform-aws-modules/ec2-instance/aws\"\n  version = \"4.1.0\"  # é bom pinar versão\n\n  # Parâmetros exigidos pelo módulo (conforme documentação do módulo)\n  name = \"MyServer\"\n  instance_count = 2\n  ami = \"ami-083654bd07b5da81d\"\n  instance_type = \"t2.micro\"\n  # ... e muitos outros possíveis ...\n}\n```\n\nOs módulos oficiais costumam ter documentação no registry com exemplos de uso. Usá-los pode poupar muito trabalho, mas certifique-se de entender o que fazem e adequar às suas necessidades.\n\n**5. Boas práticas de módulos:**\n\n- **Coesão:** um módulo deve ter uma responsabilidade clara. Por exemplo, não misture no mesmo módulo recursos de rede e de instância. Mantenha com propósito específico.\n- **Variáveis e Outputs bem definidos:** exponha o que faz sentido para reutilização e mantenha detalhes internos encapsulados. Forneça descrições e padrões lógicos.\n- **Versões:** se publicar módulos (ou mesmo internamente), controle versões para evitar quebrar setups existentes ao atualizar.\n- **Organização de pastas:** comum termos uma estrutura de repositório como:\n    \n    ```\n    /terraform\n      /modules\n        /modulo1/...\n        /modulo2/...\n      /envs\n        /dev/main.tf  (root module usando módulos)\n        /prod/main.tf ...\n    ```\n    \n    Ou seja, separar definição de módulos reutilizáveis e as configurações específicas de cada ambiente que consomem esses módulos.\n\nNo nosso caso, fizemos um módulo simples de instância EC2. Imagine que poderíamos tê-lo parametrizado ainda mais (quantidade de discos, se executa provisioner ou não, etc.) e então reutilizá-lo para diferentes tipos de servidor definindo variáveis diferentes.\n\n**Resumo:** Módulos são elementos chave para escrever **infraestrutura como código** de forma DRY (_Don't Repeat Yourself_). Eles permitem estruturar o código em unidades reutilizáveis e compartilhar configurações entre projetos e equipes. Com módulos, podemos criar \"catálogos\" de componentes de infra (rede básica, servidor padrão, cluster XYZ, etc.) que aceleram a composição de novos ambientes.\n\nNos próximos tutoriais, vamos continuar aprimorando nosso uso do Terraform explorando os **Meta-argumentos** (parâmetros especiais que recursos aceitam) e as **Funções & Expressões** da linguagem para lógica e transformações de valores."
  },
  {
    "id": "1d94e151-8ae0-4ab7-b402-313f531d7bd0",
    "title": "Meta-argumentos: count, for_each, depends_on e lifecycle",
    "description": "Explorar os meta argumentos do Terraform, como  count, for_each, depends_on e lifecycle, que permitem maior controle e lógica na criação de recursos.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Fundamentos",
      "Meta_arguments",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/1d94e151-8ae0-4ab7-b402-313f531d7bd0",
    "markdown": "No Terraform, além das propriedades normais de recursos (definidas pelo provedor), existem alguns argumentos especiais – os **meta-argumentos** – que podem ser usados em **qualquer recurso** (ou módulo) para controlar seu comportamento. Os principais meta-argumentos são: **count**, **for_each**, **depends_on** e o bloco **lifecycle** (que inclui flags como **create_before_destroy**, **prevent_destroy**, etc.). Vamos entender cada um e ver exemplos.\n\n### count\n\nO meta-argumento **count** permite criar múltiplas instâncias de um recurso usando uma única declaração. Ele espera um valor inteiro. Se **count = 3**, por exemplo, aquele recurso será criado 3 vezes, e você poderá referenciá-los através de índices (0,1,2).\n\n**Exemplo:** Suponha que queremos 3 instâncias EC2 idênticas (como fizemos chamando módulo duas vezes, mas poderíamos usar count também). Podemos modificar nosso recurso (ou módulo) assim:\n\n```hcl\nresource \"aws_instance\" \"worker\" {\n  count         = 3\n  ami           = var.ami_id\n  instance_type = var.instance_type\n  key_name      = var.key_name\n\n  tags = {\n    Name = \"Worker-${count.index}\"\n  }\n}\n```\n\n- Definindo **count = 3**, o Terraform criará **aws_instance.worker[0]**, **aws_instance.worker[1]** e **aws_instance.worker[2]**. O **count.index** é uma variável interna disponível no contexto do recurso, indicando o índice atual (0,1,2) durante a criação de cada.\n- No exemplo, as instâncias receberão tags Name \"Worker-0\", \"Worker-1\" e \"Worker-2\" respectivamente graças ao **${count.index}**.\n- Para referenciar atributos de um recurso contado, usamos sintaxe de índice. Por exemplo, **aws_instance.worker[1].public_ip** seria o IP da segunda instância (índice 1).\n- **count** é útil para recursos homogêneos e quando a quantidade é conhecida ou controlada por variável. Por exemplo, poderíamos ter uma variável **quantidade_workers** e usar **count = var.quantidade_workers**.\n\n### for_each\n\nEnquanto **count** trabalha com um número e índices, o meta-argumento **for_each** permite iterar sobre uma coleção (lista ou mapa) de valores, criando uma instância do recurso para cada elemento. Ele dá mais controle sobre identificar cada instância por uma chave (ao invés de apenas índices numéricos).\n\n**Exemplo:** Criar múltiplos buckets S3 com nomes definidos em lista:\n\n```hcl\nvariable \"nomes_buckets\" {\n  type = list(string)\n  default = [\"logs\", \"assets\", \"backup\"]\n}\n\nresource \"aws_s3_bucket\" \"bucket\" {\n  for_each = toset(var.nomes_buckets)\n  bucket   = each.value   # each.value será um dos nomes da lista\n  acl      = \"private\"\n}\n```\n\n- Aqui, **for_each = toset(var.nomes_buckets)**. Usamos **toset(...)** para converter a lista em um conjunto (set) de valores únicos adequados para o for_each. Dessa forma, **each.value** iterará sobre \"logs\", \"assets\", \"backup\".\n- O Terraform criará 3 buckets com esses nomes. Em vez de índices, a identificação será pelas chaves do set (nesse caso as próprias strings dos nomes):\n    - **aws_s3_bucket.bucket[\"logs\"]**, **aws_s3_bucket.bucket[\"assets\"]**, etc., serão as referências.\n- Podemos acessar atributos por chave: ex: **aws_s3_bucket.bucket[\"assets\"].id** referenciaria o bucket com nome \"assets\".\n- Vantagem sobre count: se os nomes forem únicos, o for_each permite adicionar/remover elementos da coleção e o Terraform consegue identificar precisamente qual recurso corresponde a qual elemento, minimizando recriações desnecessárias. Com count, remover um item no meio da lista poderia recriar outros por mudança de índice. Com for_each usando chaves, isso é mais estável.\n\n**Dica:** **for_each** também aceita maps. Se usássemos um map, **each.key** e **each.value** estariam disponíveis dentro do recurso. Por exemplo, suponha um map de usuários para grupos, você poderia iterar e usar key e value em diferentes campos.\n\nTanto **count** quanto **for_each** **não podem ser usados simultaneamente** em um mesmo recurso. Você escolhe um ou outro de acordo com a necessidade de iterar.\n\n### depends_on\n\nO Terraform normalmente determina automaticamente as dependências entre recursos lendo referências (por exemplo, se um recurso A usa um atributo de B, então A depende de B). Porém, às vezes é preciso declarar dependências **explicitamente**, quando não há referência direta no código mas queremos forçar ordem de criação.\n\nO meta-argumento **depends_on** aceita uma lista de recursos que este recurso depende. Isso garante que, ao aplicar, o Terraform crie esses recursos dependentes antes.\n\n**Exemplo típico:** Suponha que temos um recurso que executa um comando local que precisa que _dois_ recursos diferentes estejam prontos, mas ele não referencia nenhum diretamente. Podemos usar **depends_on** para especificar ambos.\n\nOutro exemplo concreto:\n\n```hcl\nresource \"aws_instance\" \"app\" { ... }\n\nresource \"aws_eip\" \"app_eip\" {\n  vpc      = true\n  depends_on = [aws_instance.app]\n}\n```\n\nNesse caso, um Elastic IP **aws_eip.app_eip** que, digamos, é associado via user-data ou script, talvez não tenha ligação explícita no Terraform com a instância. Mas usando depends_on, garantimos que a instância **app** seja criada antes de alocar o EIP (mesmo que a associação não ocorra aqui via Terraform).\n\nEm geral, use **depends_on** quando o Terraform não consegue inferir a ordem correto por conta própria. Um exemplo de uso real dado anteriormente foi:\n\n```hcl\nresource \"aws_elb\" \"web_elb\" {\n  depends_on = [aws_instance.web]\n  instances  = [aws_instance.web.id]\n}\n```\n\nAqui o **depends_on** é redundante porque referenciar **aws_instance.web.id** já estabelece a dependência. Mas ilustra a sintaxe: uma lista de recursos dos quais este depende.\n\n### lifecycle\n\nO bloco **lifecycle** dentro de um recurso permite ajustar o comportamento do ciclo de vida de criação/atualização/remoção de um recurso. Ele suporta algumas opções:\n\n- **create_before_destroy**: Em casos de substituição (recreação) de um recurso, essa flag indica para **criar o novo antes de destruir o antigo**. Muito útil para evitar downtime em certos recursos. Por exemplo, se você estiver alterando um recurso que não pode coexistir e por padrão o Terraform destruíria primeiro e depois criaria, isso evitaria interrupção.\n    \n- **prevent_destroy**: Protege o recurso contra destruição. Se alguém tentar **terraform destroy** ou remover o recurso da config, o Terraform acusará erro em vez de destruí-lo. Use isso para recursos críticos que nunca devem ser deletados acidentalmente (por exemplo, banco de dados de produção). Ex:\n    \n    ```hcl\n    resource \"aws_s3_bucket\" \"dados_importantes\" {\n      lifecycle {\n        prevent_destroy = true\n      }\n      # ... demais config ...\n    }\n    ```\n    \n    Com isso, mesmo um **terraform destroy** geral falhará se tentar remover esse bucket, a não ser que o flag seja removido ou sobrescrito com **-target** e **-force** muito intencionalmente.\n    \n- **ignore_changes**: Serve para ignorar mudanças em alguns atributos de um recurso. Às vezes, algum campo pode mudar fora do Terraform e você quer evitar que o Terraform tente reverter. Ou há atributos que são definidos automaticamente após criação e se forem colocados na config podem causar recriações desnecessárias. Com **ignore_changes**, o Terraform não reagirá a drift nesses atributos específicos. Exemplo:\n    \n    ```hcl\n    resource \"aws_instance\" \"test\" {\n      # ... \n      lifecycle {\n        ignore_changes = [ tags[\"LastDeployed\"] ]\n      }\n    }\n    ```\n    \n    Se algum processo externo atualizar a tag LastDeployed na instância, o Terraform não vai detectá-la como mudança a ser corrigida.\n    \n- **replace_triggered_by**: (mais avançado) lista de recursos ou atributos cuja alteração deve forçar a substituição (destroy/create) deste recurso. Por exemplo, se um recurso A não depende diretamente de B, mas se B mudar você quer recriar A, pode usar isso.\n    \n\n**Exemplo de create_before_destroy:** Suponha que temos um recurso AWS Launch Configuration que não permite atualização in-place e sempre requer recriação. Poderíamos usar:\n\n```hcl\nresource \"aws_launch_configuration\" \"lc\" {\n  name_prefix = \"app-lc-\"\n  image_id    = \"ami-XYZ\"\n  instance_type = \"t3.small\"\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n```\n\nAssim, ao trocar **image_id**, o Terraform primeiro criaria a nova Launch Config antes de destruir a antiga. Isso evita problemas no Auto Scaling (que exige sempre uma LC ativa).\n\n**Recapitulando:**\n\n- **count**: múltiplos recursos indexados por número.\n- **for_each**: múltiplos recursos indexados por chave (elementos de set/map).\n- **depends_on**: define dependências extras manualmente.\n- **lifecycle**: controle fino do processo de criação/remoção:\n    - create_before_destroy, prevent_destroy, ignore_changes etc.\n\nUsar esses meta-argumentos efetivamente permite escrever configurações mais dinâmicas e seguras. Por exemplo, a combinação de **for_each** com estruturas de dados permite montar recursos de forma declarativa a partir de listas/objetos complexos; **count** e condicionais possibilitam habilitar/desabilitar recursos opcionalmente (às vezes vemos padrões como **count = var.enabled ? 1 : 0** para criar ou não um recurso baseado em flag booleana).\n\n**Exemplo prático integrando lógica condicional:** imagine que quiséssemos que a criação de um recurso fosse opcional:\n\n```hcl\nvariable \"criar_bucket_backup\" {\n  type    = bool\n  default = false\n}\n\nresource \"aws_s3_bucket\" \"backup\" {\n  count  = var.criar_bucket_backup ? 1 : 0\n  bucket = \"meu-backup-bucket-123\"\n}\n```\n\nAqui usamos um operador ternário (expressão condicional) **condicao ? valor_se_true : valor_se_false** para o count. Se **criar_bucket_backup** for true, count =1 (o recurso será criado); se false, count =0 (nenhum recurso criado). Isso dá toggle de criação.\n\nNo próximo tutorial, vamos explorar mais dessas **expressões e funções** disponíveis no Terraform, como o ternário que acabamos de usar e muitas funções built-in que ajudam a manipular strings, listas e outros valores."
  },
  {
    "id": "1ea403f4-0d4d-4b0f-9281-87d2cf00328b",
    "title": "Funções e Expressões no Terraform",
    "description": "Demonstrar o uso das funções embutidas do Terraform e expressões para manipular dados, realizar transformações e lidar com lógicas condicionais.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Fundamentos",
      "AWS",
      "Functions"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/1ea403f4-0d4d-4b0f-9281-87d2cf00328b",
    "markdown": "A linguagem do Terraform (HCL - HashiCorp Configuration Language) é declarativa, mas oferece diversos recursos para inserir lógica e transformar dados: **expressões condicionais, interpolações e uma biblioteca de funções built-in**. Isso permite que você calcule valores dinamicamente em seus arquivos **.tf**.\n\nVamos cobrir os principais elementos:\n\n### Interpolação de expressões\n\nJá vimos nos exemplos o uso de sintaxe como **\"${var.nome}\"** ou **\"${count.index}\"** dentro de strings, ou mesmo direto **var.instancia_tipo**. Em Terraform 0.12+, muitas vezes você pode usar a variável diretamente (sem **${}**) quando o contexto espera uma expressão. Por exemplo, **ami = var.ami_id** é equivalente a **ami = \"${var.ami_id}\"** (caso seja string). Dentro de strings, para concatenar variáveis e texto, você usa a sintaxe **${ ... }**.\n\nEx:\n\n```hcl\ntags = {\n  Name = \"${var.env}-${var.app}\" \n}\n```\n\nAqui, se **env = \"dev\"** e **app = \"loja\"**, o resultado será Name = \"dev-loja\". Você também pode usar a **função** **format** para formatação:\n\n```hcl\nName = format(\"%s-%s\", var.env, var.app)\n```\n\nteria o mesmo efeito, usando placeholder do estilo Go (onde **%s** insere strings).\n\n### Expressões condicionais (ternário)\n\nA expressão condicional em Terraform usa o operador **? :** e tem a forma:\n\n```\ncondição ? resultado_se_true : resultado_se_false\n```\n\nPor exemplo, do tutorial anterior:\n\n```hcl\ncount = var.tipo == \"prod\" ? 3 : 1\n```\n\nIsso configuraria **count** para 3 se o tipo for \"prod\", caso contrário 1. Você pode usar condicionais para definir valores de parâmetros ou decidir quantidades. Útil também em outputs, por exemplo:\n\n```hcl\noutput \"url\" {\n  value = var.env == \"prod\" ? aws_alb.prod.dns_name : \"http://staging.example.local\"\n}\n```\n\nAssim, em produção daria o DNS real do load balancer, em outros ambientes talvez um endereço placeholder.\n\nLembre que tanto o resultado true quanto false devem ser do **mesmo tipo** (ou conversíveis) porque o Terraform precisa determinar o tipo da expressão.\n\n### Funções built-in\n\nO Terraform possui [diversas funções built-in](https://developer.hashicorp.com/terraform/language/functions) para operar com strings, números, coleções e estruturas. Vamos citar algumas das mais comuns por categoria:\n\n- **Strings:**\n    \n    - **lower(\"TXT\")** - \"txt\" (minúsculas)\n    - **upper(\"abc\")** - \"ABC\" (maiúsculas)\n    - **concat(\"a\",\"b\")** - \"ab\" (concatenar strings; embora normalmente interpolação simples ou **format** sejam preferidos)\n    - **substr(\"abcdef\", 1, 3)** - \"bcd\" (substring da posição 1 com comprimento 3)\n    - **replace(\"filename.txt\", \".txt\", \".bak\")** - \"filename.bak\" (substituição simples)\n    - **file(\"path/to/file\")** - lê o conteúdo de um arquivo local e retorna como string (usamos no provisioner).\n- **Números e math:**\n    \n    - **max(5, 3, 9)** -> 9 (maior)\n    - **min(5, 3, 9)** -> 3 (menor)\n    - Operadores aritméticos diretos: você pode usar **a + b**, **a * b**, etc., dentro de expressões também, ex: **count = var.num_servers * 2**.\n- **Booleanos:**\n    \n    - Lógicos padrão: **&&** (AND), **||** (OR), e **!** (NOT) funcionam dentro de **${ }**.\n    - Ex: **var.env == \"prod\" && var.desplegar == true ? 1 : 0** combinas condições.\n    - **can** e **coalesce**: **coalesce(var.opt1, var.opt2, \"default\")** retorna o primeiro valor não-nulo (bom para defaults entre variáveis); **can()** verifica se uma expressão é válida sem lançar erro.\n- **Listas e Maps:**\n    \n    - **length(list or string)** - retorna comprimento. Ex: **length(var.nomes)** retorna quantos nomes na lista.\n    - **element([\"a\",\"b\",\"c\"], 1)** - \"b\" (pega elemento pelo índice).\n    - **slice(list, startIndex, endIndex)** - extrai sub-lista.\n    - **tolist(...)** e **toset(...)** convertendo tipos.\n    - **keys(map)** - lista de chaves do map\n    - **values(map)** - lista de valores.\n    - **lookup(map, key, default)** -> obtém valor pela chave ou default se não existir. Ex:\n        \n        ```hcl\n        variable \"amis\" {\n          type = map(string)\n          default = {\n            \"us-east-1\" = \"ami-aaaa\"\n            \"us-west-2\" = \"ami-bbbb\"\n          }\n        }\n        ami = lookup(var.amis, var.regiao, \"ami-default\")\n        ```\n        \n        Aqui escolhemos a AMI conforme a região, e se a região não estiver no mapa, usamos \"ami-default\".\n    - **merge(map1, map2, ...)** - combina maps.\n    - **zipmap(keys_list, values_list)** - cria um map a partir de duas listas (um de keys, outra de valores correspondentes).\n- **Outras úteis:**\n    \n    - **terraform.workspace** não é bem função mas uma variável global que diz o workspace atual (ambiente) se usar Terraform Workspaces.\n    - **cidrsubnet(base_cidr, new_prefix, subnet_index)** - calcula subnets de um CIDR base (muito útil em automação de rede).\n    - **base64encode(str)** / **base64decode(str)** para trabalhar com base64.\n    - **sha256(str)** para gerar hash (às vezes para digitar um material ou gerar senhas temporárias).\n    - **templatefile(\"file.tpl\", { var1 = val1, ... })** - carrega um arquivo de template e insere as variáveis fornecidas (substitui placeholders). Útil para criar arquivos de configuração ou scripts dentro do Terraform e passar para provisioners.\n\n**Exemplos combinando funções:**\n\n- Gerar um nome único concatenando prefixo, ambiente e um sufixo calculado:\n    \n    ```hcl\n    locals {\n      prefix = \"myapp\"\n      env    = var.env  # ex: \"prod\"\n      rand   = substr(replace(uuid(), \"-\", \"\"), 0, 4)  # 4 hex chars aleatórios\n    }\n    resource \"aws_s3_bucket\" \"unique\" {\n      bucket = \"${local.prefix}-${local.env}-${local.rand}\"\n      acl    = \"private\"\n    }\n    ```\n    \n    Aqui usamos **uuid()** (gera um UUID), tiramos os **-** com replace, e pegamos os 4 primeiros caracteres para um sufixo aleatório curto. Isso ajuda a garantir que o nome do bucket seja único (bucket names globais precisam ser únicos).\n    \n- Uso de condicionais com lista:\n    \n    ```hcl\n    resource \"aws_security_group\" \"example\" {\n      # ... regras ingress/egress ...\n      tags = merge(\n        var.common_tags,\n        var.env == \"prod\" ? { \"Critical\" = \"Yes\" } : {}\n      )\n    }\n    ```\n    \n    Neste snippet hipotético, usamos **merge** para unir tags comuns com uma tag adicional \"Critical\" somente se for ambiente de produção (caso contrário, merge com um map vazio não adiciona nada). Isso demonstra que a expressão condicional pode resultar em um mapa vazio ou com conteúdo, e **merge** combina com o outro mapa de tags.\n    \n\n**Looping dentro da configuração:**\n\nAlém de **count** e **for_each**, há também **expressões for** para construir listas ou maps dinamicamente:\n\n```hcl\nlocals {\n  nomes_upper = [ for nome in var.nomes: upper(nome) ]\n}\n```\n\nIsso iteraria **var.nomes** (lista de strings) e produziria uma nova lista **nomes_upper** em que cada nome é transformado para maiúsculas. As expressões **for** suportam também cláusula **if** para filtrar:\n\n```hcl\nlocals {\n  buckets_com_backup = { for k, v in var.bucket_configs : k => v if v.backup }\n}\n```\n\nAcima, supõe **var.bucket_configs** é um map de configurações de bucket, e filtramos apenas os que têm backup habilitado, criando um novo map só com eles.\n\nEssas construções avançadas permitem manipular dados declarativamente dentro do HCL, sem precisar de um script externo.\n\n### Resumo e dicas finais:\n\n- **Funções built-in:** Terraform tem muitas; consulte a [documentação oficial de funções](https://developer.hashicorp.com/terraform/language/functions) para ver todas. Sempre que você se pegar pensando \"preciso transformar este valor de tal forma\", há boa chance de já existir uma função (split, join, trim, formatdate, etc.).\n- **Limitações de linguagem:** Lembre-se que o Terraform não é uma linguagem de programação imperativa completa. Não há loops tradicionais do tipo \"for i in 1..10\" (em vez disso usa-se **count**/**for_each** para repetição de recursos ou for expressions para listas/mapas). Não há if/else campos como num script, mas o condicional ternário supre a maioria das lógicas condicionais.\n- **Locals:** Utilizamos brevemente **locals**. Os **local values** são variáveis locais dentro da configuração para armazenar resultados de expressões complexas ou simplificar repetição de valores. Eles não aparecem no output nem são inputs – são só para organização interna mesmo.\n- **Exprimir lógica com clareza:** Embora seja possível aninhar muitas funções e expressões em uma linha, pondere usar **locals** ou dividir passos para manter o código legível. Por exemplo, criar um **local** para um cálculo complexo e depois usar o local no recurso pode facilitar a leitura e manutenção.\n\nCom isso concluímos nossa série de tutoriais de Terraform. 🚀 Você aprendeu a:\n\n- Preparar o ambiente e entender o básico de IaC e Terraform.\n- Criar recursos na AWS com Terraform e usar os comandos fundamentais (**init/plan/apply/destroy**).\n- Usar variáveis e outputs para generalizar configurações e obter informações.\n- Compreender o Terraform state e boas práticas de armazenamento/compartilhamento.\n- Utilizar provisioners para ações pós-provisionamento (embora com moderação).\n- Modularizar sua infraestrutura com módulos reutilizáveis, inclusive usando módulos de fonte externa.\n- Aplicar meta-argumentos para criar múltiplos recursos, controlar dependências e o ciclo de vida de recursos.\n- Aproveitar funções e expressões da linguagem para tornar as configurações mais dinâmicas e poderosas.\n\nCom esses conhecimentos, você está bem equipado para começar a construir infraestruturas mais complexas e robustas como código. A documentação oficial do Terraform e dos provedores (como AWS) será sua aliada para descobrir novos recursos e aprofundar-se em detalhes específicos. Boa sorte na sua jornada com Terraform!"
  },
  {
    "id": "72a0de7a-ad07-4797-86d1-477e1581e0cc",
    "title": "Preparar o ferramental e ambiente",
    "description": "Configuração do ambiente de desenvolvimento, incluindo criação da conta AWS, instalação da AWS CLI, configuração de credenciais e escolha de um editor de código para escrever templates CloudFormation.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Cloudformation",
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/72a0de7a-ad07-4797-86d1-477e1581e0cc",
    "markdown": "Antes de começar a usar AWS CloudFormation, precisamos preparar o ambiente de trabalho com as ferramentas necessárias. Vamos garantir que você tenha acesso à AWS e as ferramentas instaladas para escrever e implantar templates do CloudFormation. Siga os passos abaixo:\n\n1. **Criar uma conta AWS:** Caso ainda não tenha, acesse o site da AWS e crie uma conta. É necessário um cartão de crédito para validar a conta, mas a AWS oferece um nível gratuito (_free tier_) que permite experimentar muitos serviços sem custos por 12 meses. Após criar a conta, faça login no [Console de Gerenciamento da AWS](https://aws.amazon.com/console/).\n    \n2. **Configurar credenciais de acesso:** Para usar o CloudFormation e outros serviços, você precisará de credenciais de acesso (Access Key ID e Secret Access Key) de um usuário AWS com permissões adequadas. Uma prática comum é criar um usuário IAM específico para você com permissões de administrador ou permissões restritas apenas ao CloudFormation e recursos necessários. No console AWS, navegue até **IAM (Identity and Access Management)** > **Users (Usuários)** e crie um usuário se ainda não tiver. Ao criar, marque a opção de **Acesso programático** para obter a Access Key ID e Secret. Guarde essas credenciais em local seguro.\n    \n3. **Instalar a AWS CLI:** A AWS Command Line Interface (CLI) é uma ferramenta de linha de comando que permite interagir com os serviços AWS, inclusive criar stacks do CloudFormation via terminal. Baixe e instale a AWS CLI v2 (disponível para Windows, Linux e macOS) do site oficial da AWS. Após a instalação, verifique no terminal com `aws --version` se tudo foi instalado corretamente.\n    \n4. **Configurar a AWS CLI:** Execute `aws configure` no terminal para configurar suas credenciais e região padrão. Esse comando solicitará seu **AWS Access Key ID**, **Secret Access Key**, **Default region name** (região AWS padrão, por exemplo `us-east-1` para Norte Virgínia) e **Default output format** (formato de saída, como `json`). Insira os valores quando solicitado. Este processo cria um _profile_ padrão com suas credenciais e configurações. Por exemplo, ao rodar o comando você verá prompts interativos:\n    \n    ```plaintext\n    $ aws configure\n    AWS Access Key ID [None]: <SEU_ACCESS_KEY_ID>\n    AWS Secret Access Key [None]: <SEU_SECRET_ACCESS_KEY>\n    Default region name [None]: us-east-1\n    Default output format [None]: json\n    ```\n    \n    Isso definirá a região padrão (neste caso, us-east-1) e o formato JSON para respostas. Você pode alterar essas configurações depois editando os arquivos de configuração (`~/.aws/credentials` e `~/.aws/config` no Linux/macOS ou `%UserProfile%\\.aws\\...` no Windows).\n    \n5. **Escolher uma região AWS:** Durante os tutoriais, usaremos uma região AWS para criar os recursos (por exemplo, **us-east-1** – Norte da Virgínia). Prefira regiões próximas geograficamente ou onde sua conta tenha _quotas_ disponíveis. Lembre-se de que os recursos do CloudFormation são criados dentro da região selecionada. Você pode mudar a região nas configurações da CLI (passo anterior) ou selecionando no menu superior direito do Console da AWS.\n    \n6. **Ferramenta para editar templates (opcional):** CloudFormation templates são arquivos de texto (JSON ou YAML). Você pode usar qualquer editor de texto ou IDE para escrevê-los. É recomendável um editor com realce de sintaxe para YAML/JSON e suporte a código, como VS Code, Atom ou Sublime Text. A extensão **cfn-lint** (CloudFormation Linter) está disponível para vários editores, ajudando a validar templates e apontar erros de sintaxe ou opções inválidas enquanto você digita. O cfn-lint é uma ferramenta de linha de comando que verifica templates CloudFormation e ajuda a identificar erros e más práticas. Por exemplo, ele checa se você usou nomes de propriedades válidos, tipos corretos e valores permitidos, garantindo que seu template siga as normas do CloudFormation antes mesmo de tentar executar. Embora opcional, utilizar o linter ou mesmo o comando `aws cloudformation validate-template` para validação básica pode evitar frustrações com erros de formatação.\n    \n7. **Verificar o ambiente:** Com a CLI configurada, teste listando os buckets S3 ou stacks existentes. Por exemplo, execute `aws s3 ls` (que lista buckets S3) ou `aws cloudformation list-stacks` para verificar se você recebe uma resposta (mesmo que vazia, sem erros de autorização). Se o comando executar com sucesso, seu ambiente está pronto. Em caso de erro de credenciais ou região, revise os passos anteriores.\n    \n\nCom esses passos, você terá uma conta AWS pronta e o ambiente configurado com as ferramentas necessárias. No próximo tutorial, começaremos a explorar o AWS CloudFormation em si."
  },
  {
    "id": "a5f7fbf7-3162-479d-a81c-04492f64c4d4",
    "title": "Introdução ao AWS Cloudformation",
    "description": "Explicação dos conceitos fundamentais: templates, stacks e infraestrutura como código, abordando os benefícios de automação e orquestração de recursos na AWS.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Cloudformation",
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/a5f7fbf7-3162-479d-a81c-04492f64c4d4",
    "markdown": "Agora que o ambiente está pronto, vamos entender o que é o AWS CloudFormation e por que usá-lo. O CloudFormation é um serviço de **Infraestrutura como Código (IaC)** que permite modelar e provisionar recursos de nuvem AWS por meio de templates. Em vez de criar manualmente recursos pelo console, você define tudo em um arquivo de texto (template) e o CloudFormation automatiza a criação e configuração seguindo aquele modelo. Isso traz diversos benefícios:\n\n### O que é e por que usar o CloudFormation?\n\n- **Infraestrutura como código declarativo:** Com CloudFormation, você declara em um arquivo todos os recursos AWS desejados (como instâncias EC2, buckets S3, etc.) e suas configurações. O serviço então provisiona e configura esses recursos para você, respeitando dependências e ordem automaticamente. Isso significa menos tempo clicando em interfaces e mais consistência — o template descreve exatamente o estado da infraestrutura.\n    \n- **Gerenciamento de grupo de recursos como unidade única:** Recursos definidos juntos em um template formam uma **stack** (pilha). Você pode criar, atualizar ou deletar todos aqueles recursos como uma única unidade (a stack). Por exemplo, um aplicativo web pode ter EC2, Load Balancer e banco de dados; todos são criados juntos e, se você deletar a stack, todos são removidos juntos. O CloudFormation facilita tratar recursos relacionados como um grupo coeso para gerenciamento.\n    \n- **Reprodução rápida de ambientes:** Como os templates são arquivos, você pode reutilizá-los para criar cópias idênticas da infraestrutura em outras regiões ou contas. Se você precisa levantar a mesma aplicação em _staging_ e _production_, basta rodar o mesmo template em ambas as regiões, garantindo consistência. Isso ajuda em escalabilidade multi-região e recuperação de desastres — o mesmo template pode criar o mesmo conjunto de recursos repetidamente de forma confiável.\n    \n- **Controle de mudanças e versionamento:** Templates sendo código texto podem ser versionados (ex: usando Git). Alterações na infraestrutura são feitas editando-se o template e fazendo _deploy_ (atualização da stack). Você pode rastrear no histórico de versões o que mudou, quando e por quem, assim como faria com código de software. Em caso de problema após uma mudança, você pode reverter para um template anterior e reimplantar a infraestrutura estável. Essa capacidade de auditar e reproduzir mudanças aumenta a confiabilidade.\n    \n- **Automação de implantações e segurança:** CloudFormation se integra a fluxos de CI/CD, permitindo automatizar a implantação de infraestrutura juntamente com aplicações. Além disso, por padrão todos os recursos são criados de forma segura e com as configurações definidas — reduzindo erros humanos de configuração. Se algo falhar durante a criação ou atualização, o CloudFormation pode reverter automaticamente (rollback) para o estado anterior, ajudando a manter a estabilidade.\n    \n\nEm resumo, o CloudFormation habilita práticas de DevOps na infraestrutura: automação, consistência, rapidez na criação de ambientes e melhor governança.\n\n### Conceitos básicos: Templates e Stacks\n\nDois conceitos fundamentais no CloudFormation são **template** e **stack**:\n\n- **Template:** é o arquivo (JSON ou YAML) que define os recursos a serem criados e suas configurações. Ele contém seções como Resources, Parameters, Outputs, etc. (vamos detalhar adiante). É como o “plano de construção” da infraestrutura desejada. Por exemplo, um template pode dizer \"criar uma instância t2.micro no Amazon EC2, um bucket no S3 e um tópico no SNS\".\n    \n- **Stack (Pilha):** é a instância de um template em execução. Quando você usa um template para criar recursos, o CloudFormation cria uma stack, que nada mais é do que o conjunto real de recursos provisionados conforme o template. A stack mantém o rastreamento de todos esses recursos. Se mais tarde você precisar atualizar a infraestrutura, você atualiza a stack fornecendo um template modificado (CloudFormation calculará as diferenças e aplicará as mudanças). Cada stack tem um estado e um histórico de eventos das operações (criação, atualizações, etc). No console AWS, você verá as stacks listadas e pode inspecionar os recursos dentro de cada stack.\n    \n\nUma analogia simples: o template é a receita, a stack é o prato pronto feito a partir daquela receita. Você pode usar a mesma receita para fazer vários pratos (p. ex., criar várias stacks de um mesmo template).\n\n**Custo:** Utilizar o AWS CloudFormation em si não tem custo adicional — o serviço é gratuito. Você paga apenas pelos recursos AWS que forem criados pelo CloudFormation dentro da sua conta. Por exemplo, se seu template cria uma instância EC2 e um banco de dados RDS, você será cobrado pelo EC2 e RDS nos preços normais, mas nada pelo CloudFormation orquestrar isso.\n\n### Estrutura de um template CloudFormation\n\nUm template CloudFormation é um arquivo de texto estruturado em seções. Cada **seção** cumpre uma finalidade específica dentro do template. Algumas das principais seções são:\n\n- **Resources (Recursos):** seção **obrigatória** em todos os templates. É o coração do template, onde você especifica cada recurso AWS que deseja criar e suas propriedades. Cada recurso recebe um nome lógico único no template e um tipo (por exemplo, AWS::EC2::Instance) que corresponde a um serviço AWS. Nesta seção você descreve detalhes como tamanho de uma instância, nome de um bucket, etc.\n    \n- **Parameters (Parâmetros):** seção opcional onde você pode declarar entradas personalizáveis para o template. Parâmetros permitem passar valores na hora de criar/atualizar a stack, tornando o template reutilizável em diferentes contextos. Por exemplo, você pode ter um parâmetro para o tipo da instância EC2, em vez de fixar no template. Assim o mesmo template pode criar um servidor t2.micro em dev ou m5.large em produção, conforme o parâmetro. Durante a criação da stack, o CloudFormation pede os valores desses parâmetros (ou usa valores padrão se definidos).\n    \n- **Outputs (Saídas):** também opcional, define valores de saída que o CloudFormation retorna após criar a stack. Outputs são usados para destacar informações importantes dos recursos criados, como o URL de um site, o ID de uma instância ou nomes gerados. Essas saídas aparecem no console (aba Outputs da stack) e também podem ser referenciadas por outras stacks (cross-stack reference) se você exportá-las, permitindo compartilhar recursos entre stacks.\n    \n- **Mappings (Mapeamentos):** seção opcional que funciona como uma tabela de consulta estática no template. Você pode mapear uma chave para diferentes valores dependendo de alguma condição predefinida, como região AWS ou ambiente. Por exemplo, um mapping poderia mapear nomes de região para IDs de AMI específicos daquela região. Usando a função intrínseca `Fn::FindInMap`, seu template pode buscar o valor correto no mapping conforme a região atual, sem precisar de vários parâmetros ou lógica complexa.\n    \n- **Conditions (Condições):** seção opcional para declarar condições lógicas que controlam a criação de recursos ou atribuição de propriedades. Você pode decidir criar ou não certos recursos com base em valores de parâmetros ou pseudo-parâmetros. Por exemplo, criar duas instâncias EC2 somente se o parâmetro _Environment_ for \"prod\", caso contrário criar apenas uma em dev. As condições permitem usar um único template para cenários diferentes (produção vs teste) sem modifica-lo, ativando/desativando partes conforme necessário.\n    \n\nAlém dessas, existem seções menos usadas como **Metadata** (metadados do template), **Description** (descrição do template), **Rules** (para validar combinacões de parâmetros) e **Transform** (para macros e inclusão de macros como AWS::Serverless transform). Não entraremos em detalhes sobre elas neste momento, pois não serão foco em nossos tutoriais iniciais.\n\nEm resumo, **todo template tem, no mínimo, a seção Resources**. As outras seções enriquecem o template com flexibilidade (Parameters, Conditions), saída de informações úteis (Outputs) e lógica auxiliar (Mappings). Nos próximos tutoriais, exploraremos cada uma dessas seções com exemplos práticos.\n\n### Criando sua primeira stack com CloudFormation\n\nVamos agora praticar criando um recurso simples via CloudFormation para ver o processo completo. Faremos um exemplo mínimo: criar um bucket S3 usando um template básico. A ideia é entender a dinâmica de escrever um template e executá-lo.\n\n**Exemplo 1: Template YAML para criar um Bucket S3**\n\nAbaixo está um pequeno template em YAML. Ele contém apenas a seção **Resources**, definindo um recurso do tipo S3 Bucket. Há comentários explicando cada parte do template:\n\n```yaml\n# Template CloudFormation de exemplo - Cria um bucket Amazon S3\nAWSTemplateFormatVersion: '2010-09-09'  # Versão do formato do template (opcional, boa prática incluir)\nDescription: \"Exemplo de template criando um bucket S3\"  # Descrição do template (opcional)\n\nResources:\n  MeuBucket:\n    Type: AWS::S3::Bucket  # Especifica o tipo de recurso AWS (Bucket S3)\n    Properties:\n      # Nenhuma propriedade específica definida.\n      # Sem nome fornecido, o CloudFormation gera um nome único automaticamente.\n```\n\n**Explicação:** No template acima, definimos um recurso chamado `MeuBucket` do tipo `AWS::S3::Bucket`. Não especificamos nenhuma propriedade, então o bucket será criado com configurações padrão. Note que não definimos um nome para o bucket – quando não fornecido, o CloudFormation irá gerar um nome único automaticamente para evitar conflitos de nomes globais em S3. Incluímos também campos opcionais no topo: `AWSTemplateFormatVersion` (que indica a versão do esquema de template, aqui usamos a mais comum `2010-09-09`) e `Description` para documentar o que o template faz.\n\n**Criando a stack na AWS:** Agora que temos o template, vamos criar a stack:\n\n- **Via Console AWS:** Entre no Console AWS e vá para o serviço _CloudFormation_. Clique em **Create stack (Criar pilha)** e escolha **\"With new resources (com novos recursos)\"**. Na seção _Specify template (Especificar template)_, selecione o arquivo YAML do template acima (salve-o como, por exemplo, `bucket.yaml` no seu computador). Prossiga para a página seguinte, dê um nome para a stack (ex: \"StackExemploBucket\"), não há parâmetros neste template para preencher. Na próxima etapa, você pode deixar as opções padrão. Avance até **Review** e então clique em **Create stack (Criar pilha)**. A stack começará a ser criada. Você pode ir acompanhando na coluna de status até aparecer **CREATE_COMPLETE**, indicando sucesso.\n    \n- **Via AWS CLI:** Como alternativa, você pode usar a CLI para criar a stack, já que configuramos as credenciais. Por exemplo, rode:\n    \n    ```shell\n    aws cloudformation create-stack --stack-name StackExemploBucket \\\n      --template-body file://bucket.yaml\n    ```\n    \n    Isso enviará o template para o CloudFormation criar a stack chamada \"StackExemploBucket\". Você pode adicionar `--region <regiao>` se não quiser usar a região padrão configurada. Use `aws cloudformation describe-stacks --stack-name StackExemploBucket` para acompanhar o status ou cheque no Console AWS.\n    \n\nApós a criação, vá ao console S3 e verifique se um novo bucket apareceu. Ele terá um nome gerado automaticamente (algo como `stackexemplobucket-meubucket-1a2b3c4d`). Essa é a **Physical ID** do recurso, gerada pelo CloudFormation, distinta do nome lógico `MeuBucket` que usamos no template. Essa distinção é importante: o nome lógico identifica o recurso dentro do template/stack para referências e dependências, enquanto o nome físico é o identificador real no serviço AWS. Você pode ver ambos no console do CloudFormation, na aba **Resources** dentro da stack, listando o recurso com seu Logical ID e Physical ID.\n\nParabéns, você criou sua primeira infraestrutura através de um template CloudFormation! 🎉 Vimos como um template simples se traduz em um recurso real na nuvem. Nos próximos tutoriais, construiremos sobre esses conceitos, adicionando parâmetros, outputs e outros recursos ao template para explorar mais funcionalidades do CloudFormation."
  },
  {
    "id": "25e43b39-c9d0-4fda-b38a-5d79d76b7825",
    "title": "Parametrização",
    "description": "Uso da seção Parameters para criar templates flexíveis e reutilizáveis, permitindo configurar valores como tipo de instância EC2, nomes de recursos e configurações sem editar o código.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Ambiente",
      "Fundamentos"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/25e43b39-c9d0-4fda-b38a-5d79d76b7825",
    "markdown": "Nesta seção, vamos abordar a **Parametrização** em CloudFormation, ou seja, como tornar templates mais dinâmicos e reutilizáveis usando **Parameters**. Parâmetros permitem que você personalize a instância de uma stack sem alterar o código do template – valores diferentes podem ser passados em cada criação/atualização da stack. Isso é essencial para não precisar duplicar templates para cenários diferentes.\n\n### Por que usar parâmetros?\n\nSem parâmetros, todos os valores dentro de um template são fixos. Parâmetros introduzem flexibilidade. Com eles, você consegue:\n\n- Reutilizar o mesmo template em contextos diferentes passando valores customizados. Por exemplo, um único template pode servir para dev e produção, bastando fornecer parâmetros distintos (tamanho de servidor, quantidade de nós, etc.) em cada ambiente.\n- **Evitar duplicação de templates:** sem parâmetros, você talvez mantivesse vários templates quase idênticos diferindo apenas em configurações menores (por exemplo, um template para cada região ou para cada tamanho de VM). Com parâmetros, um template generalizado cobre todos os casos.\n- **Facilitar implantações automatizadas:** em pipelines CI/CD, você pode definir os valores dos parâmetros para cada deploy (por exemplo, usar uma instância menor nos testes e maior em produção), sem editar o template em si.\n- **Compartilhar templates publicamente ou internamente:** outras pessoas podem usar seu template passando valores apropriados. Por exemplo, um template publicado pela AWS no GitHub pode pedir como parâmetro um **KeyName** para acesso SSH, ao invés de ter um valor fixo que não funcionaria para todos.\n\nEm resumo, parâmetros tornam o template **personalizável e adaptável** a múltiplos cenários, aumentando sua vida útil e abrangência.\n\n### Definindo parâmetros no template\n\nParâmetros são definidos em uma seção **Parameters** no template. Cada parâmetro tem um nome lógico (ID), um tipo e opcionalmente metadados como descrição e valores padrão ou permitidos. A sintaxe geral em YAML é assim:\n\n```yaml\nParameters:\n  NomeDoParametro:\n    Type: DataType\n    Description: Texto explicativo (opcional)\n    Default: valor_padrao (opcional)\n    AllowedValues: [opcional lista de valores permitidos]\n    AllowedPattern: <regex opcional para validar formato>\n    MinLength:  ... (opcionais para Strings)\n    MaxLength:  ... \n    MinValue:   ... (opcionais para Numbers)\n    MaxValue:   ... \n    ConstraintDescription: \"Mensagem de erro personalizada se violar constraints\"\n```\n\nO único campo obrigatório para cada parâmetro é **Type**, que indica o tipo de dado esperado. Os tipos básicos são **String** (cadeia de caracteres), **Number** (número) e **List<type>** (lista de valores separados por vírgula). Além desses, o CloudFormation define **AWS-specific parameter types** – tipos especiais que correspondem a identificadores de recursos AWS existentes (veremos a seguir).\n\nVamos aplicar isso em nosso exemplo prático evoluindo o template anterior. Suponha que queremos permitir definir o **nome do bucket S3** em vez de aceitar um nome gerado automaticamente. Podemos introduzir um parâmetro para o nome do bucket. Além disso, vamos também parametrizar o tipo da instância EC2 (que adicionaremos no próximo tutorial), para já praticar com múltiplos parâmetros.\n\n**Exemplo 2: Adicionando parâmetros**\n\nVamos expandir o template do bucket para incluir parâmetros de _BucketName_ e _InstanceType_, e então usar um deles no recurso. O trecho abaixo mostra a seção de Parameters e como referenciar o parâmetro no recurso:\n\n```yaml\nParameters:\n  BucketName:\n    Description: \"Nome único para o bucket S3 (deve ser globalmente único)\"\n    Type: String\n    MinLength: 3\n    MaxLength: 63\n    AllowedPattern: \"^[a-z0-9-]+$\"\n    ConstraintDescription: \"O nome pode conter letras minúsculas, números e hifens.\"\n    # Nenhum Default definido => tornará obrigatório fornecer um valor na criação da stack.\n\n  InstanceType:\n    Description: \"Tipo da instância EC2\"\n    Type: String\n    Default: t2.micro\n    AllowedValues:\n      - t2.micro\n      - t3.micro\n      - t3.small\n    ConstraintDescription: \"Deve ser um tipo de instância válido e disponível.\"\n\nResources:\n  MeuBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: !Ref BucketName  # Usa o valor fornecido pelo parâmetro como nome do bucket\n```\n\n**Explicação:** Definimos dois parâmetros:\n\n- **BucketName** do tipo String, sem valor default (logo, obrigatório). Incluímos algumas restrições: tamanho mínimo 3 e máximo 63 (limites dos nomes de bucket S3) e um padrão regex que exige apenas letras minúsculas, dígitos e hifens (padrão de nomes S3). A propriedade **ConstraintDescription** fornece uma mensagem amigável caso o usuário insira um valor inválido. Esse parâmetro será usado para dar nome ao bucket S3. **Atenção:** nomes de bucket S3 são globais, então o valor passado **deve ser único em toda AWS** (não apenas na sua conta). Escolha um nome pouco provável (por exemplo, incluindo seu nome ou algo exclusivo) para evitar conflitos.\n    \n- **InstanceType** do tipo String também, mas aqui colocamos um Default (**t2.micro**) e uma lista de AllowedValues. Isso significa que se o usuário não especificar nada, o tipo de instância será **t2.micro**. Se quiser customizar, só poderá escolher entre **t2.micro**, **t3.micro** ou **t3.small** no nosso exemplo. Usamos poucos valores apenas para demonstração; na prática, você não precisaria listar todos os tipos possíveis, poderia permitir qualquer valor ou usar um tipo específico da AWS (veremos adiante). A ideia é mostrar como restringir opções e definir padrão.\n    \n\nNo recurso **MeuBucket** (um S3 Bucket), adicionamos a propriedade **BucketName** e usamos **!Ref BucketName**. **!Ref** (abreviação de função intrínseca Ref) retorna o valor do parâmetro _BucketName_ fornecido pelo usuário. Assim, o nome do bucket criado será exatamente o valor passado para o parâmetro na hora da criação da stack.\n\n> **Dica:** A função intrínseca **Ref** serve para referenciar o valor de um parâmetro ou o identificador de um recurso. No caso de parâmetros, **Ref** retorna o valor inserido. No caso de recursos, **Ref** retorna o ID físico primário do recurso (por exemplo, para AWS::S3::Bucket retornaria o nome do bucket; para AWS::EC2::Instance, retorna o Instance ID). Usaremos **Ref** e outras funções intrínsecas conforme avançamos nos exemplos.\n\n**Criando a stack com parâmetros:** Ao criar ou atualizar uma stack com esse template, o CloudFormation irá solicitar valores para **BucketName** (já que não tem default) e **InstanceType** (opcional, tem default). No Console AWS, ao chegar na tela \"Specify stack details (Especificar detalhes da pilha)\", você verá campos para inserir cada parâmetro com suas descrições. No caso do **InstanceType**, terá até um menu suspenso com as opções permitidas (t2.micro, t3.micro, etc), já que definimos AllowedValues. Isso facilita a seleção e previne erros de digitação. Para **BucketName**, você terá um campo livre, mas se digitar algo fora do padrão (como letras maiúsculas), o CloudFormation mostrará um erro com a **ConstraintDescription** fornecida quando tentar criar.\n\nSe usar a CLI, você deve passar os parâmetros via linha de comando. Exemplo usando o AWS CLI:\n\n```shell\naws cloudformation create-stack --stack-name StackComParams \\\n  --template-body file://template-com-parametros.yaml \\\n  --parameters ParameterKey=BucketName,ParameterValue=meu-bucket-unico-123 \\\n               ParameterKey=InstanceType,ParameterValue=t3.micro\n```\n\nNo comando acima, passamos dois **ParameterKey** com seus **ParameterValue** correspondentes. A CLI então envia esses valores para o CloudFormation usar durante a criação.\n\n**AWS-Specific Parameter Types:** No exemplo usamos parâmetros do tipo **String**. Porém, o CloudFormation disponibiliza tipos especiais para facilitar a entrada de recursos existentes. Por exemplo, ao invés de pedir um texto livre para o nome de uma Key Pair para acesso SSH, você pode declarar o parâmetro com Type **AWS::EC2::KeyPair::KeyName**. Assim, ao criar a stack, o console AWS **listarÁ automaticamente** as Key Pairs válidas da sua conta para seleção, evitando erros de digitação e garantindo que o valor existe. Existem vários AWS::... Parameter Types, como VPC IDs (**AWS::EC2::VPC::Id**), Subnet IDs, Security Group IDs, etc. Esses tipos ajudam a capturar _IDs de recursos existentes_ de forma amigável: o console preenche as opções e valida que o ID existe na conta/região. No nosso caso, poderíamos definir **InstanceType** como **AWS::EC2::InstanceType** (se existisse este tipo) para listar automaticamente tipos disponíveis, mas esse tipo específico não existe. Em vez disso, usamos AllowedValues manual. Fique ciente dessa funcionalidade – por exemplo, quando fizermos um parâmetro de KeyName, usaremos **AWS::EC2::KeyPair::KeyName** para ajudar o usuário.\n\nAgora temos um template parametrizado. Se implantarmos este template (certificando-se de fornecer um BucketName único), o bucket terá o nome escolhido e teremos flexibilidade para escolher outros tipos de instância no futuro (a máquina EC2 em si adicionaremos no tutorial de **Recursos** a seguir).\n\n**Recapitulando:**\n\n- Declaramos parâmetros em **Parameters** com Type e outras restrições.\n- **Referenciamos** parâmetros dentro de **Resources** (ou Outputs, etc.) usando **!Ref NomeDoParametro**.\n- Parâmetros permitem que quem cria a stack insira valores, tornando o template adaptável.\n- Evite usar parâmetros para informações sensíveis (como senhas) sem necessidade absoluta, pois eles podem aparecer em logs ou outputs; para senhas, o ideal é usar serviços como AWS Secrets Manager ou Parameter Store com tipos seguros.\n\nNos próximos tutoriais, continuaremos a evoluir nosso template – vamos adicionar mais **Recursos** e ver como utilizar esses parâmetros e novos recursos na prática."
  },
  {
    "id": "70600cf9-b332-41c9-a2ed-eeb74a7ab512",
    "title": "Recursos",
    "description": "Estrutura da seção Resources, onde os recursos AWS são declarados. Aprendemos a definir instâncias EC2, buckets S3, bancos de dados e a referenciá-los corretamente dentro do template.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/70600cf9-b332-41c9-a2ed-eeb74a7ab512",
    "markdown": "A seção **Resources** é o núcleo de qualquer template CloudFormation. É onde declaramos os **recursos AWS** que queremos que o CloudFormation crie e gerencie. Cada recurso é definido por um tipo (por exemplo, **AWS::S3::Bucket** para um bucket S3, **AWS::EC2::Instance** para uma instância EC2) e um conjunto de propriedades específicas daquele recurso. Nesta parte, vamos aprofundar na sintaxe de recursos, adicionar novos recursos ao nosso exemplo e entender como eles interagem.\n\n### Entendendo a definição de recursos\n\nCada recurso em um template possui três atributos principais:\n\n- **Logical ID (Nome lógico):** É o identificador usado dentro do template para referenciar o recurso. Pode ser qualquer string alfanumérica sem espaços (diferente de todos os outros recursos). Por exemplo, usamos **MeuBucket** e poderemos usar **MinhaInstancia**. Esse nome não é enviado para a AWS; ele serve apenas no escopo do template/stack.\n    \n- **Type (Tipo de recurso):** Especifica o serviço e tipo do recurso que queremos criar. A sintaxe é sempre **AWS::<Produto>::<Tipo>**. Por exemplo: **AWS::EC2::Instance**, **AWS::S3::Bucket**, **AWS::DynamoDB::Table**. A AWS fornece [documentação de referência](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html) para cada tipo, listando quais propriedades são suportadas e quais são obrigatórias.\n    \n- **Properties (Propriedades):** São os parâmetros específicos daquele recurso que você pode configurar. Variam conforme o Type. Por exemplo, uma instância EC2 possui propriedades como InstanceType, ImageId, KeyName, SecurityGroups, etc., enquanto um bucket S3 tem propriedades como BucketName, AccessControl, VersioningConfiguration. Na definição do recurso, você lista as propriedades relevantes como chaves YAML aninhadas sob o recurso.\n    \n\nAlém disso, recursos podem ter metadados adicionais ou atributos substitutos:\n\n- **DependsOn:** um campo opcional que você pode usar para forçar a criação de um recurso após outro, caso o CloudFormation não consiga deduzir automaticamente a dependência. Geralmente, se um recurso refere outro (via **Ref** ou outras funções), a dependência é implícita e você não precisa usar DependsOn.\n- **DeletionPolicy:** uma propriedade especial que não faz parte das propriedades \"normais\" do recurso, mas você pode adicionar para controlar o que acontece ao **deletar a stack**. Ex: **DeletionPolicy: Retain** em um bucket S3 impede que ele seja deletado automaticamente junto com a stack (útil para preservar dados).\n- **UpdatePolicy/UpdateReplacePolicy:** controlam comportamentos em atualizações, dependendo do recurso.\n\nVamos manter nosso foco no essencial: definir recursos com propriedades básicas.\n\n### Adicionando uma instância EC2 ao template\n\nNo template em construção, já temos um recurso S3 Bucket (**MeuBucket**). Agora, vamos adicionar uma instância EC2 para simular uma pequena aplicação web, por exemplo. Nossa instância EC2 terá:\n\n- Um tipo configurável (usaremos o parâmetro **InstanceType** já definido).\n- Uma AMI (imagem de máquina) específica para a região. Usaremos, por enquanto, um ID de AMI estático válido para nossa região (N. Virgínia, us-east-1) e posteriormente melhoraremos isso com Mappings.\n- Sem par de chaves (KeyName) e security groups customizados por enquanto, para simplificar (a instância ficará no default VPC e default security group). \n\n**Observação:** Sem um KeyName, não será possível acessar via SSH essa instância, mas nosso propósito aqui é apenas demonstrar a criação. Se quiser acessar a instância, você pode criar um Key Pair na AWS e adicionar uma propriedade KeyName referenciando-o via parâmetro.\n\nVamos incluir o recurso EC2 no template. Segue o trecho relevante:\n\n```yaml\nResources:\n  MeuBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: !Ref BucketName\n\n  MinhaInstancia:\n    Type: AWS::EC2::Instance\n    Properties:\n      InstanceType: !Ref InstanceType  # utiliza o parâmetro InstanceType para o tamanho da instância\n      ImageId: ami-0ff8a91507f77f867   # ID da AMI Amazon Linux (HVM 64 bits) para us-east-1\n      # KeyName: !Ref KeyNameParam    # (opcional) se desejar SSH, adicione um parâmetro de KeyName e referencia aqui\n```\n\n**Explicação:** Adicionamos um recurso lógico chamado **MinhaInstancia** do tipo **AWS::EC2::Instance**. Nas propriedades, definimos:\n\n- **InstanceType** como **!Ref InstanceType** para usar o valor do parâmetro que já configuramos (por padrão, t2.micro, ou outro se especificado). Isso torna fácil mudar o tamanho da VM.\n    \n- **ImageId** com um valor fixo **ami-0ff8a91507f77f867**. Este é o ID de uma Amazon Machine Image para Amazon Linux 2 (HVM, x86_64) na região **us-east-1**. **Importante:** AMI IDs variam por região. Essa AMI exata existe em us-east-1; em outras regiões, o ID será diferente. Ou seja, se você tentar usar este template em **sa-east-1** (São Paulo) ou **eu-west-3** (Paris), por exemplo, ele falhará dizendo que a AMI não foi encontrada. Por enquanto, assumiremos que estamos implantando em us-east-1. No **Tutorial 6 (Mapeamentos)** vamos solucionar esse problema permitindo que o template escolha a AMI correta conforme a região automaticamente, mas aqui mantemos fixo para simplicidade.\n    \n- Não definimos **KeyName** ou **SecurityGroups** explicitamente. Sem **SecurityGroups**, a instância será associada ao security group padrão da VPC default (que normalmente permite todo tráfego de saída e nenhum de entrada). E sem **KeyName**, não haverá chave SSH associada. Estamos focando apenas na criação do recurso e suas dependências no CloudFormation, não na sua acessibilidade. Em um cenário real, provavelmente você adicionaria um parâmetro para **KeyName** (usando Type **AWS::EC2::KeyPair::KeyName** para listar as chaves disponíveis) e talvez definisse regras de segurança. Mas isso adicionaria complexidade que fugiria do escopo de fundamentos do CloudFormation – nosso objetivo aqui é entender a mecânica do template.\n    \n\nAgora nosso template tem dois recursos. O CloudFormation, ao criar a stack, identificará automaticamente que não há dependência direta entre o bucket e a instância, então pode tentar criar em paralelo. Isso não é um problema. Se houvesse dependência (por exemplo, se a instância dependesse de algum valor do bucket ou vice-versa), ele garantiria a ordem correta ou você usaria **DependsOn**.\n\n### Referenciando recursos e atributos\n\nUma vantagem de definir recursos no CloudFormation é poder referenciá-los entre si. Por exemplo, se a instância EC2 precisasse do nome do bucket para, digamos, fazer algo no user data script (não vamos cobrir user data aqui), poderíamos passar como metadado usando **Ref MeuBucket**. Como mencionado, **Ref MeuBucket** retornaria o nome físico do bucket criado. Outras funções intrínsecas como **Fn::GetAtt** permitem obter atributos específicos do recurso, caso ele tenha (ex: o bucket S3 tem um atributo **DomainName**, uma instância EC2 tem atributos como **PublicIp** ou **AvailabilityZone**). Veremos **GetAtt** no próximo tutorial de Outputs.\n\nNo nosso exemplo atual, os recursos são independentes, então não usamos nenhuma referência entre eles. Mas é importante saber que se usássemos, CloudFormation entenderia essa **dependência implícita**. Por exemplo, se fizéssemos:\n\n```yaml\nSomeProperty: !Ref MeuBucket\n```\n\nem **MinhaInstancia**, automaticamente o CloudFormation criaria o **MeuBucket** primeiro, pois é referenciado, e só depois a **MinhaInstancia**. Essa gestão automática simplifica muito arquiteturas com múltiplos componentes – você não precisa sequenciar manualmente, apenas conecta as peças e o CloudFormation cuida da ordem.\n\n### Dicas ao definir recursos\n\n- Consulte a documentação do tipo de recurso para saber quais propriedades são obrigatórias. Por exemplo, para **AWS::EC2::Instance**, **InstanceType** e **ImageId** são obrigatórias, mas **KeyName** não (apesar de útil), por isso colocamos as mínimas.\n- Utilize nomes lógicos descritivos e coesos. Isso ajuda a entender o template e, ao ver eventos no CloudFormation, é mais fácil identificar qual recurso deu erro/sucesso. Nomes lógicos como **WebServer** ao invés de **MinhaInstancia** poderiam ser mais semânticos dependendo do caso.\n- Lembre-se que muitos recursos AWS têm limites ou requisitos. Nós colocamos um bucket S3 e uma instância EC2, que não têm uma interdependência. Se você definir algo como uma associação de Elastic IP a instância, por exemplo, você teria que garantir que a instância existe antes de associar o IP (tipicamente CloudFormation sabe pela referência).\n- **Recursos obrigatórios:** Em CloudFormation, a seção Resources deve ter pelo menos um recurso. Se você não tiver nada lá (ex: só parâmetros e outputs), a stack vai falhar pois não tem o que criar. Por outro lado, é possível criar recursos lógicos que não correspondem a recursos físicos dependendo de condições (ver tutorial 7) ou usar transformações macros que geram recursos.\n\n### Atualizando a stack com novo recurso\n\nSe você já criou a stack no tutorial anterior (com apenas o bucket) e quer **atualizá-la** para adicionar a instância EC2, basta aplicar o novo template. No Console CloudFormation, escolha **Update (Atualizar)** na stack existente e forneça o template modificado. O CloudFormation verá que um novo recurso **MinhaInstancia** foi adicionado e criará apenas ele, mantendo o bucket intacto. Esse é o poder do gerenciamento de stack: ele calcula diferenças de estado e executa apenas o necessário (nesse caso, um _Create_ adicional).\n\nSe estiver fazendo do zero, pode criar já com ambos recursos. Lembre de fornecer todos parâmetros necessários (BucketName e InstanceType se quiser diferente do default).\n\nAgora nosso template acumulou várias partes importantes: parâmetros (BucketName, InstanceType), recursos (bucket S3 e instância EC2) e uso de referências (**!Ref**). Estamos progredindo bastante! No próximo tutorial, exploraremos a seção de **Outputs**, que permitirá extrair informações úteis desses recursos (como o ID da instância EC2 criada, ou o nome do bucket) e apresentá-las para uso externo."
  },
  {
    "id": "a3e233b2-822b-4a62-b4f6-61f36fc10f4a",
    "title": "Outputs",
    "description": "Como expor informações importantes sobre os recursos criados na stack, como ID de instância, URL de um Load Balancer ou ARN de um serviço, facilitando integrações e documentações.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/a3e233b2-822b-4a62-b4f6-61f36fc10f4a",
    "markdown": "Nesta etapa, aprenderemos a usar a seção **Outputs** do CloudFormation. Outputs são saídas da stack – você pode pensar neles como \"resultados\" que a stack divulga após ser criada ou atualizada. Eles servem para destacar informações importantes dos recursos criados ou passar dados para outras stacks.\n\n### Por que usar Outputs?\n\n- **Facilitar acesso a informações-chave:** Muitas vezes, os recursos que criamos têm identificadores ou endpoints que precisamos anotar ou reutilizar. Por exemplo, se seu template criou um Bucket S3, você pode querer saber o nome real do bucket; se criou uma instância EC2, pode querer o ID ou IP público; se criou um load balancer, o DNS dele, etc. Em vez de procurar manualmente cada recurso, você pode definir Outputs no template para exibir essas informações de forma organizada. No console do CloudFormation, existe uma aba \"Outputs\" na stack onde esses valores aparecem, facilitando a visualização após a criação.\n    \n- **Referenciar em outras stacks (Cross-stack reference):** Outputs podem ser **exportados** e importados por outras stacks. Suponha que você tenha uma stack de rede (criando VPC, subnets) e quer usar a VPC em outra stack de aplicação. Você pode exportar o ID da VPC na stack de rede e, na stack de aplicação, usar **Fn::ImportValue** para obter esse ID a partir do nome de Export. Isso permite compor infraestruturas em módulos. (Nota: para exportar, você deve fornecer um nome único global na propriedade **Export** do Output). .\n    \n- **Documentação e Debug:** Outputs também funcionam como documentação viva – quem implanta a stack vê claramente quais recursos principais foram criados e seus valores. E durante desenvolvimento, é útil expor valores para conferir se estão corretos (p. ex., ver se uma função Lambda retornou algo ou se uma condicional escolheu o recurso certo).\n    \n\n**Importante:** Evite expor dados sensíveis em Outputs. **Tudo que aparece em Outputs fica visível para quem tiver acesso à stack**. Por exemplo, nunca coloque senhas, chaves privadas ou secrets como output. CloudFormation não oculta esses valores. Para segredos, prefira armazenar no AWS Secrets Manager ou Parameter Store e não exibir no CloudFormation.\n\n### Definindo Outputs no template\n\nA sintaxe da seção Outputs é simples. Podemos declarar até 200 outputs por template, cada um com um Nome lógico, Value e opcionalmente Description e Export. Exemplo básico:\n\n```yaml\nOutputs:\n  OutputName:\n    Description: \"Descrição do valor de saída\"\n    Value: <valor a retornar>\n    Export:\n      Name: <nome_de_exportacao_opcional>\n    Condition: <opcional, só produzir output se condição true>\n```\n\nVamos adicionar alguns outputs ao nosso template para ilustrar. Queremos ver, por exemplo:\n\n- O nome do bucket S3 criado (que no nosso template é dado pelo parâmetro BucketName, então será o valor que o usuário forneceu).\n- O ID da instância EC2 criada.\n- (Opcionalmente) talvez o IP público da instância EC2, para mostrar uso de **Fn::GetAtt**.\n\nAdicionaremos esses outputs e também vamos demonstrar como condicionar outputs (no próximo tutorial, condicionaremos a criação do bucket, e podemos condicionar o output do bucket também). Por ora, vamos adicioná-los sem condições.\n\n**Exemplo 3: Outputs para bucket e instância**\n\n```yaml\nOutputs:\n  BucketNameOutput:\n    Description: \"Nome do bucket S3 criado pela stack\"\n    Value: !Ref MeuBucket   # Ref do bucket retorna seu nome (se BucketName property foi setada, retorna aquele nome)\n    Export:\n      Name: !Sub \"${AWS::StackName}-BucketName\"  # (Opcional) exporta o nome do bucket, usando o nome da stack como parte do export\n\n  InstanceId:\n    Description: \"ID da Instância EC2 criada\"\n    Value: !Ref MinhaInstancia  # Ref da instância EC2 retorna o InstanceId\n    Export:\n      Name: !Sub \"${AWS::StackName}-InstanceId\"\n\n  InstancePublicIP:\n    Description: \"IP público da instância EC2\"\n    Value: !GetAtt MinhaInstancia.PublicIpAddress  # Obtém o atributo IP público da instância\n```\n\n**Explicação:** Definimos três outputs:\n\n- **BucketNameOutput:** Pega o nome do bucket. Usamos **!Ref MeuBucket**. Para recursos do tipo Bucket, **Ref** devolve o nome do bucket. Note que poderíamos também ter usado diretamente o parâmetro **BucketName** (porque no nosso template, o bucket name físico = valor do parâmetro). Mas usar **Ref MeuBucket** é mais geral, pois mesmo que o nome fosse gerado automaticamente (no caso de não termos passado BucketName), ele retornaria o nome gerado. Colocamos também um **Export** com um nome construído dinamicamente usando **!Sub**. Aqui concatenamos o nome da stack (**${AWS::StackName}**) com \"-BucketName\" para formar um nome de exportação único. Isso permitiria outra stack importar pelo mesmo nome. O uso de `!Sub` no Export nos deixa incluir o nome da stack (que é único em sua conta/região) para que o export também seja único (já que exports precisam ser únicos por região/conta). Export é opcional – incluímos para ilustrar, mas se não precisar compartilhar, você pode omiti-lo.\n    \n- **InstanceId:** Similar, com **!Ref MinhaInstancia** para obter o ID da instância EC2 criada. Exportamos também com um nome único. Assim, outra stack poderia importar esse ID se necessário (por exemplo, para criar um alarm no CloudWatch atrelado a essa instância).\n    \n- **InstancePublicIP:** Demonstra uso de **!GetAtt** (Get Attribute). Muitos recursos do CloudFormation possuem atributos consultáveis listados na documentação. Para EC2 Instance, um dos atributos é **PublicIpAddress** (IP público IPv4). Usando **!GetAtt MinhaInstancia.PublicIpAddress**, pegamos esse valor atual após a criação. Lembrando: a instância EC2 só terá PublicIp se estiver em uma subnet com auto-assign public IP habilitado (por padrão, subnets da VPC default sim têm). Então assumindo default VPC, este output retornará o IP público. Caso contrário, pode vir vazio. De qualquer forma, exemplifica como pegar atributos além do ID padrão. Não exportamos esse, é só informativo.\n    \n\nAgora, quando a stack for criada, você poderá ir no CloudFormation, selecionar a stack e ver esses outputs na aba **Outputs**. O nome do bucket será exibido, o InstanceId e o IP. Esses valores também podem ser obtidos via CLI:\n\n```shell\naws cloudformation describe-stacks --stack-name NomeDaStack --query \"Stacks[0].Outputs\"\n```\n\nEsse comando consultaria os outputs via API.\n\n### Considerações sobre Outputs e mudanças\n\nOutputs podem depender de recursos ou parâmetros. Se você tentar atualizar uma stack e remover um output que estava exportado e sendo importado por outra stack, o CloudFormation não permitirá a menos que você remova a importação primeiro – há um acoplamento. Fora isso, outputs são fáceis de adicionar ou remover. No nosso caso, são simples e não afetam a lógica de recursos.\n\nReforçando: não coloque senhas ou segredos nos outputs, pois ficam visíveis em texto plano.\n\nAgora nosso template tem outputs úteis. Ao criar/atualizar a stack com o template contendo outputs, veremos:\n\n- _BucketNameOutput_: exibindo o nome do bucket.\n- _InstanceId_: exibindo o ID da instância.\n- _InstancePublicIP_: exibindo o IP público (se houver).\n\nEsses outputs confirmam que tudo foi criado corretamente e nos dão informações para eventualmente usar fora do CloudFormation (por exemplo, acessar o bucket ou instância). Também mostramos como exportar outputs para reuso.\n\nNo próximo tutorial, vamos explorar **Mapeamentos (Mappings)** para melhorar nosso template, tornando-o ciente de diferenças entre regiões. Especificamente, resolveremos o problema do ID de AMI fixo usando um mapping por região."
  },
  {
    "id": "63a64568-78d9-45ae-b448-ca9188480966",
    "title": "Mapeamentos",
    "description": "Uso da seção Mappings para definir tabelas de configuração que variam por região, ambiente ou tipo de recurso, garantindo valores adequados para cada contexto sem a necessidade de parâmetros.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/63a64568-78d9-45ae-b448-ca9188480966",
    "markdown": "Nesta seção, vamos apresentar a funcionalidade de **Mappings** (Mapeamentos) nos templates CloudFormation. Mappings permitem definir dados estáticos no template como se fossem tabelas de consulta, para usar posteriormente com base em alguma chave. Eles são úteis quando certos valores precisam variar de acordo com a região, ambiente ou outro critério, e você não quer expor isso via parâmetros ao usuário (até porque poderiam ser muitos valores ou detalhes internos). Com mappings, a lógica de seleção do valor apropriado fica dentro do template.\n\n### O que são Mappings e quando usá-los?\n\nUm Mapping é basicamente uma lista de pares chave-valor aninhados. Você pode imaginá-lo como um dicionário de dois níveis. Você fornece duas chaves para buscar um valor: uma chave de primeiro nível e uma de segundo nível. Por exemplo, um mapping clássico é **RegionMap** que mapeia regiões para algum valor específico daquela região:\n\n- Chave de primeiro nível: nome da região (ex: \"us-east-1\")\n- Chave de segundo nível: algum atributo ou categoria (ex: \"AMI\")\n- Valor: por exemplo, o ID da AMI daquela região.\n\nUma vez definido um mapping, usamos a função intrínseca **Fn::FindInMap** (ou em YAML a forma curta **!FindInMap**) para recuperar o valor, dado um mapping name e duas keys.\n\n**Quando usar:**\n\n- **Variações por Região:** AMIs, por exemplo, diferem por região. Em vez de pedir ao usuário para fornecer a AMI correta via parâmetro (o que exige que ele saiba ou calcule isso), você pode manter internamente um mapping **Region -> AMI**. Assim, ao criar em us-west-2 versus us-east-1, o template automaticamente pega a AMI adequada.\n- **Variações por Ambiente:** Suponha que sua infra em Dev use instâncias menores ou menos nós que em Prod. Você poderia ter um mapping `Environment -> { param1: X, param2: Y }` e escolher valores conforme um parâmetro Environment. Em alguns casos, Conditions podem fazer algo parecido; mapeamentos são úteis se há muitas variações combinatórias.\n- **Tabelas de constantes:** Qualquer conjunto de valores fixos que você não quer recalcular. Exemplo: taxas de cobrança ou IDs de ARNs fixos por região. Mappings deixam o template autocontido com esses dados.\n\n**Diferença de Parameters vs Mappings:** Parâmetros expõem escolhas para quem cria a stack; Mappings mantêm as diferenças encapsuladas no template. Use parâmetros para entradas que o usuário deve controlar (como nomes, contagens, tipos). Use mappings quando o template pode decidir sozinho com base em informações conhecidas (região, ambiente) sem incomodar o usuário.\n\n### Sintaxe de Mappings\n\nA seção **Mappings** no template é escrita assim (YAML):\n\n```yaml\nMappings:\n  NomeDoMapping:\n    ChavePrimaria1:\n      ChaveSecundaria1: Valor\n      ChaveSecundaria2: Valor\n    ChavePrimaria2:\n      ChaveSecundaria1: Valor\n      ChaveSecundaria2: Valor\n```\n\nNão há limite prático muito baixo (são suportados até centenas de entradas). Você **não pode** usar funções intrínsecas ou referências dentro dos Mappings – eles são estáticos. As chaves devem ser strings literais (não podem ser via Ref). Os valores podem ser strings ou listas de strings.\n\nVamos aplicar isso ao nosso exemplo. O problema identificamos: o ID da AMI para a instância EC2 varia por região. Nós fixamos para us-east-1, mas e se quisermos permitir criar a stack em outras regiões sem exigir que o usuário descubra o ID correto? Podemos inserir um Mapping com mapeamento de região para AMI.\n\nA AWS geralmente publica IDs de AMIs para Amazon Linux 2 e outros. Para demonstração, usaremos alguns valores conhecidos (mesmos do exemplo da documentação do CloudFormation). Vamos mapear algumas regiões para seus AMIs Amazon Linux 2 HVM EBS General Purpose:\n\nDefiniremos um mapping chamado **RegionMap**. As chaves de primeiro nível serão códigos de regiões AWS (us-east-1, us-west-1, eu-west-1, ap-southeast-1, ap-northeast-1 por exemplo). Como queremos mapear para um ID de AMI, podemos usar a segunda chave fixa, digamos \"AMI\". (Também poderíamos usar o esquema do exemplo AWS de ter \"HVM64\" e \"HVMG2\" para diferenciar AMIs de instâncias normais vs GPU, mas para simplificar vamos assumir só um tipo).\n\n**Exemplo 4: Definindo um Mapping para AMIs por região**\n\n```yaml\nMappings:\n  RegionMap:\n    us-east-1:\n      AMI: \"ami-0ff8a91507f77f867\"   # Amazon Linux 2 em N. Virginia\n    us-west-1:\n      AMI: \"ami-0bdb828fd58c52235\"   # Amazon Linux 2 em N. California\n    eu-west-1:\n      AMI: \"ami-047bb4163c506cd98\"   # Amazon Linux 2 em Irlanda\n    ap-southeast-1:\n      AMI: \"ami-08569b978cc4dfa10\"   # Amazon Linux 2 em Singapura\n    ap-northeast-1:\n      AMI: \"ami-06cd52961ce9f0d85\"   # Amazon Linux 2 em Tóquio\n```\n\n**Explicação:** Criamos **RegionMap** com 5 regiões e a respectiva AMI (64-bit HVM ebs-ssd) para Amazon Linux 2 em cada uma. Esses IDs foram obtidos de referência pública de AMIs comuns. Note que:\n\n- Se a região onde você rodar não estiver listada, não haverá correspondência e o CloudFormation dará erro ao tentar fazer FindInMap. Listamos algumas das principais. Você pode adicionar outras se quiser suportar mais regiões (por ex, sa-east-1, us-east-2, etc) – para fins de exemplo mantivemos 5.\n- As keys como \"us-east-1\" **devem** corresponder exatamente ao valor do pseudo-parâmetro **AWS::Region** daquela região. Felizmente, em CloudFormation existe o pseudo parâmetro **AWS::Region** que sempre contém a região atual da stack. Usaremos ele para buscar a linha correta.\n\nAgora, para usar esse mapping, precisamos alterar a propriedade **ImageId** da nossa instância para não ser fixa e sim referenciar o mapping. Em YAML, usamos a função intrínseca **Fn::FindInMap** ou seu atalho **!FindInMap** fornecendo uma lista: **[ NomeDoMapping, ChavePrimaria, ChaveSecundaria ]**.\n\nQueremos: Map = RegionMap, PrimaryKey = (valor de AWS::Region), SecondaryKey = \"AMI\".\n\nA sintaxe curta YAML:\n\n```yaml\nImageId: !FindInMap [RegionMap, !Ref \"AWS::Region\", AMI]\n```\n\nExplicação: **!Ref \"AWS::Region\"** retorna automaticamente a região de execução (ex: \"us-west-2\"). Com isso, **FindInMap** vai pegar **RegionMap[us-west-2][\"AMI\"]** e nos dar o ID correspondente. Se a região não existir no map, erro; se existir, retorna string.\n\nVamos aplicar ao nosso recurso:\n\n```yaml\nMinhaInstancia:\n  Type: AWS::EC2::Instance\n  Properties:\n    InstanceType: !Ref InstanceType\n    ImageId: !FindInMap [RegionMap, !Ref \"AWS::Region\", AMI]\n```\n\nAgora, nosso template está preparado para escolher a AMI correta automaticamente. Se você rodar a stack em **us-east-1**, **!Ref AWS::Region** será \"us-east-1\" e o mapping retornará **ami-0ff8a91507f77f867**. Em **eu-west-1**, pegará **ami-047bb4163c506cd98**. Assim não precisamos criar um parâmetro para AMI nem fixar via código.\n\n### Revisão do template até aqui\n\nAté este ponto, temos incorporado:\n\n- **Parameters:** BucketName e InstanceType (e possivelmente KeyName se quisermos, mas não implementamos ainda).\n- **Mappings:** RegionMap para AMIs.\n- **Resources:** MeuBucket (S3) e MinhaInstancia (EC2), usando os parâmetros e mapping (ImageId agora usando RegionMap).\n- **Outputs:** Nome do bucket, ID e IP da instância, etc.\n\nNosso template está muito mais robusto pois suporta múltiplas regiões de forma transparente ao usuário e está bem parametrizado.\n\n### Quando não usar Mappings\n\nVale observar: com o aprimoramento dos serviços AWS, em alguns casos Mappings tradicionais foram substituídos por alternativas:\n\n- No caso de AMIs, a AWS agora possibilita usar **SSM Parameter Store public parameters** para obter a AMI mais recente de determinado sistema operacional. Inclusive a própria documentação menciona que ao invés de usar mapping fixo de AMI, poderíamos usar o tipo de parâmetro **AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>** apontando para um alias, garantindo sempre a image mais atual. Isso elimina a necessidade de atualizar manualmente o template quando sai AMI nova. Porém, entender Mappings continua importante porque nem tudo está disponível via SSM Parameters, e muitos templates legados usam.\n- Em muitos casos, você pode usar lógica condicional com menos chave, ou até script externo. Mappings brilham quando há uma matriz estática de combinações.\n\n### Testando o Mapping\n\nPara testar, tente criar/atualizar a stack em uma região diferente. Se você criou antes em us-east-1, experimente rodar em eu-west-1 sem mudar nada no template (exceto talvez o BucketName para ser único global). A instância EC2 deverá ser criada usando a AMI daquela região (no caso ami-047bb4163c506cd98, que é Amazon Linux 2 Ireland). Você pode confirmar no console EC2 > Instances, cheque o Ami ID ou nome da AMI utilizada.\n\nNos eventos da stack, você verá o CloudFormation usando **FindInMap** – internamente ele já resolve antes de chamar a API.\n\nIsso conclui o uso de Mappings. Agora nosso template está capaz de se adaptar à região fornecendo o AMI correto. Nos outputs, o InstanceId continua válido e refletirá a instância naquela região. O IP público também.\n\nNo próximo tutorial, aprenderemos sobre **Condições (Conditions)**, que nos permitirão criar ou não certos recursos com base em parâmetros. Vamos implementar uma condição para criar o bucket S3 somente em ambientes específicos, por exemplo, e ver como ajustar outputs e recursos conforme essa lógica condicional."
  },
  {
    "id": "6a882d36-409a-413b-82a5-bd59338ac108",
    "title": "Condições",
    "description": "Implementação de lógica condicional nos templates. Aprendemos a habilitar ou omitir recursos, propriedades e outputs dependendo de variáveis como ambiente (produção vs teste) ou requisitos específicos.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/6a882d36-409a-413b-82a5-bd59338ac108",
    "markdown": "Agora focaremos nas **Condições (Conditions)** nos templates CloudFormation. Condições permitem incluir lógica booleana no template para decidir se determinados recursos ou outputs devem ser criados ou definidos, dependendo de valores de parâmetros ou pseudo-parâmetros. Isso é extremamente útil para escrever um único template que sirva para cenários variados (ex.: ambiente de teste vs produção) sem criar tudo em todos os casos.\n\n### O que são e por que usar Condições?\n\nUma condição avalia a **verdadeiro ou falso** no momento da criação ou atualização da stack, e com base nisso o CloudFormation pode:\n\n- **Criar ou ignorar um recurso**: Recursos associados a uma condição só serão criados se a condição for verdadeira; se for falsa, o CloudFormation _ignora_ aquele recurso (como se não existisse no template).\n- **Incluir ou omitir valores em propriedades**: Com a função intrínseca **Fn::If**, você pode definir que certa propriedade tome um valor X se condição verdadeira ou Y se falsa.\n- **Controlar outputs condicionalmente**: Outputs também podem ter o campo **Condition**, de modo que só serão emitidos se a condição for verdadeira.\n\nCenários típicos:\n\n- **Ambientes diferentes (dev/prod)**: No template você pode ter recursos de alta disponibilidade (como múltiplas instâncias, ou uma RDS de grande porte) e usar condições para só criá-los quando um parâmetro Environment for \"prod\". Em \"dev\", aqueles recursos não são criados, economizando custos.\n- **Recursos opcionais**: Talvez você queira um template que _opcionalmente_ crie um recurso, controlado por um parâmetro booleano (ex: **CreateCache = true/false** decide se cria um cluster ElastiCache).\n- **Customizações regionais/conta**: Às vezes um serviço não existe em certa região, você pode condicionar para não tentar criar lá.\n\nSem condições, esses casos exigiriam manter templates separados ou instruir usuários a habilitar/desabilitar coisas manualmente. Com condições, um template único se adapta.\n\n### Definindo condições\n\nAs condições são definidas na seção **Conditions** do template. Lá, cada condição tem um nome lógico e uma expressão booleana usando funções intrínsecas lógicas (ex: **Fn::Equals**, **Fn::And**, **Fn::Or**, **Fn::Not**). CloudFormation avalia todas as condições no início da criação/atualização da stack, com base nos parâmetros fornecidos e nos pseudo-parâmetros disponíveis.\n\nSintaxe exemplar:\n\n```yaml\nConditions:\n  IsProd: !Equals [ !Ref EnvironmentType, \"prod\" ]\n  CreateCacheCluster: !And \n    - !Equals [ !Ref CacheEnabled, \"true\" ]\n    - !Not [ !Equals [ !Ref EnvironmentType, \"test\" ] ]\n```\n\nAqui supomos existir parâmetros **EnvironmentType** e **CacheEnabled**. **IsProd** será true se **EnvironmentType** for \"prod\". Já **CreateCacheCluster** seria true se CacheEnabled for \"true\" **e** o ambiente não for \"test\", por exemplo.\n\nAs funções lógicas disponíveis:\n\n- **Fn::Equals( A, B )**: retorna true se A == B.\n- **Fn::And( cond1, cond2, ... )**: true se todas verdadeiras.\n- **Fn::Or( cond1, cond2, ... )**: true se pelo menos uma verdadeira.\n- **Fn::Not( cond )**: true se cond for falsa.\n- (Há também **Fn::If**, mas este é usado inline em propriedades/outputs, não dentro de Conditions section.)\n\nVocê pode também referenciar valores diretamente (como no exemplo, com !Ref de parâmetros).\n\n### Aplicando condições no nosso exemplo\n\nVamos introduzir um parâmetro chamado **EnvironmentType** que pode ser \"dev\" ou \"prod\". Com isso, faremos:\n\n- Uma condição **IsProd** que verifica se **EnvironmentType** == \"prod\".\n- Usaremos essa condição para decidir se o bucket S3 deve ser criado. Imaginemos que em ambiente de produção queremos um bucket para armazenar logs, mas em dev não é necessário.\n- Também usaremos a condição nos Outputs relacionados ao bucket, para não listar output de bucket quando ele não for criado.\n\nPassos:\n\n1. Adicionar o parâmetro **EnvironmentType**.\n2. Adicionar a condição **CreateBucketCondition** (por exemplo) usando **Fn::Equals** com \"prod\".\n3. Adicionar o campo **Condition: CreateBucketCondition** no recurso do Bucket e no output do Bucket.\n4. Opcional: poderíamos usar Fn::If para algum valor, mas neste caso não precisamos – estamos simplesmente incluindo/excluindo o recurso.\n\n**Atualização do template com Environment e Condition:**\n\n```yaml\nParameters:\n  EnvironmentType:\n    Description: \"Tipo de ambiente (dev ou prod)\"\n    Type: String\n    Default: dev\n    AllowedValues:\n      - dev\n      - prod\n\nConditions:\n  CreateBucketCondition: !Equals [ !Ref EnvironmentType, \"prod\" ]\n\nResources:\n  MeuBucket:\n    Type: AWS::S3::Bucket\n    Condition: CreateBucketCondition  # Só cria o bucket se for ambiente prod\n    Properties:\n      BucketName: !Ref BucketName\n  MinhaInstancia:\n    Type: AWS::EC2::Instance\n    Properties:\n      InstanceType: !Ref InstanceType\n      ImageId: !FindInMap [RegionMap, !Ref \"AWS::Region\", AMI]\n\nOutputs:\n  BucketNameOutput:\n    Description: \"Nome do bucket S3 criado pela stack\"\n    Value: !Ref MeuBucket\n    Condition: CreateBucketCondition  # Só aparece se bucket foi criado\n    Export:\n      Name: !Sub \"${AWS::StackName}-BucketName\"\n  InstanceId:\n    Description: \"ID da Instância EC2 criada\"\n    Value: !Ref MinhaInstancia\n    Export:\n      Name: !Sub \"${AWS::StackName}-InstanceId\"\n  InstancePublicIP:\n    Description: \"IP público da instância EC2\"\n    Value: !GetAtt MinhaInstancia.PublicIpAddress\n```\n\n**Explicação das alterações:**\n\n- **Parâmetro EnvironmentType:** Permitimos \"dev\" ou \"prod\", padrão \"dev\". Assim, se o usuário não especificar, assumimos dev (não criar bucket). Em casos de uso real, poderíamos ter mais nuance, mas isso basta.\n    \n- **Condition CreateBucketCondition:** Usa **Fn::Equals** (forma YAML **!Equals**) para comparar o valor de EnvironmentType com \"prod\". Se for igual, condição verdadeira; se diferente (ex: \"dev\"), condição falsa. É uma condição simples boolean.\n    \n- **Usando a condição no recurso S3:** Adicionamos **Condition: CreateBucketCondition** ao recurso **MeuBucket**. Isso instruirá o CloudFormation a **criar esse recurso somente se a condição for true (prod)**. Caso contrário, se for dev, ele pulará a criação do bucket. Internamente, é como se o recurso não existisse para fins de criação. (Observação: se a condição for falsa, você _não paga_ nem nada pelo recurso, ele realmente não é criado.)\n    \n- **Usando a condição no Output do bucket:** Colocamos a mesma condição no output **BucketNameOutput**. Assim, em dev (quando nenhum bucket foi criado), não aparecerá um output vazio. Em produção, o output aparecerá com o nome do bucket. Isso é importante: se tentássemos referenciar **!Ref MeuBucket** em output sem condicionar, e o bucket não fosse criado, o CloudFormation teria um problema (referência a recurso inexistente). Então devemos condicionar ambos de forma consistente.\n    \n- Decidimos não condicionar o output InstanceId/IP porque a instância criamos sempre (independente do ambiente nesse exemplo). Mas poderíamos, se quiséssemos que em dev nem instância houvesse, por exemplo.\n    \n\n**Efeito das condições na prática:**\n\n- Quando **EnvironmentType=dev**: **CreateBucketCondition** = false. CloudFormation, ao criar a stack, _ignora_ o recurso MeuBucket. Ele não tentará criá-lo. Apenas criará a instância. Nos outputs, pulará o BucketNameOutput. Assim, a stack resultante não terá bucket S3 e nem output relacionado a bucket. Mais tarde, se você atualizar a stack trocando EnvironmentType para \"prod\", o CloudFormation perceberá que agora a condição virou true e **criará** o bucket S3 durante a atualização (e passará a mostrar o output).\n- Quando **EnvironmentType=prod**: **CreateBucketCondition** = true. CloudFormation criará tanto instância quanto bucket. Outputs incluirão ambos. Se depois você atualiza para \"dev\", ele vai ver que a condição virou false e **deletará o bucket** automaticamente durante a atualização, porque o recurso agora está associado a uma condição falsa. Isso é importante: CloudFormation gerencia a existência contínua conforme as condições. Recurso que deixa de atender condição é removido (a menos que você proteja com políticas de atualização, etc).\n\nEssa última parte vale destacar: se você mudar uma condição de modo que um recurso existente passe a não ser mais necessário, o CloudFormation **vai apagá-lo** na atualização da stack. Portanto, use condições com cautela se há dados associados. No nosso exemplo, se o bucket tivesse objetos e você mudasse para dev, ele tentaria deletar o bucket – se tiver objetos e nenhuma DeletionPolicy Retain, a deleção da stack falharia (buckets não esvaziados não podem ser deletados por padrão). Então, em casos reais, combine Conditions com DeletionPolicy: Retain se não quiser perder dados ao desativar recurso. Ou documente que ao voltar de prod para dev pode haver perda (ou manual intervention de esvaziar bucket antes).\n\n### Função Fn::If em propriedades\n\nAlém de condicionar presença de recursos/outputs, você pode usar **Fn::If** para fornecer valores diferentes para propriedades. A sintaxe é:\n\n```yaml\n!If [ NomeCondicao, ValorSeTrue, ValorSeFalse ]\n```\n\nPor exemplo, poderíamos setar a propriedade **InstanceType** condicionalmente:\n\n```yaml\nInstanceType: !If [ CreateBucketCondition, \"t3.small\", \"t2.micro\" ]\n```\n\nEste não é um exemplo muito prático (pois já temos param InstanceType), mas ilustraria: se prod (cond true), usa t3.small, senão t2.micro. **Fn::If** permite inclusive retornar um pseudo-valor chamado **AWS::NoValue** que indica \"não definir nada\". Isso é útil para casos como uma propriedade que você só quer incluir quando condicao true, e omitir completamente quando false. Ex:\n\n```yaml\nBucketEncryption: !If [ EnableEncryption, { ServerSideEncryptionConfiguration: [...] }, !Ref \"AWS::NoValue\" ]\n```\n\nSe condição EnableEncryption = false, !Ref AWS::NoValue faz com que CloudFormation remova/ignore essa propriedade, como se não estivesse presente.\n\nNo nosso template, não precisamos de Fn::If porque simplesmente não criamos o bucket em dev. Mas é bom saber que existe essa opção para propriedades e até mesmo para gerar parte de strings (usando Fn::Sub internamente com Ifs).\n\nNosso template está quase completo cobrindo todos os fundamentos.\n\nRecapitulando as condições implementadas:\n\n- Parâmetro **EnvironmentType** controla lógica.\n- Condição **CreateBucketCondition** avalia a necessidade do bucket.\n- Usamos em **Resources** (MeuBucket) e **Outputs** (BucketNameOutput).\n\n### Testando as condições\n\nFaça testes:\n\n- Crie a stack com **EnvironmentType=dev**. Espere finalizar. Verifique que nenhum bucket S3 foi criado (no CloudFormation, a aba Resources listará apenas a instância EC2). Outputs devem mostrar apenas InstanceId e IP, nada de bucket.\n- Atualize a mesma stack mudando **EnvironmentType** para \"prod\". Acompanhe os eventos: deverá criar agora o recurso S3 bucket. Verifique no S3 que o bucket apareceu. Outputs agora mostram BucketNameOutput também. (Lembre de fornecer BucketName param válido e único).\n- Tente atualizar de volta para dev (não faça isso se colocou dados no bucket a não ser que esteja testando – pois vai deletar o bucket). Nos eventos, verá que vai deletar o recurso S3 (como não tinha DeletionPolicy Retain, vai tentar deletar mesmo). Se o bucket estiver vazio, a deleção ocorre e a stack fica sem bucket novamente.\n\nIsso demonstra o poder do CloudFormation gerenciando todo o ciclo de vida dos recursos conforme as condições e mudanças de parâmetros – reduzindo muita mão de obra manual.\n\nAgora nosso template abordou: Parâmetros, Recursos, Mapeamentos, Condições e Outputs – cobrindo todos os aspectos principais de templates CloudFormation.\n\nNo próximo (e último) tutorial, sairemos um pouco do template em si e falaremos de **CloudFormation Drift** (Derivações), uma ferramenta para detectar desvios entre o estado atual dos recursos e o que está definido no template da stack. Isso nos ajuda a garantir que, depois de criado, ninguém alterou manualmente algum recurso por fora, fugindo do controle do CloudFormation."
  },
  {
    "id": "15236da4-4326-43aa-9ef6-9aeed8dfe9cc",
    "title": "Cloudformation Drift",
    "description": "Detecção de desvios entre a infraestrutura real e o que está definido no template. Exploramos o Drift Detection, como identificar mudanças feitas fora do CloudFormation e corrigir inconsistências.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/15236da4-4326-43aa-9ef6-9aeed8dfe9cc",
    "markdown": "Após implementar sua infraestrutura com CloudFormation, é fundamental mantê-la consistente com o que o template define. **CloudFormation Drift** (desvio ou deriva) refere-se à situação em que os recursos reais em execução **não correspondem mais** ao que está descrito no template da stack. Isso acontece quando houve mudanças feitas **fora** do CloudFormation – por exemplo, alguém manualmente alterou uma configuração diretamente pelo console AWS ou CLI, ou deletou/adicionou algo sem passar pelo CloudFormation.\n\nO CloudFormation possui um recurso chamado **Drift Detection** (detecção de deriva) para identificar esses casos. Vamos entender e aprender a usar.\n\n### O que é Drift e por que é um problema?\n\nIdealmente, toda modificação de infraestrutura ocorreria via atualização de stack (template). Porém, na prática, times podem fazer “hotfixes” rápido diretamente nos recursos, ou administradores podem ajustar algo temporariamente e esquecer de refletir no template. Isso causa **drift**: a stack no CloudFormation _pensa_ que o recurso está de uma maneira (porque assim está no template), mas na realidade ele foi mudado para outra configuração. Exemplos:\n\n- Alguém acessa o console EC2 e muda o tipo da instância de t2.micro para t3.small sem passar pelo CloudFormation.\n- Um bucket S3 criado pela stack teve a versioning habilitada manualmente fora do CF.\n- Um recurso foi deletado diretamente (ex: um desenvolvedor apagou um bucket pelo S3 console).\n\nEssas discrepâncias podem causar problemas graves:\n\n- O CloudFormation ao atualizar a stack pode falhar ou reverter porque encontra recursos em estado inesperado. Por exemplo, se ele tentar modificar algo mas detecta que o recurso foi alterado externamente, pode gerar conflito.\n- Ao deletar a stack, pode falhar (ex: bucket não vazio, etc) se não estava conforme esperado.\n- Mais importante: a infraestrutura real não condiz com o que _achamos_ que está (o template), o que desafia o propósito de IaC. Isso dificulta reproduzir ambientes ou rastrear mudanças.\n\n### Como o Drift Detection funciona?\n\nO AWS CloudFormation Drift Detection permite verificar se há desvios:\n\n- Você pode rodar drift detection em uma **stack inteira** ou em recursos específicos dentro da stack.\n- O CloudFormation então examina o template da stack e as configurações atuais de cada recurso e compara.\n- Se um recurso tem todas as propriedades exatamente conforme definidas no template, ele será marcado como **IN_SYNC** (em conformidade).\n- Se alguma propriedade diverge, o recurso será marcado como **MODIFIED**; se um recurso existente na stack foi excluído manualmente, marcará como **DELETED**; se algum recurso extra foi criado fora (que a CF não conhece), ele não aparece na lista pois CF só rastreia o que está no template.\n- O resultado geral da stack será **DRIFTED** se um ou mais recursos estão com drift, ou **IN_SYNC** se tudo bate. Recursos não suportados pela drift detection aparecem como **NOT_CHECKED** (nem todos os tipos de recurso são verificáveis ainda).\n\nDrift detection **não corrige** nada automaticamente – ele apenas detecta e reporta diferenças. Cabe a você ou sua equipe tomar ação:\n\n- Ou ajustar o recurso manualmente de volta ao estado definido no template.\n- Ou atualizar o template/stack para o novo estado desejado (assim eliminando o drift porque agora o template concorda).\n- Em casos de recurso deletado manual, você pode removê-lo do template (atualizar stack) ou recriá-lo via stack (por exemplo, usando recurso Retain + import).\n- **Dica:** CloudFormation tem recurso de import stack (importar recurso externo para dentro da stack) caso alguém criou algo manual e você quer trazê-lo sob gerenciamento CF – mas isso é avançado.\n\n### Executando Drift Detection\n\n**Via Console AWS:**\n\n1. No Console CloudFormation, selecione a stack desejada.\n2. No menu **Stack actions (Ações da pilha)**, clique em **Detect drift (Detectar deriva)**.\n3. O CloudFormation iniciará a análise. Você verá uma barra de progresso informando que iniciou a detecção de deriva. Aguarde alguns minutos (o tempo depende do número de recursos).\n4. Quando concluir, a página da stack mostrará o campo **Drift status**. Se divergências foram encontradas, aparecerá **DRIFTED**, senão **IN_SYNC**. Também haverá uma marcação de tempo da última verificação.\n5. Vá na aba **Resources** da stack e você verá agora colunas extras de Drift. Cada recurso listado terá um status: IN_SYNC, MODIFIED, DELETED ou NOT_CHECKED. Se estiver MODIFIED, você pode clicar para ver detalhes das diferenças. Haverá uma comparação listando cada propriedade diferente – mostrando valor esperado (do template) vs valor atual. Por exemplo, pode destacar que o InstanceType esperado era t2.micro mas o atual é t3.small, ou que BucketVersioning era false mas agora true, etc., com destaques em amarelo/verde/vermelho para mudanças.\n6. Você pode também clicar em **View drift results (Ver resultados de deriva)** no menu de ações da stack para ver um resumo completo incluindo recursos não driftados.\n\n**Via AWS CLI:** Você pode usar os comandos:\n\n- **aws cloudformation detect-stack-drift --stack-name NomeDaStack** – isso inicia a detecção. A saída dará um StackDriftDetectionId.\n- Com esse ID, rode **aws cloudformation describe-stack-drift-detection-status --stack-drift-detection-id <id>** para ver o status (IN_PROGRESS, COMPLETED, etc.).\n- Uma vez completo, use **aws cloudformation describe-stack-resource-drifts --stack-name NomeDaStack** para obter a lista de recursos e seus drift statuses e detalhes. A saída JSON listará cada recurso com **StackResourceDriftStatus** (MODIFIED, IN_SYNC, etc) e, se modificado, um diff das propriedades divergentes.\n\nPara nossa stack de exemplo, vamos simular drift:\n\n- Imagine que alguém habilitou manualmente o versionamento no bucket S3 (se criado) via Console S3.\n- Ou mudou uma tag ou propriedade.\n- Ou alterou o Security Group padrão afetando a instância (coisa fora do template).\n- Qualquer mudança serve.\n\nApós isso, rodamos a detecção de drift:\n\n- O CloudFormation detectará, por exemplo, que o bucket S3 tem **VersioningConfiguration** driftado (no template não estava habilitado, atual está Enabled). Ele marcaria o recurso bucket como MODIFIED.\n- Nossa instância se ninguém mexeu, ficaria IN_SYNC.\n- O Drift status da stack apareceria como DRIFTED (pois pelo menos um recurso driftou).\n\nVocê pode então tomar ação: no caso do bucket versioning habilitado manual, você decide se quer manter isso. Se sim, você deveria atualizar o template para habilitar versioning (assim alinhando o template com a realidade). Se não, desabilite manual de volta para ficar como template e rerode drift detect para confirmar que voltou a IN_SYNC.\n\nSe um recurso foi **deletado manualmente** (por exemplo, alguém apagou o bucket inteiro fora do CloudFormation), a detecção marcaria o recurso como **DELETED**. Nesse caso, a stack estaria driftada. Você poderia:\n\n- Recriar manual e esperar CF percebê-lo (não, CF não vai \"adotar\" espontaneamente).\n- Atualizar a stack removendo aquele recurso do template, efetivamente dizendo a CF para esquecê-lo (e talvez recriá-lo via import).\n- Ou simplesmente deletar a stack (que vai notar que recurso já não existe e provavelmente só remover o registro dele).\n\n**Recursos não suportados:** Se algum tipo de recurso não é suportado pelo drift detection, ele aparece como NOT_CHECKED. Por exemplo, AWS::IAM::Role drift detection não pega mudanças em certos managed policies, etc., se não suportado. A lista de suportados está na doc (a maioria dos principais são suportados). Nesse caso, conside usar Config ou outras ferramentas para monitorar aqueles.\n\n### Incorporando Drift Detection no fluxo\n\nÉ boa prática:\n\n- Executar drift detection periodicamente ou antes de atualizações importantes, para garantir que a stack está limpa.\n- Integrar com AWS Config para monitorar mudanças fora do CF, ou usar CloudFormation StackSets com guardrails.\n- Educar a equipe a **não mudar manualmente** recursos gerenciados por CloudFormation, ou se fizer, pelo menos sincronizar depois.\n\nCloudFormation Drift Detection ajuda a **garantir consistência** e **alertar** caso alguém tenha fugido do script. Assim, você mantém infraestrutura como código de verdade, sabendo que o que está rodando bate com o que está codificado.\n\n### Resumo final\n\nNeste tutorial final, aprendemos a:\n\n- Definir drift e entender por que evitar mudanças fora do CloudFormation.\n- Usar a detecção de deriva via console e CLI para identificar discrepâncias.\n- Interpretar os resultados (IN_SYNC vs DRIFTED, recurso MODIFIED/DELETED).\n- Tomar ações para corrigir drift (atualizar template ou recursos).\n\nCom isso, concluímos nossa série de fundamentos do AWS CloudFormation. Passamos por todos os componentes principais de um template (Parâmetros, Recursos, Outputs, Mapeamentos, Condições) e também abordamos boas práticas de manutenção (Drift).\n\nAgora você está apto a criar templates básicos, reutilizá-los para diferentes ambientes, e manter suas stacks sob controle de forma segura e auditável. CloudFormation é uma ferramenta poderosa – continue explorando recursos adicionais como **Metadata**, **Transform (SAM)** para serverless, **StackSets** para múltiplas contas/regiões, e a vasta biblioteca de tipos de recursos disponíveis.\n\nBom trabalho e boa sorte na sua jornada de infraestrutura como código na AWS! 🏗️🚀 "
  },
  {
    "id": "f14a7880-39d7-4733-87db-193ff6e74329",
    "title": "Landing Zone com Terraform",
    "description": "Neste tutorial, você aprenderá a construir uma Landing Zone usando Terraform para estruturar uma base segura e organizada para os seus projetos na AWS. São abordadas as melhores práticas de separação de contas, configuração de VPC, sub-redes (públicas e privadas), e o uso de Internet/NAT Gateways. Essa base garantirá que, tanto a aplicação monolítica quanto a arquitetura de microsserviços, possam ser provisionadas de forma consistente e segura.",
    "tool": "Terraform",
    "level": "intermediario",
    "tags": [
      "Landing Zone",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/f14a7880-39d7-4733-87db-193ff6e74329",
    "markdown": "## Visão Geral e Configuração Inicial\n\nAntes de criar qualquer recurso na nuvem, é recomendável preparar uma **Landing Zone** – uma configuração básica de ambiente em nuvem que segue boas práticas de segurança e organização. Na AWS, uma Landing Zone geralmente inclui a criação de uma **VPC (Virtual Private Cloud)** com sub-redes configuradas, gateways de rede e definições de segurança iniciais (como grupos de segurança e roles de IAM). Em resumo, a Landing Zone é uma fundação isolada e segura sobre a qual você vai construir sua infraestrutura. No contexto do nosso projeto BFF (Backend for Frontend), usaremos uma única conta AWS e uma VPC para manter todos os recursos necessários de forma organizada.\n\n> ℹ️ **O que é uma VPC?**  \n> Uma VPC é uma rede virtual isolada dentro da AWS onde podemos provisionar recursos. Ela se assemelha a uma rede tradicional em um data center próprio, mas usufrui da escalabilidade da AWS. Dentro da VPC vamos criar **sub-redes** (subnets) públicas e privadas. Conforme a documentação da AWS: _“A subnet é um intervalo de endereços IP em sua VPC... Após adicionar subnets, você pode implantar recursos AWS na VPC”_. Ou seja, subnets públicas terão acesso direto à internet, enquanto subnets privadas não serão acessíveis diretamente de fora, aumentando a segurança para recursos sensíveis (como bancos de dados).\n\nNesta primeira etapa, vamos criar os seguintes componentes básicos com Terraform:\n\n- **VPC personalizada** com um bloco CIDR (faixa de IPs) para englobar nossa rede (ex: **10.0.0.0/16**).\n- **Sub-rede pública** (por exemplo **10.0.1.0/24**) para hospedar recursos que precisam de acesso à internet (como servidores de aplicação).\n- **Sub-rede privada** (por exemplo **10.0.2.0/24**) para recursos internos (como banco de dados) sem acesso público.\n- **Internet Gateway (IGW)** para permitir que instâncias em sub-rede pública se conectem à internet.\n- **Route Table** associada à sub-rede pública, com rota padrão via o Internet Gateway (assim, tráfego 0.0.0.0/0 sai para a internet).\n- **Grupo de segurança base** definindo regras iniciais de tráfego (por exemplo, permitir SSH ou HTTP de locais apropriados).\n\nAbaixo, um diagrama simplificado da arquitetura de rede que vamos criar:\n\n![Landingzone](/tfLandingzone.svg)\n\n_Figura: Diagrama de rede mostrando a VPC com uma sub-rede pública (conectada a um Internet Gateway para acesso internet) e uma sub-rede privada (isolada da internet diretamente)._ Note que não incluímos um NAT Gateway neste diagrama para simplificar – em cenários reais, um NAT Gateway seria usado se recursos em sub-rede privada precisassem acessar a internet (para atualizações, por exemplo).\n\n## Criando a VPC e Sub-redes com Terraform\n\nVamos agora escrever o código Terraform passo a passo para criar a Landing Zone. Certifique-se de já ter configurado suas credenciais AWS localmente (via arquivo de credenciais ou variáveis de ambiente) e o Terraform instalado. A configuração consistirá em recursos do provedor AWS. Vamos começar definindo o provedor e a região, depois os recursos de rede:\n\n```hcl\n# Configurar o provedor AWS e a região\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"  # versão do provedor AWS\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = \"us-east-1\"   # Região AWS onde os recursos serão criados\n}\n\n# 1. Criar a VPC principal da Landing Zone\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = \"10.0.0.0/16\"      # Bloco de IPs da VPC\n  enable_dns_support   = true               # Habilita DNS interno na VPC\n  enable_dns_hostnames = true               # Necessário p/ resolver nomes de host de instâncias\n  tags = {\n    Name = \"bff-vpc\"                        # Nomear a VPC para fácil identificação\n  }\n}\n\n# 2. Sub-rede pública na VPC\nresource \"aws_subnet\" \"public\" {\n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = \"10.0.1.0/24\"   # Faixa de IPs da sub-rede pública\n  map_public_ip_on_launch = true           # Garante atribuição de IP público às instâncias\n  availability_zone       = \"us-east-1a\"   # Zona de disponibilidade (exemplo)\n  tags = {\n    Name = \"bff-public-subnet\"\n  }\n}\n\n# 3. Sub-rede privada na VPC\nresource \"aws_subnet\" \"private\" {\n  vpc_id            = aws_vpc.main.id\n  cidr_block        = \"10.0.2.0/24\"       # Faixa de IPs da sub-rede privada\n  availability_zone = \"us-east-1a\"\n  tags = {\n    Name = \"bff-private-subnet\"\n  }\n}\n\n# 4. Internet Gateway para acesso externo da sub-rede pública\nresource \"aws_internet_gateway\" \"gw\" {\n  vpc_id = aws_vpc.main.id\n  tags = {\n    Name = \"bff-internet-gateway\"\n  }\n}\n\n# 5. Tabela de rota para a sub-rede pública\nresource \"aws_route_table\" \"public\" {\n  vpc_id = aws_vpc.main.id\n  route {\n    cidr_block = \"0.0.0.0/0\"                     # Rota padrão para qualquer destino\n    gateway_id = aws_internet_gateway.gw.id      # Encaminha tráfego para o Internet Gateway\n  }\n  tags = {\n    Name = \"bff-public-rt\"\n  }\n}\n\n# 6. Associação da tabela de rota à sub-rede pública\nresource \"aws_route_table_association\" \"public_assoc\" {\n  subnet_id      = aws_subnet.public.id\n  route_table_id = aws_route_table.public.id\n}\n```\n\n**Explicação do código:** Definimos o provedor AWS e, em seguida, um recurso **aws_vpc** com um CIDR **/16**, o que nos dá um bloco grande de IPs privados para subdividir. Em seguida, criamos duas subnets: uma pública (com **map_public_ip_on_launch = true**, o que atribui IP público automaticamente às instâncias lançadas nela) e uma privada (sem IPs públicos). A sub-rede pública terá conectividade com a internet por meio de um Internet Gateway anexado à VPC. Isso é refletido na tabela de rotas pública, onde definimos que todo tráfego destinado a **0.0.0.0/0** (internet) deve sair pelo **aws_internet_gateway.gw**. Associamos essa tabela de rota à sub-rede pública para que a rota tenha efeito lá. A sub-rede privada, por outro lado, **não** recebe uma rota para o IGW, permanecendo isolada da internet (recursos nela só comunicam com outros recursos internos da VPC ou via gateways especializados, como NAT Gateway ou endpoints privados, se configurados).\n\nAgora que a base de rede está pronta, podemos definir um **Security Group** padrão. Security Groups atuam como firewalls em nível de instância, controlando tráfego de entrada e saída. De acordo com a AWS, _“um Security Group é um firewall virtual que permite especificar protocolos, portas e origens IP que podem alcançar suas instâncias”_. Vamos criar um grupo de segurança que permita, por exemplo, acesso SSH (porta 22) e HTTP (porta 80) a instâncias públicas, e todas as comunicações internas necessárias dentro da VPC.\n\n```hcl\n# 7. Grupo de Segurança base \nresource \"aws_security_group\" \"base\" {\n  name        = \"bff-base-sg\"\n  description = \"Grupo de segurança base para app BFF\"\n  vpc_id      = aws_vpc.main.id\n\n  # Regras de entrada (ingress)\n  ingress {\n    description = \"Acesso SSH permitido\"\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]    # permitir SSH de qualquer IP (evitar em produção!)\n  }\n  ingress {\n    description = \"HTTP web permitido\"\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]    # permitir acesso HTTP de qualquer IP\n  }\n\n  # Regras de saída (egress) – padrão permite toda saída\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"            # -1 significa todos os protocolos\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"bff-base-sg\"\n  }\n}\n```\n\n**Nota:** As regras acima abrem SSH e HTTP para todas as origens (**0.0.0.0/0**), o que _não é recomendável em produção_. Fizemos isso apenas para simplificar o acesso durante os testes. Em ambientes reais, restrinja o SSH a IPs específicos (por exemplo, somente do seu IP ou VPN) e considere usar HTTPS (porta 443) ao invés de HTTP puro.\n\nCom essa configuração de Terraform, nossa Landing Zone está definida. Salvando o código acima em arquivos (**main.tf**, por exemplo, ou divididos em arquivos lógicos como **vpc.tf**, **subnet.tf**, etc.), estamos prontos para aplicar. Seguem as etapas para executar o Terraform:\n\n1. **Inicializar**: na pasta dos arquivos **.tf**, execute **terraform init**. Isso baixará o provedor AWS e preparará o ambiente Terraform.\n2. **Plan** (planejar): execute **terraform plan** para ver uma prévia dos recursos que serão criados. Revise se tudo parece correto.\n3. **Aplicar**: execute **terraform apply** e confirme digitando \"yes\". O Terraform então criará todos os recursos na AWS conforme definido.\n\nDepois de aplicado, você pode verificar no console AWS que a VPC e sub-redes foram criadas, assim como o Internet Gateway e o security group. A VPC customizada agora existe em sua conta, isolada do resto da AWS, pronta para receber as instâncias da nossa aplicação BFF.\n\n> ⚠️ **Infraestrutura como Código (IaC)**  \n> Note como definimos toda a infraestrutura em arquivos de configuração em vez de criar manualmente pela console. Essa é a essência da IaC: manter a definição do ambiente em código versionável, permitindo recriação e alterações reproduzíveis. A AWS inclusive fornece orientações oficiais de como usar Terraform como IaC em seus projetos: _“HashiCorp Terraform é uma ferramenta IaC que ajuda a gerenciar sua infraestrutura... você define recursos cloud em arquivos de configuração que podem ser versionados, reutilizados e compartilhados”_. Isso traz benefícios de automação, consistência e auditoria de mudanças no ambiente.\n\nCom a Landing Zone pronta, podemos prosseguir para criar nossa aplicação monolítica de exemplo e seus recursos."
  },
  {
    "id": "3577bc27-0f1d-45a1-9fb5-e9eeecb093f6",
    "title": "APP monólito com Terraform",
    "description": "Neste tutorial, você aprenderá a provisionar uma aplicação BFF monolítica utilizando Terraform. A infraestrutura incluirá uma única instância EC2 que hospeda o backend, frontend, banco de dados e API da aplicação JavaScript didática. O código explica detalhadamente cada recurso e conexão, demonstrando como consolidar todos os componentes em uma única máquina para facilitar testes e aprendizado inicial.",
    "tool": "Terraform",
    "level": "intermediario",
    "tags": [
      "monólito",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/3577bc27-0f1d-45a1-9fb5-e9eeecb093f6",
    "markdown": "## Arquitetura Monolítica vs. Microsserviços\n\nAntes de implementar a aplicação, vale entender a diferença de arquitetura que vamos vivenciar nesta série de tutoriais. Inicialmente, teremos uma aplicação **monolítica**: todos os componentes (frontend, backend, banco de dados e API) executando juntos em uma única instância de servidor. Posteriormente, migraremos para uma arquitetura de **microsserviços/BFF**, onde cada componente será separado em serviços distintos que se comunicam entre si.\n\nEm termos simples, **monólito** significa uma única base de código e implantação unificada para todas as funções do sistema. Já em microsserviços, a aplicação é dividida em pequenos serviços independentes, cada um realizando uma função de negócio e se comunicando via APIs. Monólitos são mais fáceis de iniciar e implantar no começo, mas podem dificultar escalabilidade e manutenção conforme crescem. Microsserviços exigem mais planejamento (definir boundaries de serviços, contratos de API), porém permitem escalar ou alterar cada parte de forma independente e mais ágil.\n\nPara resumir algumas diferenças:\n\n|Aspecto|Monólito (Tudo em Um)|Microsserviços (BFF)|\n|---|---|---|\n|**Codebase**|Única, todos os módulos juntos num só projeto|Múltiplos projetos/serviços independentes|\n|**Implantação**|Um único pacote/artefato implantado por vez|Cada serviço implantado separadamente (docker, função etc)|\n|**Escalabilidade**|Escala a aplicação inteira de uma vez (clonar instâncias)|Escala-se apenas os serviços necessários, individualmente|\n|**Manutenção e Atualização**|Mudança exige build/teste/deploy de todo o sistema|Mudanças isoladas em um serviço; menor impacto no todo|\n|**Comunicação**|Chamadas internas diretas (métodos/funções na memória)|Vias de comunicação com APIs (HTTP/REST, etc.)|\n\n_Tabela: Comparação resumida entre arquitetura monolítica e microsserviços._\n\nNo contexto do padrão **Backend for Frontend (BFF)**, a ideia é que cada tipo de cliente (web, mobile, etc.) possa ter um backend dedicado para suas necessidades. Em uma arquitetura tradicional monolítica, um único backend geral tenta servir diferentes clientes, o que pode levar a **gargalos e acúmulo de responsabilidades**. Ao aplicar BFF, estamos, de certa forma, criando uma **versão especializada de backend** – no nosso caso, uma para atender especificamente ao frontend web da nossa aplicação de exemplo. Primeiro, porém, vamos implementar tudo junto (monólito) para então refatorar em BFF + microsserviços.\n\n## Configurando a Instância EC2 Monolítica\n\nCom a Landing Zone criada (VPC, subnets e segurança), podemos provisionar a instância EC2 que rodará a aplicação monolítica. Usaremos a **sub-rede pública** para que possamos acessá-la diretamente pela internet (para visualizar a interface web e API). Esta instância incluirá: o servidor de frontend, a lógica de backend (API) e um banco de dados local para persistência – tudo em um só host.\n\n### Escolha da AMI e tipo de instância\n\nPara simplificar, podemos usar uma AMI do Amazon Linux 2 ou Ubuntu Server (gratuita) e um tipo de instância pequeno, como **t2.micro** (elegível para camada gratuita). A Amazon define EC2 como _“capacidade de computação escalável sob demanda na nuvem AWS”_ – uma VM flexível onde instalaremos nosso stack.\n\nNo Terraform, o recurso **aws_instance** será utilizado para criar a instância EC2. Precisamos especificar pelo menos: AMI, tipo, subnet, grupo de segurança e um **User Data** script para instalar e iniciar nossa aplicação automaticamente quando o servidor for lançado. Conforme a documentação AWS, o _user data_ é um mecanismo para passar scripts que executam na inicialização da instância, permitindo automação da configuração inicial. Aproveitaremos isso para instalar dependências (Node.js, por exemplo) e subir nossa aplicação automaticamente.\n\n### User Data – Instalando a aplicação automaticamente\n\nNossa aplicação didática BFF será implementada em JavaScript (Node.js). Para fins de teste, podemos imaginar que ela consiste em:\n\n- Um pequeno servidor web Express (Node.js) que sirva uma página HTML estática (frontend) e exponha uma API REST simples (endpoint **/api/hello** por exemplo).\n- Um banco de dados local – para simplificar, usaremos SQLite (banco de dados em arquivo) ou até mesmo um array em memória simulando dados, só para validar funcionamento. (Em um monólito real poderíamos instalar MySQL/PostgreSQL localmente, mas isso aumenta complexidade de script).\n- Testes básicos embutidos (por exemplo, rota **/api/health** que retorna ok, ou um script que verifica se a página carrega e API responde).\n\nVamos considerar um script de User Data que instala Node.js, baixa ou contém o código da nossa aplicação e a executa. Uma forma comum é entregar o código via repositório Git ou zip. Para fins educacionais, poderíamos incluir todo o código da aplicação dentro do próprio user data (não é usual em produção, mas serve para demo). Aqui, resumidamente, criaremos um servidor Node que responde \"Hello World\" em uma página e via API, para confirmar se está tudo funcionando.\n\nAbaixo está o Terraform para criar a instância EC2 monolítica:\n\n```hcl\n# 1. Definir variables para AMI e Key Pair (opcionalmente)\nvariable \"ami_id\" {\n  description = \"AMI do Amazon Linux 2 para região us-east-1\"\n  default     = \"ami-09d56f8956ab235b3\"   # Exemplo de AMI Amazon Linux 2 (us-east-1)\n}\nvariable \"key_name\" {\n  description = \"Nome do Key Pair para acesso SSH\"\n  default     = null  # pode ser preenchido com um key pair existente se desejar SSH\n}\n\n# 2. Criar a instância EC2 monolítica\nresource \"aws_instance\" \"monolith\" {\n  ami                         = var.ami_id\n  instance_type               = \"t2.micro\"\n  subnet_id                   = aws_subnet.public.id\n  vpc_security_group_ids      = [aws_security_group.base.id]\n  key_name                    = var.key_name  # se quiser acessar via SSH usando um key pair\n\n  # Script de inicialização (User Data) para instalar Node.js e rodar a aplicação\n  user_data = <<-EOF\n              #!/bin/bash\n              # Atualizar pacotes e instalar Node.js\n              sudo yum update -y\n              # Instala Node.js 16.x (usando gerenciador n ou dnf, dependendo da AMI)\n              curl -sL https://rpm.nodesource.com/setup_16.x | sudo bash -\n              sudo yum install -y nodejs\n\n              # Cria um app Node simples\n              cat > app.js << 'EOL'\n              const http = require('http');\n              const fs = require('fs');\n              const PORT = 80;\n\n              const html = `<html>\n              <head><title>Monólito BFF</title></head>\n              <body><h1>Bem-vindo ao Monólito BFF</h1>\n              <p id=\"msg\"></p>\n              <script>\n                fetch('/api/hello').then(res=>res.json()).then(data=>{\n                  document.getElementById('msg').innerText = data.message;\n                });\n              </script>\n              </body></html>`;\n\n              const requestListener = function(req, res) {\n                if (req.url === '/' && req.method === 'GET') {\n                  res.writeHead(200, { 'Content-Type': 'text/html' });\n                  res.end(html);\n                } else if (req.url === '/api/hello' && req.method === 'GET') {\n                  res.writeHead(200, { 'Content-Type': 'application/json' });\n                  res.end(JSON.stringify({ message: \"Olá do Monólito BFF!\" }));\n                } else {\n                  res.writeHead(404);\n                  res.end();\n                }\n              };\n\n              const server = http.createServer(requestListener);\n              server.listen(PORT, () => {\n                console.log('Server running on port', PORT);\n              });\n              EOL\n\n              # Inicia o app node em background\n              node app.js > /home/ec2-user/app.log 2>&1 &\n              EOF\n\n  tags = {\n    Name = \"bff-monolith-instance\"\n  }\n}\n```\n\n**Detalhes importantes do código acima:**\n\n- **AMI**: usamos uma variável **ami_id** com default de uma AMI Amazon Linux 2 (substitua pelo ID correto da região escolhida). Essa AMI permite usar o gerenciador de pacotes **yum** para instalar Node.js facilmente.\n- **user_data**: No bloco **user_data**, usamos o formato HEREDOC (**<<-EOF** ... **EOF**) para escrever um script bash multi-linha. Esse script atualiza o sistema, instala Node.js, e então cria um arquivo **app.js** contendo nosso servidor Node embutido (usando **cat > app.js << 'EOL' ... EOL**). O servidor é bem básico: se requisitado na raiz (**/**), retorna um HTML com uma mensagem de boas-vindas e um script JavaScript no navegador que chama nossa API; se requisitado em **/api/hello**, responde com JSON contendo uma mensagem. Finalmente, o script inicia o servidor Node na porta 80 e sai em background (para continuar rodando após o boot).\n- **Porta 80**: Escolhemos rodar na porta 80 para evitar configurar firewall adicional — nossa Security Group base já permite tráfego HTTP (porta 80) de qualquer origem. Assim que a instância iniciar, teoricamente poderemos acessar pelo navegador o seu IP público (porta 80) e ver a página carregando. Essa página por sua vez fará uma chamada fetch ao endpoint **/api/hello** da própria instância e exibirá a mensagem recebida.\n- **Key Pair**: Incluímos opcionalmente a variável **key_name** e passamos ao recurso. Se você tiver criado um Key Pair na AWS e deseja acesso SSH direto, defina **var.key_name**. Caso contrário, a instância inicia sem chave SSH (você não poderá logar manualmente, apenas via user data e outputs do Terraform).\n\nApós rodar **terraform apply** para essa configuração, o Terraform vai iniciar a instância EC2. A saída da execução do Terraform provavelmente mostrará o ID da instância e o endereço IP público dela (a menos que capturemos explicitamente via outputs). Podemos adicionar um **output** para conveniência:\n\n```hcl\noutput \"monolith_public_ip\" {\n  value = aws_instance.monolith.public_ip\n  description = \"IP público da instância monolítica\"\n}\n```\n\nCom o IP em mãos, teste a aplicação:\n\n- Acesse pelo navegador **http://<IP_público>/**. Você deve ver a página \"Bem-vindo ao Monólito BFF\" e, após alguns instantes, a mensagem carregada via API (ex: \"Olá do Monólito BFF!\").\n- Se preferir via terminal, use **curl**: **curl http://<IP>/api/hello** deve retornar o JSON **{\"message\":\"Olá do Monólito BFF!\"}**.\n- Também experimente **curl http://<IP>/** para ver o HTML bruto retornado.\n\nSe esses testes passaram, significa que nossa instância monolítica está rodando corretamente: o servidor Node está servindo tanto o frontend quanto a API e respondendo às solicitações, e tudo está confinado a um só host. Internamente, caso tivéssemos usado um banco de dados local, o backend acessaria diretamente arquivos ou socket de banco de dados local – sem camadas de rede ou autenticação adicionais.\n\n### Verificando Recursos e Comentários\n\nNesta etapa, verifique no console AWS os recursos criados:\n\n- A instância EC2 deve estar na sub-rede pública da VPC personalizada (veja as propriedades de rede da instância; o ID da VPC e subnet devem corresponder aos criados no Tutorial 1).\n- O grupo de segurança associado deve ser o \"bff-base-sg\", com as regras de porta 22 e 80 abertas conforme definimos.\n- O atributo _User Data_ da instância (visualizável em **Instance Settings > View/Change User Data** no console) deve conter o script que escrevemos. Conforme a AWS, _o user data é codificado em base64 e entregue à instância_, que por sua vez interpreta e executa no boot. Podemos inspecionar no **System Log** da instância (EC2 > Instância > Monitoramento > Logs do sistema) para ver os logs do nosso script (por exemplo, confirmação de instalação do Node ou erros de sintaxe, se houver).\n\n**Considerações sobre o monólito:** Embora funcional, essa arquitetura tem limitações. Todo o sistema depende de uma única VM: se ela falhar, tudo sai do ar de uma vez. Escalar requer lançar uma segunda instância com tudo (duplicando também o banco de dados, o que pode causar inconsistências se não sincronizado). Além disso, o acoplamento é alto – a equipe de desenvolvimento gerencia front, back e dados num mesmo projeto. Esses problemas motivam a migração para microsserviços. No próximo tutorial, começaremos a refatoração separando o frontend em seu próprio componente."
  },
  {
    "id": "8c59bb28-1b14-451b-9973-a447015d7f95",
    "title": "APP BFF Frontend com Terraform",
    "description": "Aqui, o foco é a criação de um ambiente dedicado para o Frontend da aplicação BFF. Utilizando Terraform, o tutorial demonstra como configurar recursos específicos para servir o conteúdo web, como instâncias dedicadas, load balancers e grupos de segurança que garantam o acesso ao serviço. Cada trecho de código é comentado para esclarecer a função de cada item e facilitar a compreensão para iniciantes.",
    "tool": "Terraform",
    "level": "intermediario",
    "tags": [
      "frontend",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/8c59bb28-1b14-451b-9973-a447015d7f95",
    "markdown": "## Separando o Frontend do Monólito\n\nAgora que o monólito básico está rodando, vamos iniciar a transição para a arquitetura BFF com microsserviços. O primeiro passo é **separar o frontend** (interface do usuário) do backend. Em um monólito, o frontend (HTML/CSS/JS) geralmente é servido pelo mesmo servidor que a API. Vamos quebrar essa dependência criando um serviço dedicado para o frontend.\n\n**O que isso significa?** Em vez de uma única instância servindo tudo, teremos _duas instâncias_: uma para o **frontend** (por exemplo, um servidor web estático ou aplicação React/Angular compilada servida via Nginx ou Node) e outra para o **backend** (a API e lógica de negócio). Nesta etapa, o backend ainda permanecerá acoplado ao banco de dados (a separação do banco virá no próximo tutorial), mas já não irá mais entregar arquivos de frontend – o frontend separado fará isso.\n\nEssa mudança traz benefícios imediatos: o frontend pode ser desenvolvido/desdobrado independentemente, possivelmente escalado separadamente (ex.: mais servidores web se houver muita carga de usuários estáticos) e podemos otimizar a entrega de conteúdo estático (usando CDN, cache etc. futuramente). Do lado do backend, ele exporá apenas APIs para serem consumidas.\n\nNo nosso projeto de exemplo, o monólito Node servia tanto HTML quanto a resposta JSON. Vamos ajustar: a instância de frontend servirá o HTML/JS e fará requisições para a API do backend (que agora estará em outra instância). Essa comunicação acontecerá via HTTP interno na VPC ou via internet? Como ambas instâncias estarão na mesma VPC, podemos configurá-las para se comunicar internamente (mais seguro e rápido). Para isso, garantiremos que o backend tenha um nome DNS interno ou IP conhecido e o frontend o acesse pelo endereço privado.\n\nO diagrama agora fica assim:\n\n![tfFrontend](/tfFrontend.svg)\n\n_Figura: Arquitetura após separar o Frontend. O usuário acessa a instância de Frontend (por exemplo, Nginx ou um servidor Node que serve páginas). O frontend se comunica com a API do backend (na outra instância) para obter dados. O backend ainda usa um banco de dados local interno nesta etapa._\n\nObserve que o **Amazon API Gateway** ainda não entrou – por enquanto, o frontend se comunica diretamente com o backend. Mais adiante, introduziremos o API Gateway como camada entre eles.\n\n## Provisionando a Instância de Frontend\n\nVamos criar uma nova instância EC2 para hospedar o frontend. Podemos reutilizar grande parte da configuração do Terraform do monólito, com diferenças nas **regras de segurança** e no **user data**.\n\n### Grupo de segurança para comunicação interna\n\nPrecisamos de uma comunicação **frontend -> backend** na porta da API (no exemplo do monólito, a API rodava na porta 80 também, mas vamos ajustar para evitar conflito – podemos colocar o backend para rodar na porta 3000, por exemplo, já que 80 será usada agora exclusivamente pelo frontend web).\n\nPortanto:\n\n- O **frontend** aceitará tráfego HTTP (porta 80) de usuários externos (internet) – isso já tínhamos no SG base.\n- O **backend** aceitará tráfego HTTP da aplicação (porta 3000, por exemplo) **somente** da instância de frontend (não expor porta 3000 para toda internet). Isso é uma melhora de segurança: o endpoint de API não precisará ficar público agora, apenas o frontend acessa ele internamente.\n\nPara implementar isso, podemos criar um novo Security Group para o backend com uma regra de entrada que referencie o SG do frontend como origem. O Terraform permite referenciar grupos de segurança nas regras (via **source_security_group_id** ou **security_groups** na regra ingress).\n\nVamos definir dois SGs específicos:\n\n- **frontend_sg**: permite porta 80 de internet (e possivelmente 443 se fosse HTTPS).\n- **backend_sg**: permite porta 3000 _somente_ do SG do frontend.\n\nAlém disso, ambos SGs permitirão saída irrestrita (padrão) para permitir que ambos façam chamadas externas se necessário (ex: backend acessando repositórios, etc., apesar de idealmente backend não chamaria internet agora).\n\n### Criando os recursos de frontend e backend\n\nNo Terraform, adicionaremos novos resources:\n\n- **aws_security_group.frontend** e **aws_security_group.backend_api**.\n- **aws_instance.frontend** e **aws_instance.backend**.\n\nTambém teremos que decidir o que fazer com a instância monolítica anterior. Em um cenário real, faríamos _redeployment_ (desligar monólito, subir novos serviços). Para fins do tutorial, podemos **reutilizar** a instância monolítica como backend inicialmente (já que ela já tem a lógica API e DB rodando). Porém, para maior clareza, vamos criar uma instância nova de backend também, e então desconsiderar/desligar a monolítica. Isso demonstra desde já como seria criar tudo separado via Terraform. (Caso esteja seguindo na prática, você pode destruir a instância **aws_instance.monolith** antiga ou simplesmente deixá-la parada enquanto experimenta as novas.)\n\nAbaixo o código Terraform complementar para esta etapa:\n\n```hcl\n# 1. Security Group para o Frontend\nresource \"aws_security_group\" \"frontend\" {\n  name        = \"bff-frontend-sg\"\n  description = \"Permite acesso HTTP público ao frontend\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    description = \"HTTP do mundo para frontend\"\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]   # qualquer origem pode acessar o frontend web\n  }\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  tags = { Name = \"bff-frontend-sg\" }\n}\n\n# 2. Security Group para o Backend/API\nresource \"aws_security_group\" \"backend_api\" {\n  name        = \"bff-backend-sg\"\n  description = \"Permite acesso à API somente do Frontend\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    description      = \"API aberta para Frontend SG\"\n    from_port        = 3000\n    to_port          = 3000\n    protocol         = \"tcp\"\n    security_groups  = [aws_security_group.frontend.id]  # somente instâncias com sg frontend\n  }\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  tags = { Name = \"bff-backend-sg\" }\n}\n\n# 3. Instância EC2 para o Frontend\nresource \"aws_instance\" \"frontend\" {\n  ami                    = var.ami_id           # reutiliza a mesma AMI do Amazon Linux 2\n  instance_type          = \"t2.micro\"\n  subnet_id              = aws_subnet.public.id\n  vpc_security_group_ids = [aws_security_group.frontend.id, aws_security_group.base.id]\n  key_name               = var.key_name\n\n  user_data = <<-EOF\n              #!/bin/bash\n              sudo yum update -y\n              # Instalar nginx para servir conteúdo estático (opcional: poderíamos usar Node)\n              sudo amazon-linux-extras install -y nginx1\n              sudo systemctl start nginx\n              sudo systemctl enable nginx\n\n              # Criar página estática de frontend\n              echo \"<h1>Frontend BFF</h1><p>Aplicação Frontend funcionando.</p>\" | sudo tee /usr/share/nginx/html/index.html\n\n              # Configurar nginx para passar chamadas /api para o backend\n              cat <<EOT | sudo tee /etc/nginx/conf.d/bff.conf\n              server {\n                listen 80;\n                server_name _;\n                location / {\n                  root /usr/share/nginx/html;\n                }\n                location /api/ {\n                  proxy_pass http://#{aws_instance.backend.private_ip}:3000;\n                }\n              }\n              EOT\n\n              sudo systemctl restart nginx\n              EOF\n\n  tags = { Name = \"bff-frontend-instance\" }\n}\n\n# 4. Instância EC2 para o Backend\nresource \"aws_instance\" \"backend\" {\n  ami                    = var.ami_id\n  instance_type          = \"t2.micro\"\n  subnet_id              = aws_subnet.private.id   # backend na sub-rede privada\n  vpc_security_group_ids = [aws_security_group.backend_api.id]\n  key_name               = var.key_name\n\n  user_data = <<-EOF\n              #!/bin/bash\n              sudo yum update -y\n              # Instalar Node.js (similar ao monólito)\n              curl -sL https://rpm.nodesource.com/setup_16.x | sudo bash -\n              sudo yum install -y nodejs\n              # Código básico do backend (porta 3000)\n              cat > backend.js << 'EOL'\n              const http = require('http');\n              const server = http.createServer((req, res) => {\n                if (req.url === '/api/hello' && req.method === 'GET') {\n                  res.writeHead(200, { 'Content-Type': 'application/json' });\n                  res.end(JSON.stringify({ message: \"Olá do Backend BFF!\" }));\n                } else {\n                  res.writeHead(404);\n                  res.end();\n                }\n              });\n              server.listen(3000);\n              EOL\n              node backend.js > /home/ec2-user/backend.log 2>&1 &\n              EOF\n\n  tags = { Name = \"bff-backend-instance\" }\n}\n```\n\nAlguns pontos a destacar neste código:\n\n- Colocamos o **backend** na sub-rede privada (**aws_subnet.private.id**). Isso significa que ele _não possui IP público_. Ele só será acessível dentro da VPC. Isso melhora a segurança do backend, pois não pode ser chamado diretamente da internet. Mesmo para atualizações de pacotes via yum e npm, nossa instância backend conseguirá acesso? Note que sem um NAT Gateway, instâncias em sub-rede privada **não têm acesso de saída à internet**. Nesse caso, nosso script de user data no backend que tenta baixar Node.js do repositório externo poderia falhar. Para contornar isso sem adicionar NAT agora, poderíamos:\n    \n    - Colocar o backend também na sub-rede pública temporariamente, ou\n    - Aproveitar que nossa VPC por padrão deve ter um DNS interno resolvendo endereços do pacote (mas sem rota à internet não adiantaria).\n    \n    Simplicando: para que o user data do backend funcione corretamente, **vamos trocar e colocar o backend também na sub-rede pública** (ou adicionar um NAT Gateway). Neste tutorial, modificaremos para **subnet_id = aws_subnet.public.id** no backend, com a consciência de que ele ficará com IP público (embora não vamos usá-lo diretamente). _Em um ambiente real,_ preferiríamos criar um NAT em nossa Landing Zone e manter backend realmente privado. Caso opte por isso, adicione um NAT Gateway na VPC e uma rota 0.0.0.0/0 na tabela da sub-rede privada apontando para o NAT, para que o user data do backend consiga baixar pacotes.\n    \n- O **nginx** no frontend é configurado para servir conteúdo estático e também atuar como proxy para requisições **/api/** para o backend. Repare na configuração do **proxy_pass**: usamos um placeholder **#{aws_instance.backend.private_ip}**. Em teoria, gostaríamos de injetar dinamicamente o IP privado da instância de backend aqui. Entretanto, o Terraform não permite interpolação direta dentro do **user_data** porque ele trata o bloco como string literal a ser enviada. Poderíamos usar o **templatefile** para compor o user data externamente. Para manter simples, podemos nesta fase editar manualmente após deploy ou supor que sabemos o IP do backend. (Numa implementação real, usaríamos por exemplo o DNS privado do backend via Route 53 ou Service Discovery do ECS, etc.)\n    \n    Para fins de tutorial, vamos supor o IP privado do backend (ou poderíamos rodar o Terraform em duas etapas: criar backend primeiro, anotar IP, depois passar para o script do frontend). Já que estamos descrevendo conceitualmente, não nos aprofundaremos nesse detalhe – mas tenha em mente a importância de descobrir o endereço interno do backend para configurar o front. Outra opção seria o backend rodar com um nome DNS privado (por exemplo, usando AWS Cloud Map ou simplesmente referenciando o hostname padrão da EC2 backend dentro do mesmo VPC). No Amazon Linux 2, cada instância registra no DNS interno um nome estilo **<instance-id>.ec2.internal**. Poderíamos usar isso no proxy_pass também, bastando pegar o ID ou usar um data source de AWS instances filtrado. Vamos simplificar assumindo IP fixo conhecido para não complicar com templatefile agora.\n    \n- O **security group** do frontend e backend já garantem as restrições: somente instâncias com SG do frontend podem atingir a porta 3000 do backend. Isso significa que mesmo nosso backend estando (nesse momento) com IP público, ele não aceitará requisições de fora naquela porta – apenas do frontend. Se tentássemos acessar **http://IP_publico_backend:3000/api/hello** de casa, não teríamos resposta (timeout), pois o firewall bloqueia. Mas se o frontend (dentro da VPC) acessar **http://IP_privado_backend:3000/api/hello**, vai passar.\n    \n\nApós aplicar essas configurações, teremos duas novas instâncias EC2. Devemos testar:\n\n- **Frontend:** obter seu IP público e acessar via navegador **http://<IP-frontend>/**. A página deve mostrar \"Frontend BFF\" (conforme colocamos no HTML) e se tudo deu certo, a aplicação do frontend talvez tente acessar **/api/hello** e obter resposta do backend. Se configuramos corretamente o proxy e o IP do backend, ao acessar a página do frontend ela deveria exibir algum retorno da API do backend. Podemos verificar via console do navegador ou logs do nginx na instância se o proxy conseguiu encaminhar.\n- **Backend:** mesmo sem acesso externo, podemos testar chamando sua API a partir do frontend ou internamente. Para uma confirmação rápida, podemos SSH na instância frontend (já que abrimos 22 e tem key) e de lá executar **curl http://<IP_privado_backend>:3000/api/hello**. Deve retornar o JSON **{\"message\":\"Olá do Backend BFF!\"}**.\n\nCaso não tenhamos adicionado NAT Gateway, o script de user data do backend pode não ter instalado Node e iniciado corretamente (lembrar da questão do acesso internet). Podemos logar no frontend, depois SSH no backend via sessão (ex: usando o front como jump host) ou habilitar temporariamente porta 22 no SG do backend e usar VPN/bastion. Esse nível de detalhe foge um pouco do tutorial – assumiremos que resolvemos essa dependência de instalação.\n\n## Conclusão Parcial\n\nConseguimos separar o frontend do backend do nosso aplicativo. Agora, o usuário acessa um servidor dedicado ao frontend e este servidor busca dados do backend via chamadas de API internas. Já estamos nos beneficiando de uma pequena arquitetura BFF: o frontend poderia ser otimizado independentemente (por exemplo, colocando um cache estático ou CDN), e o backend já não precisa servir conteúdo estático, focando apenas em lógica de negócio e dados.\n\nNo próximo passo, vamos isolar também o banco de dados, extraindo-o para um serviço gerenciado (Amazon RDS). Isso completará a separação dos principais componentes (frontend, backend, database). Em seguida, integraremos o **Amazon API Gateway** para orquestrar as chamadas de API de forma mais eficiente e segura."
  },
  {
    "id": "fa672bb6-64d4-4569-9bdf-64b786d65194",
    "title": "APP BFF Backend com Terraform",
    "description": "Focado no Backend da aplicação BFF, este tutorial mostra como utilizar Terraform para provisionar os componentes que processam a lógica de negócios, lidam com requisições e se conectam ao banco de dados. São abordadas práticas para definição de variáveis, criação de recursos e integração com outros serviços, garantindo uma arquitetura modular que permite escalabilidade e manutenção facilitada.",
    "tool": "Terraform",
    "level": "intermediario",
    "tags": [
      "backend",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/fa672bb6-64d4-4569-9bdf-64b786d65194",
    "markdown": "## Isolando o Banco de Dados e Backend\n\nContinuando nossa evolução arquitetural, agora vamos separar o **banco de dados** do backend e ajustar este para funcionar como um microsserviço independente. Até o momento, nosso backend BFF ainda estava executando com um banco de dados local (no monólito ou na própria VM backend). Isso pode funcionar em protótipos, mas em produção é desejável usar um serviço de banco dedicado, confiável e escalável.\n\nUsaremos o **Amazon RDS (Relational Database Service)** para criar um banco de dados gerenciado. Conforme a AWS, _“Amazon RDS é um serviço web que facilita configurar, operar e escalar um banco de dados relacional na nuvem AWS”_. O RDS nos poupa do trabalho de instalar software de banco, gerenciar backups, etc., pois oferece recursos como backups automatizados, replicação e escalabilidade sob demanda. Também melhoramos a segurança colocando o RDS em subnet privada (sem acesso público) e controlando acesso apenas para nosso backend via segurança de rede.\n\n### Escolhendo o tipo de Banco de Dados\n\nPara uma aplicação BFF didática, podemos usar um banco MySQL ou PostgreSQL pequeno no RDS (por exemplo, um db.t2.micro). Vamos optar por **MySQL** para este tutorial, mas os passos seriam similares para Postgres. Precisaremos providenciar alguns detalhes:\n\n- Nome do banco de dados, usuário e senha de admin.\n- Subnet group no RDS para especificar que subnets o banco usará (faremos uso das subnets privadas criadas na Landing Zone).\n- Segurança: um Security Group para o RDS que permita acesso apenas do backend (similar à ideia anterior, mas RDS não ficará em SG de EC2 comum, e sim SG do próprio RDS).\n\nFaremos:\n\n1. Criar um **RDS Subnet Group** incluindo nossas sub-rede privada (para RDS criar instância dentro delas).\n2. Criar a instância RDS MySQL (definindo engine, versão, tamanho, credenciais).\n3. Criar um Security Group para o RDS aceitar conexões na porta 3306 somente da SG do backend.\n4. Atualizar a configuração do backend para conectar ao RDS em vez do DB local (isso envolve passar host, porta, user, senha do RDS para o nosso app).\n\nNo Terraform, o recurso é **aws_db_instance** para o RDS, e **aws_db_subnet_group** para o subnet group.\n\n#### Configurar credenciais de forma segura\n\nÉ importante **não expor senhas** no código. Podemos usar variáveis do Terraform para passar a senha do banco. Idealmente, usar o AWS Secrets Manager ou Parameter Store para armazená-las, mas para simplificar, usaremos variável.\n\nVamos adicionar no Terraform:\n\n```hcl\n# 1. Grupo de sub-rede para RDS (utiliza sub-rede privada existente)\nresource \"aws_db_subnet_group\" \"bff_db_subnets\" {\n  name       = \"bff-db-subnet-group\"\n  subnet_ids = [aws_subnet.private.id]   # usar sub-rede privada para instância do RDS\n  tags = {\n    Name = \"bff-db-subnet-group\"\n  }\n}\n\n# 2. Security Group para o RDS\nresource \"aws_security_group\" \"rds\" {\n  name        = \"bff-rds-sg\"\n  description = \"Permite acesso DB do SG do backend\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    description     = \"MySQL do backend\"\n    from_port       = 3306\n    to_port         = 3306\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.backend_api.id]  # somente backend pode conectar\n  }\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  tags = { Name = \"bff-rds-sg\" }\n}\n\n# 3. Instância RDS MySQL\nvariable \"db_password\" {\n  description = \"Senha do usuário admin do banco de dados\"\n  type        = string\n  sensitive   = true\n}\n\nresource \"aws_db_instance\" \"bffdb\" {\n  allocated_storage    = 20\n  engine               = \"mysql\"\n  engine_version       = \"8.0\"\n  instance_class       = \"db.t3.micro\"\n  identifier           = \"bff-database\"\n  username             = \"admin\"             # usuário administrador\n  password             = var.db_password\n  db_name              = \"bffapp\"            # nome do database inicial\n  publicly_accessible  = false               # não ter IP público\n  vpc_security_group_ids = [aws_security_group.rds.id]\n  db_subnet_group_name   = aws_db_subnet_group.bff_db_subnets.name\n\n  # Habilitar deletar sem snapshot para fins de teste (não faça isso em produção sem backup!)\n  skip_final_snapshot = true\n\n  tags = {\n    Name = \"bff-mysql-db\"\n  }\n}\n```\n\nExplicação:\n\n- **aws_db_subnet_group.bff_db_subnets**: agrupa a(s) sub-rede(s) privadas para o RDS usar. Assim garantimos que o banco vai rodar dentro da nossa subnet privada (isolado da internet).\n- **aws_security_group.rds**: firewall do banco. Apenas a porta 3306 (MySQL) liberada e apenas para o SG do backend. Isso significa que somente instâncias marcadas com **bff-backend-sg** poderão chegar no MySQL. Mesmo dentro da VPC, outra instância que não tenha esse SG não conseguirá conectar (por padrão RDS também requer SSL ou credenciais, mas essa camada de rede já reforça segurança).\n- **aws_db_instance.bffdb**: cria a instância MySQL. Definimos um tamanho pequeno (20GB storage, classe t3.micro), definimos **publicly_accessible = false** para evitar IP público. Passamos o SG e subnet group criados para situar corretamente. A senha vem de **var.db_password** que marcamos como sensível (assim não aparece em logs do Terraform). Observação: Em produção, recomendamos não codificar senhas no tfvars simples; usar AWS Secrets ou Path no SSM com **data \"aws_ssm_parameter\"** por exemplo. Aqui mantemos simples com variável.\n\n> ⚠️ **Amazon RDS em sub-rede privada:**  \n> Colocar o banco em sub-rede privada é uma prática comum de segurança. Dessa forma, mesmo se alguém descobrisse credenciais, não conseguiria se conectar de fora da AWS, pois o host do DB não é acessível externamente. A AWS recomenda utilizar mecanismos como esse em conjunto com IAM para reforçar a proteção dos dados.\n\n### Ajustando o Backend para usar o RDS\n\nCom o banco pronto, precisamos atualizar nosso serviço backend para conectar-se a ele em vez do banco local. No caso do nosso exemplo, antes o backend respondia com uma mensagem estática. Vamos simular uma query ao banco: por exemplo, ler uma tabela de cumprimentos e devolver um aleatório. Não entraremos em SQL detalhado, mas focaremos em como o backend sabe onde o DB está.\n\nPrecisamos passar para o backend:\n\n- **HOST do DB**: o endpoint do RDS (pode ser obtido via **aws_db_instance.bffdb.address**).\n- **Usuário e Senha**: admin e variável password.\n- **Nome do banco**: \"bffapp\" como definimos.\n\nPara simplificar, podemos passar isso via variáveis de ambiente no user_data ou via arquivo de config. Vamos optar por definir no user_data do backend e ajustar o código para usar MySQL. Teríamos que instalar um client MySQL (ex: **mysql** CLI ou talvez um driver npm **mysql2**).\n\nPor brevidade, vamos supor que a gente instala o CLI do MySQL e realiza uma consulta simples via comando shell (só para validar conectividade) quando a instância subir. E podemos alterar a resposta da API para confirmar conexão.\n\nAlteração no **user_data** do backend:\n\n```hcl\n# Adicionar no user_data do backend (na criação ou via novo apply)\nuser_data = <<-EOF\n            #!/bin/bash\n            sudo yum update -y\n            # Instalar MySQL client e Node.js\n            sudo yum install -y mariadb  # MariaDB client to connect to MySQL\n            curl -sL https://rpm.nodesource.com/setup_16.x | sudo bash -\n            sudo yum install -y nodejs\n\n            # Esperar alguns segundos para garantir que RDS já esteja acessível\n            sleep 30\n\n            # Testar conexão com o DB\n            mysql -h ${aws_db_instance.bffdb.address} -u admin -p${var.db_password} -e \"SHOW DATABASES;\" > /home/ec2-user/db_check.txt 2>&1\n\n            # Iniciar servidor Node (exemplo simples)\n            cat > backend.js << 'EOL'\n            const http = require('http');\n            const { execSync } = require('child_process');\n            const server = http.createServer((req, res) => {\n              if (req.url === '/api/dbcheck' && req.method === 'GET') {\n                try {\n                  const result = execSync('mysql -h ${aws_db_instance.bffdb.address} -u admin -p${var.db_password} -e \"SELECT 1;\"');\n                  res.writeHead(200, {'Content-Type': 'application/json'});\n                  res.end(JSON.stringify({ db: \"ok\" }));\n                } catch (err) {\n                  res.writeHead(500, {'Content-Type': 'application/json'});\n                  res.end(JSON.stringify({ db: \"error\", detail: err.message }));\n                }\n              } else {\n                res.writeHead(404);\n                res.end();\n              }\n            });\n            server.listen(3000);\n            EOL\n            node backend.js &\n            EOF\n```\n\n**O que fizemos:**\n\n- Instalamos o cliente **mariadb** para ter o comando **mysql** disponível.\n- Colocamos **sleep 30** para aguardar 30 segundos antes de tentar conectar (RDS pode demorar alguns instantes para estar disponível após criação).\n- Executamos um comando **mysql** para listar databases e redirecionamos saída para um arquivo **db_check.txt**. Isso serve para debug: se a conexão falhou, esse arquivo terá o erro.\n- Ajustamos o servidor Node para ter um endpoint **/api/dbcheck** que simplesmente tenta executar um **SELECT 1** no banco (usando **execSync** chamando o cliente mysql) e retorna **{\"db\":\"ok\"}** se sucesso ou erro se falhar. Essa não é a forma mais elegante (ideal seria usar um driver Node MySQL), mas é a mais rápida aqui para validar conectividade.\n\nCom isso, podemos aplicar a alteração. O Terraform vai perceber mudanças no **user_data** do **aws_instance.backend** e aplicar (obs: mudar user_data implica recriar a instância por ser campo que não atualiza em execução; esteja preparado para downtime do backend durante a troca, ou use **user_data_replace_on_change = true** se quiser controlar).\n\nApós subir, testar do frontend ou via SSH:\n\n- Do frontend (que tem acesso à internet e, via SG, ao backend): **curl http://<IP_privado_backend>:3000/api/dbcheck**. Deverá retornar **{\"db\":\"ok\"}** se a conexão MySQL do backend foi bem-sucedida. Caso contrário, ver o arquivo **db_check.txt** na instância backend para diagnosticar (pode acusar acesso negado – verificar SG, ou usuário/senha errada, ou RDS não pronto).\n- Também é útil verificar no console AWS RDS os indicadores: a instância RDS deve estar em status \"available\". Pegue o endpoint endpoint (Endpoint & Port nas informações do RDS) e confira que condiz com o que o Terraform mostra no output (podemos criar um output para endpoint também).\n\n### Benefícios da separação do DB\n\nNeste ponto, temos:\n\n- **Frontend:** instância própria servindo interface.\n- **Backend:** instância própria servindo API e comunicando com DB.\n- **Banco de Dados:** serviço RDS gerenciado, acessível apenas pelo backend.\n\nEstamos próximos de uma arquitetura de microsserviços tradicional. O backend BFF agora é um serviço stateless (sem armazenamento local, já que usa um DB externo). Podemos facilmente escalar esse backend replicando instâncias (colocando atrás de um balanceador de carga, por exemplo) sem risco de divergência de dados, pois todos usam o mesmo RDS central.\n\nColocar o DB em RDS traz robustez: backups automáticos, facilidade de failover (se configurássemos Multi-AZ), entre outros benefícios. Também isolamos mais as falhas – um problema no backend não corrompe o banco tão facilmente, e _vice-versa_ o banco pode ser mantido/atualizado independentemente do código do backend.\n\nPara fechar essa parte, podemos apagar a instância antiga monolítica se ainda estiver rodando, pois já migramos toda funcionalidade para as novas instâncias + RDS. Agora temos o **BFF Backend** implementado como um microsserviço Node em sua VM, comunicando-se com um DB dedicado.\n\n## Recapitulando a Arquitetura Atual\n\nVamos visualizar como ficou nossa arquitetura após este tutorial:\n\n![tfBackend](/tfBackend.svg)\n\n_Figura: Arquitetura com microsserviço backend e banco de dados isolado. O Frontend está na subnet pública respondendo ao usuário; ele chama a API do Backend (que agora roda numa subnet privada). O Backend consulta o banco MySQL no RDS, também na subnet privada. Linhas pontilhadas indicam limites de sub-rede e flechas mostram o fluxo de comunicação._\n\nCom essa estrutura, já atingimos um nível de arquitetura de microsserviço web: conteúdo estático e cliente separados do servidor de API e este separado do armazenamento de dados. No próximo (e último) tutorial, introduziremos o **Amazon API Gateway** para gerenciar as chamadas para o backend BFF. O API Gateway funcionará como uma porta de entrada única para a API, oferecendo recursos adicionais como autenticação, limitación de throughput, caching e facilidade de roteamento sem expor diretamente nosso backend. Essa é uma prática comum em designs BFF: usar um _gateway_ para orquestrar múltiplos serviços de backend e apresentar ao frontend uma interface unificada."
  },
  {
    "id": "50fe5d55-025d-4ffe-8a62-cb80b9633367",
    "title": "APP API Gateway Com Terraform",
    "description": "Neste último tutorial da série Terraform, você aprenderá a criar um API Gateway que atua como ponto de entrada para a aplicação BFF. O módulo apresentado ilustra a configuração de endpoints, integrações com os recursos backend e regras de segurança, possibilitando uma gestão centralizada das APIs e o roteamento adequado das requisições para os microsserviços.",
    "tool": "Terraform",
    "level": "intermediario",
    "tags": [
      "API",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/50fe5d55-025d-4ffe-8a62-cb80b9633367",
    "markdown": "## Por que usar um API Gateway?\n\nNa arquitetura atual, o frontend se comunica diretamente com o backend BFF via HTTP interno. Isso funciona, mas introduzir um **API Gateway** traz diversas vantagens em ambientes de microsserviços e BFF. O Amazon API Gateway, segundo a definição oficial, _“é um serviço AWS para criar, publicar, manter, monitorar e proteger APIs REST, HTTP e WebSocket em qualquer escala”_. Ele age como uma “porta de entrada” para suas APIs, lidando com roteamento de requisições, controle de tráfego, autenticação, monitoramento e versionamento. Na prática, colocar o API Gateway na frente do nosso backend nos permite:\n\n- **Unificar endpoints**: O frontend passa a chamar um domínio do API Gateway, sem se preocupar com IPs internos do backend.\n- **Melhorar segurança**: Podemos habilitar autenticação, SSL, throttling, **CORS** e outras proteções no Gateway, reduzindo a exposição direta do backend.\n- **Facilitar escalabilidade e mudanças**: Se no futuro substituirmos o backend por outra implementação ou dividirmos em vários microserviços, o frontend não precisa mudar – o API Gateway pode rotear para múltiplos serviços internamente. Isso é particularmente alinhado ao padrão BFF, onde poderíamos ter diversos microserviços e um gateway agregando para o cliente.\n- **Observabilidade**: O API Gateway oferece logs e métricas integradas (via CloudWatch) para cada chamada, facilitando monitorar o uso da API.\n\nNo nosso cenário BFF simples (um frontend e um backend), o API Gateway pode parecer opcional. Mas didaticamente, vamos usá-lo para demonstrar essas capacidades e nos preparar para futuros crescimentos.\n\n## Implementando o Amazon API Gateway via Terraform\n\nO API Gateway pode ser configurado de diversas formas. Podemos criar uma API REST (v1) ou uma API HTTP (v2) mais simples. Aqui usaremos a API REST tradicional para ilustrar mais opções. Precisaremos configurar:\n\n- Um **Rest API** em si (nome, descrição).\n- Um **Recurso** (path) e métodos. Podemos usar um recurso proxy genérico (**/{proxy+}**) com método ANY para encaminhar tudo, ou definir rotas específicas.\n- Uma **Integração** do método com nosso backend. Vamos usar integração HTTP (HTTP_PROXY) apontando para o endpoint do backend.\n- Um **Deployment** e **Stage** para tornar a API acessível em uma URL pública.\n\nComo nosso backend BFF está rodando em uma instância EC2, para o API Gateway alcançá-lo temos duas possibilidades:\n\n1. **Integração via VPC Link**: Configurar o API Gateway para acessar um alvo dentro da VPC (geralmente precisa de Network Load Balancer apontando para a instância ou serviço).\n2. **Integração HTTP pública**: Expor o backend publicamente e deixar o API Gateway chamá-lo via internet.\n\nA opção 1 é a ideal em produção (mantém backend privado). Mas configurações de VPC Link e NLB seriam complexas para este tutorial. Dado que nosso backend _temporariamente_ está com IP público (pois não adicionamos NAT), podemos aproveitar isso para simplificar: faremos o API Gateway chamar o endpoint HTTP público do backend. **Atenção**: Isso significa que, se fizemos o backend realmente privado, precisaríamos alterar para público ou usar NLB. Para fins didáticos, vamos supor que o backend tem um DNS público acessível e a porta 3000 aberta para o API Gateway. Para não abrir geral, podemos permitir no SG do backend acesso da API Gateway (mas os IPs fonte do Gateway não são fixos facilmente). Então, leve em conta que isto é uma concessão para didática; em um deploy profissional faríamos a integração privada.\n\n### Configuração no Terraform\n\nVamos definir os recursos do API Gateway (versão REST):\n\n```hcl\n# 1. Criação da API Gateway REST\nresource \"aws_api_gateway_rest_api\" \"bff_api\" {\n  name        = \"BFFTutorialAPI\"\n  description = \"API Gateway para backend BFF\"\n}\n\n# 2. Recurso proxy {proxy+} no API Gateway (para encaminhar qualquer path para o backend)\nresource \"aws_api_gateway_resource\" \"proxy\" {\n  rest_api_id = aws_api_gateway_rest_api.bff_api.id\n  parent_id   = aws_api_gateway_rest_api.bff_api.root_resource_id\n  path_part   = \"{proxy+}\"\n}\n\n# 3. Método ANY no recurso proxy (aceita todos métodos HTTP)\nresource \"aws_api_gateway_method\" \"any\" {\n  rest_api_id   = aws_api_gateway_rest_api.bff_api.id\n  resource_id   = aws_api_gateway_resource.proxy.id\n  http_method   = \"ANY\"\n  authorization = \"NONE\"  # sem auth por enquanto (público)\n  request_parameters = {\n    \"method.request.path.proxy\" = true  # permite passar adiante o proxy path\n  }\n}\n\n# 4. Integração HTTP Proxy com nosso backend\nresource \"aws_api_gateway_integration\" \"http_proxy\" {\n  rest_api_id = aws_api_gateway_rest_api.bff_api.id\n  resource_id = aws_api_gateway_resource.proxy.id\n  http_method = aws_api_gateway_method.any.http_method\n  type        = \"HTTP_PROXY\"\n  integration_http_method = \"ANY\"\n  uri         = \"http://${aws_instance.backend.public_dns}:3000/{proxy}\"\n  # Mapeia parâmetros de caminho para o proxy\n  request_parameters = {\n    \"integration.request.path.proxy\" = \"method.request.path.proxy\"\n  }\n}\n\n# 5. Deployment da API\nresource \"aws_api_gateway_deployment\" \"bff_api_dep\" {\n  depends_on = [aws_api_gateway_integration.http_proxy]  # garante integração criada antes\n  rest_api_id = aws_api_gateway_rest_api.bff_api.id\n  stage_name  = \"dev\"\n  description = \"Deploy inicial da API BFF\"\n}\n```\n\nAqui:\n\n- Criamos a API e um recurso **{proxy+}** sob o root (ou seja, qualquer path depois do **/** root).\n- Definimos o método ANY no recurso proxy, sem autenticação. A configuração **method.request.path.proxy = true** e na integração **integration.request.path.proxy = method.request.path.proxy** é necessária para o API Gateway entender e repassar os segmentos do path adequadamente.\n- A integração HTTP_PROXY aponta para **http://${aws_instance.backend.public_dns}:3000/{proxy}**. Ou seja, se alguém chamar **GET /hello** na API Gateway, ele fará um GET em **http://<DNS-do-backend>:3000/hello**. Estamos usando o DNS público do backend (**public_dns**) – poderia ser IP também. Esse DNS é algo como **ec2-3-XX-XXX-XX.compute-1.amazonaws.com**. Como deixamos o backend com IP público, isso funciona. (Novamente: em produção faríamos diferente).\n- Depois fazemos o deployment vinculando tudo no stage \"dev\". Isso cria a URL acessível, do formato **https://{api-id}.execute-api.{region}.amazonaws.com/dev**.\n\nPodemos adicionar outputs para a URL base da API:\n\n```hcl\noutput \"api_invoke_url\" {\n  value = \"${aws_api_gateway_deployment.bff_api_dep.invoke_url}\"\n  description = \"URL de invocação do API Gateway\"\n}\n```\n\n(Ou construir manualmente: **https://${aws_api_gateway_rest_api.bff_api.id}.execute-api.${var.region}.amazonaws.com/dev**.)\n\n### Testando o API Gateway\n\nDepois de **terraform apply**, obtenha a URL do API (pela saída ou AWS Console). Vamos supor algo como:\n\n```\nhttps://abc123.execute-api.us-east-1.amazonaws.com/dev\n```\n\nNosso método proxy ANY significa que qualquer caminho após **/dev** será encaminhado. Então:\n\n- Acessar **GET https://abc123.execute-api.us-east-1.amazonaws.com/dev/api/hello** deve acionar o gateway, que por sua vez chamará **http://backend:3000/api/hello**. Se nosso backend estiver rodando e respondendo, deveremos receber o JSON **{\"message\":\"Olá do Backend BFF!\"}** como resposta via gateway.\n- Podemos testar também o **/api/dbcheck** se mantivemos no backend – seria **.../dev/api/dbcheck**.\n\nSe tudo deu certo, o frontend agora pode ser ajustado para usar essa URL do API Gateway em vez de chamar o backend diretamente. No nosso nginx config do Tutorial 3, poderíamos alterar o **proxy_pass** para apontar para o endpoint público do API Gateway (ou simplesmente, no código front-end JS, usar o endpoint do API gateway). Por exemplo, no HTML do frontend, mudar:\n\n```js\nfetch('/api/hello')\n```\n\npara\n\n```js\nfetch('https://abc123.execute-api.us-east-1.amazonaws.com/dev/api/hello')\n```\n\nAssim o frontend chamaria a API Gateway (observação: precisamos habilitar CORS no API Gateway para permitir o domínio do frontend – podemos configurar isso adicionando um método OPTIONS e cabeçalhos de resposta. Por brevidade, podemos habilitar CORS pelo console nas configurações do recurso, ou suprimir a restrição se acessarmos frontend e API pelo mesmo domínio com CloudFront, etc. CORS é um detalhe importante: o API Gateway pode adicionar cabeçalhos **Access-Control-Allow-Origin** facilmente se configurado. Se o seu frontend for servir a partir de um domínio diferente do domain da API, será necessário configurar as respostas do Gateway para incluir o header de permissão do domínio do frontend).\n\n### Revisão dos Benefícios e Conclusão da Infraestrutura BFF\n\nAgora temos a arquitetura BFF finalizada:\n\n![tfAPI](/tfAPI.svg)\n\n_Figura: Arquitetura final com API Gateway. O frontend chama o API Gateway em vez de acessar diretamente o backend. O API Gateway encaminha as requisições para o backend BFF, que interage com o banco de dados conforme necessário. Todas as partes (frontend, backend, db) estão separadas e gerenciadas._\n\nCom essa infraestrutura:\n\n- O **usuário** obtém o frontend (página web) a partir do servidor dedicado ou CDN.\n- O **frontend** realiza chamadas AJAX para o **Amazon API Gateway** (por exemplo, **/dev/api/hello**).\n- O **API Gateway** recebe a chamada, aplica políticas (limites, autenticação se houvesse, etc.), e encaminha para o endpoint do **backend BFF**.\n- O **backend BFF** processa, consulta o **banco de dados** RDS se necessário, e devolve a resposta.\n- O API Gateway então retorna a resposta ao frontend.\n\nEssa implementação ilustra o padrão **Backend for Frontend**: um backend (nosso serviço Node) otimizado para servir um determinado frontend, com a ajuda de um API Gateway atuando como fachada. Se tivéssemos outros frontends (por exemplo aplicativo mobile), poderíamos criar outro backend BFF específico e expor via o mesmo API Gateway sob outro path ou até outro gateway.\n\n**Referências oficiais e documentação:** Aproveitando esse exercício, vale consultar a documentação da AWS sobre API Gateway para entender recursos avançados. Por exemplo, o API Gateway pode lidar com autenticação via Amazon Cognito ou Lambda Authorizers, e suporta deployment de diferentes stages (dev, prod) e versionamento de APIs. Também revisite a seção de _Backends for Frontends pattern_ no blog da AWS, que explica a motivação de ter backends segmentados por experiência de usuário – exatamente o que construímos.\n\n## Passos Finais e Limpeza\n\nParabéns! Você construiu uma infraestrutura completa de uma aplicação seguindo o padrão BFF usando Terraform. Repassando os tutoriais, nós:\n\n- **Tutorial 1:** Criamos a base (Landing Zone) com rede (VPC, subnets) e segurança.\n- **Tutorial 2:** Lançamos a aplicação monolítica simples em uma instância EC2, confirmando funcionamento end-to-end.\n- **Tutorial 3:** Separarmos o frontend em sua própria instância, iniciando a transição para microsserviços.\n- **Tutorial 4:** Isolamos o backend e movemos o banco de dados para um serviço gerenciado (RDS), completando a separação dos componentes.\n- **Tutorial 5:** Implementamos o Amazon API Gateway na frente da API, atingindo a arquitetura alvo BFF com frontend simplificado e uma camada de API robusta.\n\nPara finalizar, não esqueça de desligar/remover recursos se este ambiente for apenas de teste, para evitar custos inesperados (RDS e instâncias EC2 geram cobranças enquanto rodando). Você pode usar **terraform destroy** para apagar todos os recursos criados – graças ao Terraform, a limpeza é mais fácil e rastreável. Em ambientes reais, manter a infraestrutura codificada permite recriar semelhantes setups em diferentes contas (dev/staging/prod), garantindo consistência.\n\nEsperamos que esta série tenha demonstrado de forma prática como utilizar **Infraestrutura como Código** com Terraform na AWS, passando de um simples monólito a uma arquitetura moderna com microsserviços e BFF. Cada passo foi complementado com explicações e referências oficiais (AWS e Terraform) para embasar as decisões. A partir daqui, você pode expandir o projeto – por exemplo, adicionando auto scaling, usando ECS ou Lambda para o backend, integrando autenticação de usuários, etc. A base construída (Landing Zone e padrões de módulos Terraform) pode ser reutilizada nesses incrementos. Boa codificação de infra e até a próxima!"
  },
  {
    "id": "dfc9df29-0fb7-4d2f-a779-a1cc416156b7",
    "title": "Landing Zone com Cloudformation",
    "description": "Este tutorial mostra como construir uma Landing Zone utilizando AWS CloudFormation. Você aprenderá a organizar sua conta AWS com uma configuração de rede robusta, definindo VPC, sub-redes (públicas e privadas), gateways de Internet/NAT, e políticas de segurança por meio de templates. Essa base é essencial para garantir autonomia, segurança e governança na implantação de uma aplicação BFF, tanto no formato monolítico quanto em microsserviços.",
    "tool": "CloudFormation",
    "level": "intermediario",
    "tags": [
      "Landing Zone",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/dfc9df29-0fb7-4d2f-a779-a1cc416156b7",
    "markdown": "Neste primeiro tutorial, vamos preparar a **Landing Zone**, ou seja, a base de infraestrutura em nuvem onde a aplicação será implantada. Em termos simples, isso significa criar a rede (VPC), sub-redes públicas e privadas, e os componentes de rede necessários para suportar tanto a versão monolítica quanto a versão baseada em microsserviços da nossa aplicação. A Landing Zone garante que começaremos com um ambiente de nuvem organizado, isolado e seguro para construir as próximas etapas.\n\n### 1.1 Visão Geral da Arquitetura de Rede\n\nNosso objetivo é configurar uma **VPC (Virtual Private Cloud)** própria, separada da VPC padrão, com sub-redes públicas e privadas. Isso permitirá controlar completamente o tráfego de rede da aplicação. Em uma **subnet (sub-rede)** pública colocaremos recursos que precisam de acesso direto à internet (como servidores web ou gateways), enquanto em subnets privadas colocaremos recursos internos (como bancos de dados) para mantê-los isolados do acesso externo. Em outras palavras, *uma subnet é um intervalo de endereços IP dentro da VPC no qual podemos lançar recursos AWS específicos (por exemplo, instâncias EC2)*.\n\nNa borda da VPC, usaremos um **Internet Gateway (IGW)** para permitir comunicação entre a VPC e a internet pública. O IGW é um componente altamente disponível que atua como ponte entre a VPC e a internet, permitindo que instâncias com IP público enviem e recebam tráfego da internet. Cada subnet pública terá uma rota apontando para o IGW, para que instâncias nessas subnets possam receber tráfego externo. Já para as subnets privadas, configuraremos um **NAT Gateway** em cada zona de disponibilidade. O NAT Gateway fornece **Network Address Translation**, permitindo que instâncias em subnets privadas iniciem conexões para fora (por exemplo, baixar atualizações ou acessar serviços externos) sem serem acessíveis da internet. Com o NAT, recursos privados podem consumir APIs e atualizar pacotes, mas continuam inacessíveis de fora (o tráfego externo não pode iniciar conexão com eles). \n\nResumo dos componentes planejados na Landing Zone:\n\n- **VPC customizada:** intervalo IPv4, DNS habilitado.\n- **Subnets públicas** (duas, uma em cada AZ) para recursos expostos (por exemplo, servidor web, API Gateway).\n- **Subnets privadas** (duas) para recursos internos (por exemplo, banco de dados).\n- **Internet Gateway:** permite tráfego internet nas subnets públicas.\n- **NAT Gateways:** um em cada subnet pública, para saída de instâncias das subnets privadas.\n- **Route Tables:** tabela de rotas pública (rota padrão para IGW) associada às subnets públicas; tabelas privadas (rota padrão para NAT) associadas às subnets privadas.\n- **Parâmetros configuráveis:** cidr da VPC, blocos cidr de subnets, IDs de AZ etc., para tornar a infraestrutura reutilizável em diferentes regiões.\n\nAbaixo está um diagrama resumindo a arquitetura de rede que iremos criar:\n\n![cfLandingzone](/cfLandingzone.svg)\n\n*Figura 1 – Diagrama da VPC com subnets públicas/privadas, Internet Gateway e NAT Gateways.* (Componentes duplicados em duas zonas de disponibilidade para alta disponibilidade).\n\n### 1.2 Criando a VPC e Subnets via CloudFormation\n\nCompreendida a arquitetura, vamos construir tudo usando **AWS CloudFormation** para infraestrutura como código. CloudFormation nos permite descrever os recursos em um template (YAML, neste caso) e implantá-los de forma reproduzível. Iniciaremos definindo a VPC e, em seguida, as subnets.\n\nNo template YAML, configuramos a VPC com propriedades importantes como o bloco CIDR e opções de DNS. Habilitamos **EnableDnsSupport** e **EnableDnsHostnames** para que instâncias na VPC recebam nomes DNS, facilitando acesso via nome ao invés de IP. *Por padrão, uma VPC customizada não atribui hostname DNS às instâncias, a menos que essas opções estejam ativadas.* Mantemos **InstanceTenancy** padrão como **default** (hardware compartilhado) para evitar custos adicionais.\n\nAbaixo está o trecho do template para a VPC e subnets, incluindo comentários explicativos:\n\n```yaml\nAWSTemplateFormatVersion: 2010-09-09\nDescription: \"Landing Zone - VPC with public/private subnets, IGW, and NAT\"\n\nParameters:\n  VpcCidr:\n    Description: \"Bloco CIDR da VPC\"\n    Type: String\n    Default: 10.0.0.0/16\n  PublicSubnet1Cidr:\n    Description: \"CIDR da Subnet Pública 1\"\n    Type: String\n    Default: 10.0.2.0/24\n  PublicSubnet2Cidr:\n    Description: \"CIDR da Subnet Pública 2\"\n    Type: String\n    Default: 10.0.3.0/24\n  PrivateSubnet1Cidr:\n    Description: \"CIDR da Subnet Privada 1\"\n    Type: String\n    Default: 10.0.0.0/24\n  PrivateSubnet2Cidr:\n    Description: \"CIDR da Subnet Privada 2\"\n    Type: String\n    Default: 10.0.1.0/24\n\nResources:\n  # VPC principal da Landing Zone\n  AppVPC:\n    Type: AWS::EC2::VPC\n    Properties:\n      CidrBlock: !Ref VpcCidr\n      EnableDnsSupport: true            # Habilita resolução DNS dentro da VPC (necessário p/ DNS público)\n      EnableDnsHostnames: true          # Garante que instâncias recebam hostname DNS público se tiverem IP público\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-VPC\"\n\n  # Subnet Pública em AZ 1\n  PublicSubnetA:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref AppVPC\n      CidrBlock: !Ref PublicSubnet1Cidr\n      AvailabilityZone: !Select [0, !GetAZs ]   # pega a primeira AZ da região\n      MapPublicIpOnLaunch: true                # Instâncias lançadas aqui recebem IP público automaticamente\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PublicSubnetA\"\n\n  # Subnet Pública em AZ 2\n  PublicSubnetB:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref AppVPC\n      CidrBlock: !Ref PublicSubnet2Cidr\n      AvailabilityZone: !Select [1, !GetAZs ]\n      MapPublicIpOnLaunch: true\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PublicSubnetB\"\n\n  # Subnet Privada em AZ 1\n  PrivateSubnetA:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref AppVPC\n      CidrBlock: !Ref PrivateSubnet1Cidr\n      AvailabilityZone: !Select [0, !GetAZs ]\n      MapPublicIpOnLaunch: false               # Subnet privada NÃO atribui IP público às instâncias\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PrivateSubnetA\"\n\n  # Subnet Privada em AZ 2\n  PrivateSubnetB:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref AppVPC\n      CidrBlock: !Ref PrivateSubnet2Cidr\n      AvailabilityZone: !Select [1, !GetAZs ]\n      MapPublicIpOnLaunch: false\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PrivateSubnetB\"\n```\n\n**Explicando o código acima:** Definimos parâmetros para permitir ajustar os blocos CIDR conforme necessidade. Em **AppVPC**, usamos **!Ref VpcCidr** para definir o espaço de endereços (padrão 10.0.0.0/16). Ativamos explicitamente **EnableDnsSupport** e **EnableDnsHostnames** para que a resolução de nomes e atribuição de hostname público funcionem dentro da VPC (isso é **fundamental para instâncias nas subnets públicas serem acessíveis por nome DNS público**, facilitando testes e integrações). As subnets recebem um nome pelo tag e cada uma é mapeada para uma AZ diferente usando a função **!GetAZs** – isso garante alta disponibilidade distribuindo recursos entre zonas. Em subnets públicas, **MapPublicIpOnLaunch: true** faz com que qualquer EC2 lançada ali ganhe automaticamente um IP público (economiza ter que criar Elastic IP manualmente para cada instância pública). Já nas subnets privadas usamos **false** para evitar IPs públicos – recursos privados devem permanecer inacessíveis externamente, seguindo boas práticas de segurança.\n\n### 1.3 Internet Gateway, Tabelas de Rotas e NAT Gateway\n\nCom a VPC e as subnets definidas, precisamos agora permitir conectividade adequada. Primeiro, criaremos e anexaremos um **Internet Gateway** à VPC. Em seguida, configuraremos as **Route Tables** (tabelas de rotas) para direcionar tráfego de saída corretamente: a tabela de rota das subnets públicas terá um destino padrão (0.0.0.0/0) apontando para o IGW, enquanto as tabelas das subnets privadas terão rota padrão apontando para um NAT Gateway.\n\nIncluímos no template a criação do Internet Gateway e uma attachement dele à VPC. Depois, criamos duas tabelas de rotas – uma será associada a **ambas** subnets públicas (podemos usar uma só tabela para todas as públicas) e cada subnet privada terá sua própria tabela com rota via NAT (usamos duas NATs e duas tabelas privadas, uma por AZ, para alta disponibilidade).\n\n```yaml\n  # Internet Gateway para acesso à internet\n  InternetGateway:\n    Type: AWS::EC2::InternetGateway\n    Properties:\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-IGW\"\n\n  # Anexa o IGW à VPC\n  AttachGateway:\n    Type: AWS::EC2::VPCGatewayAttachment\n    Properties:\n      VpcId: !Ref AppVPC\n      InternetGatewayId: !Ref InternetGateway\n\n  # Tabela de rota para Subnets Públicas (compartilhada)\n  PublicRouteTable:\n    Type: AWS::EC2::RouteTable\n    Properties:\n      VpcId: !Ref AppVPC\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PublicRoutes\"\n\n  # Rota padrão na tabela pública apontando para o IGW (internet)\n  PublicRoute:\n    Type: AWS::EC2::Route\n    Properties:\n      RouteTableId: !Ref PublicRouteTable\n      DestinationCidrBlock: 0.0.0.0/0        # qualquer destino\n      GatewayId: !Ref InternetGateway        # sai pelo Internet Gateway\n\n  # Associação das subnets públicas à tabela de rota pública\n  PublicSubnetARouteTableAssoc:\n    Type: AWS::EC2::SubnetRouteTableAssociation\n    Properties:\n      SubnetId: !Ref PublicSubnetA\n      RouteTableId: !Ref PublicRouteTable\n\n  PublicSubnetBRouteTableAssoc:\n    Type: AWS::EC2::SubnetRouteTableAssociation\n    Properties:\n      SubnetId: !Ref PublicSubnetB\n      RouteTableId: !Ref PublicRouteTable\n\n  # NAT Gateway na Subnet Pública A \n  NatGatewayA:\n    Type: AWS::EC2::NatGateway\n    Properties:\n      AllocationId: !GetAtt NatEIPA.AllocationId   # Aloca IP elástico para o NAT\n      SubnetId: !Ref PublicSubnetA\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-NAT-A\"\n\n  # Elastic IP para NAT A \n  NatEIPA:\n    Type: AWS::EC2::EIP\n    DependsOn: AttachGateway                     # IGW deve existir para EIP ser alocado em VPC\n    Properties:\n      Domain: vpc\n\n  # NAT Gateway na Subnet Pública B \n  NatGatewayB:\n    Type: AWS::EC2::NatGateway\n    Properties:\n      AllocationId: !GetAtt NatEIPB.AllocationId\n      SubnetId: !Ref PublicSubnetB\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-NAT-B\"\n\n  NatEIPB:\n    Type: AWS::EC2::EIP\n    DependsOn: AttachGateway\n    Properties:\n      Domain: vpc\n\n  # Tabela de rota para Subnet Privada A\n  PrivateRouteTableA:\n    Type: AWS::EC2::RouteTable\n    Properties:\n      VpcId: !Ref AppVPC\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PrivateRoutesA\"\n  # Rota padrão privada apontando para NAT A\n  PrivateRouteA:\n    Type: AWS::EC2::Route\n    Properties:\n      RouteTableId: !Ref PrivateRouteTableA\n      DestinationCidrBlock: 0.0.0.0/0\n      NatGatewayId: !Ref NatGatewayA\n  # Associação Subnet Privada A -> Tabela privada\n  PrivateSubnetARouteTableAssoc:\n    Type: AWS::EC2::SubnetRouteTableAssociation\n    Properties:\n      SubnetId: !Ref PrivateSubnetA\n      RouteTableId: !Ref PrivateRouteTableA\n\n  # Tabela de rota para Subnet Privada B\n  PrivateRouteTableB:\n    Type: AWS::EC2::RouteTable\n    Properties:\n      VpcId: !Ref AppVPC\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PrivateRoutesB\"\n  PrivateRouteB:\n    Type: AWS::EC2::Route\n    Properties:\n      RouteTableId: !Ref PrivateRouteTableB\n      DestinationCidrBlock: 0.0.0.0/0\n      NatGatewayId: !Ref NatGatewayB\n  PrivateSubnetBRouteTableAssoc:\n    Type: AWS::EC2::SubnetRouteTableAssociation\n    Properties:\n      SubnetId: !Ref PrivateSubnetB\n      RouteTableId: !Ref PrivateRouteTableB\n```\n\n**Detalhes importantes:** \n\n- O recurso **VPCGatewayAttachment** (**AttachGateway**) fixa o Internet Gateway na VPC. Somente com isso as rotas para o IGW tornam-se válidas. O IGW permite que recursos **com IPs públicos na VPC** sejam alcançáveis de fora e também façam acesso de saída à internet. Sem um IGW, nossas instâncias públicas ficariam isoladas apesar de terem IP público.\n- Criamos uma route table pública e associamos as duas subnets públicas a ela (usando **SubnetRouteTableAssociation**). Dentro dela, definimos uma rota padrão (**0.0.0.0/0**) apontando para o IGW, efetivamente dizendo: \"*todo tráfego não local deve sair pela internet*\".\n- Para as subnets privadas, usamos **NAT Gateways**. Um NAT Gateway é colocado *dentro* de uma subnet pública (pois ele mesmo precisa de internet) e recebe um Elastic IP próprio (garantindo IP fixo na saída). Configuramos dois NATs (NAT A e NAT B) – assim, se uma zona de AZ falhar, a outra ainda tem um NAT funcional. Cada NAT tem uma route table privada apontando para ele. Com isso, instâncias na Subnet Privada A terão seu tráfego roteado via NAT A (que está na Subnet Pública A), e de forma semelhante para B. O efeito é: instâncias privadas conseguem acessar a internet (via NAT), mas não possuem IP público, então nada na internet inicia conexão a elas. Isso segue o princípio de manter servidores de aplicação ou bancos de dados *privados e seguros*, permitindo apenas o tráfego de saída que eles precisem iniciar.\n\n- Usamos **DependsOn: AttachGateway** nos EIPs para NAT. Isso garante que o Internet Gateway já esteja anexado quando o CloudFormation pedir o EIP. Por quê? Ao alocar um EIP com **Domain: vpc**, é necessário que haja um IGW na VPC para rotear o tráfego do EIP; indicar a dependência evita erros de provisionamento.\n\n### 1.4 Saídas (Outputs) e Testes da Landing Zone\n\nPor fim, adicionamos *outputs* no template para capturar informações que serão úteis nos próximos tutoriais (e para inspeção). Vamos expor, por exemplo, o **ID da VPC** e os IDs das subnets criadas. Isso permitirá que outros templates (da aplicação) possam referenciá-los facilmente ao implantar instâncias EC2 ou outros recursos dentro da VPC. Em CloudFormation, podemos exportar outputs e depois importá-los em outro stack via **Fn::ImportValue**. \n\nAdicionamos ao final do template YAML:\n\n```yaml\nOutputs:\n  VPCId:\n    Description: \"ID da VPC criada na Landing Zone\"\n    Value: !Ref AppVPC\n    Export:\n      Name: !Sub \"${AWS::StackName}-VPCId\"\n  PublicSubnetA:\n    Description: \"ID Subnet Pública A\"\n    Value: !Ref PublicSubnetA\n    Export:\n      Name: !Sub \"${AWS::StackName}-PublicSubnetA\"\n  PublicSubnetB:\n    Description: \"ID Subnet Pública B\"\n    Value: !Ref PublicSubnetB\n    Export:\n      Name: !Sub \"${AWS::StackName}-PublicSubnetB\"\n  PrivateSubnetA:\n    Description: \"ID Subnet Privada A\"\n    Value: !Ref PrivateSubnetA\n    Export:\n      Name: !Sub \"${AWS::StackName}-PrivateSubnetA\"\n  PrivateSubnetB:\n    Description: \"ID Subnet Privada B\"\n    Value: !Ref PrivateSubnetB\n    Export:\n      Name: !Sub \"${AWS::StackName}-PrivateSubnetB\"\n```\n\n**Validação da Landing Zone:** Após criar o stack CloudFormation da Landing Zone, você pode verificar no console AWS VPC que a VPC e subnets foram criadas. Verifique se: \n- A VPC aparece com o CIDR especificado e DNS Hostnames ativado.\n- As subnets têm nomes/tag corretos e estão divididas em AZs distintas.\n- O Internet Gateway está anexado à VPC.\n- Nas tabelas de rotas, as subnets públicas têm rota para o IGW e as privadas para os NATs.\n- Os NAT Gateways estão em estado **Available** com EIPs associados.\n\nÉ recomendável testar conectividade básica: lançar temporariamente uma instância EC2 em cada subnet (pública e privada) e conferir:\n  - Instância em subnet pública: deve obter IP público e ser capaz de pingar uma URL externa (ex: **ping amazon.com**) ou instalar updates via **yum update**. Também deve ser acessível via SSH de fora (se grupo de segurança permitir, veremos isso mais adiante).\n  - Instância em subnet privada: não terá IP público, mas deve conseguir acessar a internet *através do NAT*. Por exemplo, via SSH (conectando primeiro em uma bastion na pública) ou registrando logs, verificar se um **curl http://ifconfig.me** retorna um IP (deverá ser o EIP do NAT, não o IP privado dela).\n\nCom a Landing Zone criada e validada, estamos prontos para implantar a aplicação de teste no próximo tutorial.\n"
  },
  {
    "id": "fe18363b-d20d-4125-bec1-c7bfbbf7bdad",
    "title": "APP monólito com Cloudformation",
    "description": "Neste tutorial com CloudFormation, o foco é a implantação de uma aplicação BFF monolítica em uma única instância EC2. O template provisiona todos os componentes necessários — backend, frontend, banco de dados e API — e inclui comentários que explicam a importância de cada recurso. Essa abordagem inicial permite testar a aplicação completa num ambiente simplificado, facilitando os primeiros passos na prática de IaC.",
    "tool": "CloudFormation",
    "level": "intermediario",
    "tags": [
      "monólito",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/fe18363b-d20d-4125-bec1-c7bfbbf7bdad",
    "markdown": "Agora que temos uma Landing Zone configurada, podemos implantar a primeira versão da nossa aplicação: um **monólito**. Nesta etapa, todos os componentes da aplicação – frontend, backend, API e banco de dados – serão executados juntos em uma única instância EC2 dentro da VPC. O objetivo é entender como provisionar uma instância completa via CloudFormation, configurar seu software automaticamente e validar o funcionamento da aplicação em formato monolítico. Em seguida, usaremos essa base para evoluir para a arquitetura BFF (Backend for Frontend) de microsserviços.\n\n**Sobre a aplicação de exemplo (Monólito):** Desenvolvemos uma aplicação simples em JavaScript (Node.js) que servirá de exemplo. Essa aplicação consiste em:\n- **Frontend:** Uma página web estática (HTML/CSS/JS) que exibe uma mensagem e consome uma API.\n- **Backend/API:** Um servidor Node.js (Express) que expõe uma API REST (**/api/mensagem**) para fornecer dados (uma mensagem de texto) armazenados no banco de dados.\n- **Banco de Dados:** Um banco MySQL simples contendo uma tabela de mensagens. Inicialmente, ele terá uma mensagem (\"Olá, Mundo!\") para retorno via API.\n- Tudo isso estará rodando na mesma instância, comunicando-se localmente.\n\nComo prova de funcionamento, o frontend fará uma requisição AJAX para a API e exibirá a mensagem retornada na página. Também incluiremos um endpoint de saúde (health check) simples para verificar rapidamente se o servidor está respondendo.\n\n### 2.1 Provisionando a Instância EC2 com CloudFormation\n\nUtilizaremos CloudFormation para criar a instância EC2 e automatizar toda a configuração via **User Data**. O User Data é um mecanismo que permite passar um script para rodar automaticamente no primeiro boot da instância. Vamos aproveitá-lo para instalar o Node.js, o banco de dados local, e iniciar nossa aplicação monolítica sem intervenção manual.\n\n**Recursos envolvidos no template:**\n- **Security Group:** definindo regras de firewall da instância (SSH e HTTP abertos para teste).\n- **EC2 Instance:** a própria instância, em uma subnet pública da nossa VPC, associada ao Security Group e com um script de user data que realiza toda a instalação/ configuração da aplicação.\n- **Output:** IP público ou DNS público da instância, para acessarmos a aplicação após implantação.\n\n**Escolha da AMI:** Usaremos uma AMI Linux padrão – por exemplo, Amazon Linux 2 – como sistema operacional base. O ID da AMI varia por região, então podemos usar um parâmetro do Systems Manager para pegar a AMI mais recente automaticamente. A AWS disponibiliza parâmetros SSM públicos, como **/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2** para Amazon Linux 2 (HVM EBS General Purpose). No template, utilizaremos a função **!Sub** com *dynamic reference* para obter o ID da AMI atual:\n\n```yaml\nParameters:\n  LatestAmiId:\n    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>\n    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2\n```\n\nAcima, definimos um parâmetro que consulta o Parameter Store pelo ID da última Amazon Linux 2 (64-bit x86). Assim, não precisamos codificar um ID AMI fixo (que ficaria obsoleto com o tempo). \n\n**Definição do Security Group e Instance:**\n\n```yaml\nResources:\n  AppSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: \"Acesso HTTP e SSH para o servidor monolítico\"\n      VpcId: !ImportValue LandingZone-VPCId                 # Importa o ID da VPC da Landing Zone criada no tutorial 1\n      SecurityGroupIngress:\n        # Porta 22 (SSH) aberta para acesso (aqui estamos abrindo globalmente 0.0.0.0/0 apenas para simplicidade de teste; recomendável restringir)\n        - IpProtocol: tcp\n          FromPort: 22\n          ToPort: 22\n          CidrIp: 0.0.0.0/0\n        # Porta 80 (HTTP) aberta para todos, para servir frontend e API\n        - IpProtocol: tcp\n          FromPort: 80\n          ToPort: 80\n          CidrIp: 0.0.0.0/0\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-AppSG\"\n\n  MonolithInstance:\n    Type: AWS::EC2::Instance\n    Properties:\n      InstanceType: t3.micro\n      ImageId: !Ref LatestAmiId\n      SubnetId: !ImportValue LandingZone-PublicSubnetA      # lança na subnet pública (Zona A)\n      SecurityGroupIds: \n        - !Ref AppSecurityGroup\n      KeyName: !Ref SshKeyName                              # Par de chaves para login SSH (passado via parâmetro no template)\n      UserData: \n        Fn::Base64: !Sub |\n          #!/bin/bash\n          # Atualiza pacotes básicos\n          yum update -y\n          # Instala utilitários necessários (compilador e Make para compilar módulos nativos, caso necessário)\n          yum install -y gcc-c++ make\n          # Instala repositório NodeSource e depois Node.js (LTS)\n          curl -sL https://rpm.nodesource.com/setup_16.x | bash -\n          yum install -y nodejs\n          \n          # Instala MariaDB (MySQL) servidor e cliente\n          yum install -y mariadb-server\n          systemctl start mariadb\n          systemctl enable mariadb\n\n          # Configura banco de dados inicial (cria schema e tabela)\n          mysql -e \"CREATE DATABASE appdb; \\\n                    CREATE USER 'appuser'@'localhost' IDENTIFIED BY 'SenhaApp123'; \\\n                    GRANT ALL ON appdb.* TO 'appuser'@'localhost'; \\\n                    USE appdb; \\\n                    CREATE TABLE mensagens (id INT AUTO_INCREMENT PRIMARY KEY, texto VARCHAR(255)); \\\n                    INSERT INTO mensagens (texto) VALUES ('Olá, Mundo!');\"\n          \n          # Cria diretório da aplicação\n          mkdir -p /home/ec2-user/app\n          chown ec2-user:ec2-user /home/ec2-user/app\n\n          # Cria arquivo da aplicação Node (Express server) com código-fonte\n          cat > /home/ec2-user/app/app.js << 'APPJS'\n          // Importa módulos\n          const express = require('express');\n          const mysql = require('mysql2');\n          const app = express();\n          const port = 80;  // vamos usar porta 80 para evitar especificar porta na URL\n\n          // Configura conexão com MySQL local\n          const db = mysql.createConnection({\n            host: 'localhost',\n            user: 'appuser',\n            password: 'SenhaApp123',\n            database: 'appdb'\n          });\n\n          // Servir arquivos estáticos do frontend (pasta 'public')\n          app.use(express.static('public'));\n\n          // Endpoint API: retorna a mensagem do banco de dados\n          app.get('/api/mensagem', (req, res) => {\n            db.query('SELECT texto FROM mensagens LIMIT 1', (err, results) => {\n              if (err) {\n                console.error(\"Erro ao consultar DB:\", err);\n                return res.status(500).json({ erro: 'Erro no servidor' });\n              }\n              const msg = results[0] ? results[0].texto : 'Nenhuma mensagem encontrada';\n              res.json({ mensagem: msg });\n            });\n          });\n\n          // Endpoint de health check simples\n          app.get('/api/health', (req, res) => {\n            res.send('OK');\n          });\n\n          // Inicia o servidor\n          app.listen(port, '0.0.0.0', () => {\n            console.log(\\`Servidor monolítico rodando na porta \\${port}\\`);\n          });\n          APPJS\n\n          # Cria arquivo HTML do frontend na pasta public\n          mkdir -p /home/ec2-user/app/public\n          cat > /home/ec2-user/app/public/index.html << 'INDEXHTML'\n          <!DOCTYPE html>\n          <html lang=\"pt-BR\">\n          <head>\n            <meta charset=\"UTF-8\" />\n            <title>App Monólito - BFF Demo</title>\n            <style>\n              body { font-family: Arial, sans-serif; margin: 2em; background: #f2f2f2; }\n              h1 { color: #333; }\n              #mensagem { font-size: 1.5em; color: #007BFF; }\n              .loading { color: gray; font-style: italic; }\n            </style>\n          </head>\n          <body>\n            <h1>Bem-vindo à Aplicação Monolítica!</h1>\n            <p>Mensagem do backend: <span id=\"mensagem\" class=\"loading\">carregando...</span></p>\n            <script>\n              // Faz requisição à API para obter a mensagem\n              fetch('/api/mensagem')\n                .then(response => response.json())\n                .then(data => {\n                  document.getElementById('mensagem').textContent = data.mensagem;\n                  document.getElementById('mensagem').classList.remove('loading');\n                })\n                .catch(err => {\n                  console.error('Erro ao buscar mensagem:', err);\n                  document.getElementById('mensagem').textContent = 'Erro ao obter mensagem';\n                });\n            </script>\n          </body>\n          </html>\n          INDEXHTML\n\n          # Ajusta permissões\n          chown -R ec2-user:ec2-user /home/ec2-user/app\n\n          # Entra no diretório da aplicação e instala dependências Node (Express e MySQL)\n          cd /home/ec2-user/app\n          sudo -u ec2-user npm init -y                    # cria package.json com padrão\n          sudo -u ec2-user npm install express mysql2     # instala pacotes necessários\n\n          # Inicia a aplicação Node em background\n          nohup sudo -u ec2-user node app.js > app.log 2>&1 &\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-MonolithHost\"\n```\n\nVamos dissecar as partes críticas do *User Data* acima, pois ele realiza toda mágica de configurar o servidor monolítico automaticamente:\n\n- **Atualização e instalações base:** Atualizamos a lista de pacotes e instalamos **gcc-c++** e **make** (pré-requisitos para construir alguns pacotes Node nativos). Depois, usamos um script do NodeSource para habilitar o repositório do Node.js 16 LTS e instalamos o Node.js em si. A seguir, instalamos o servidor de banco de dados MariaDB (muito similar ao MySQL) e o iniciamos. Essas etapas automatizadas garantem que a instância tenha Node e MySQL prontos para uso.\n\n- **Configuração do banco de dados:** Usando o cliente **mysql**, executamos uma sequência de comandos SQL não interativos via **-e**. Criamos um banco de dados **appdb** e um usuário **appuser** com senha (exemplo didático **SenhaApp123** – em ambientes reais, você usaria algo seguro e não exporia em texto plano). Em seguida, criamos uma tabela **mensagens** e inserimos uma linha com texto \"Olá, Mundo!\". Assim, nosso banco já está populado com um valor para a API retornar. Tudo isso ocorre durante o boot.\n\n- **Deploy da aplicação Node.js:** Criamos um diretório **/home/ec2-user/app** para o código, garantindo que o dono seja **ec2-user** (o usuário padrão da instância Amazon Linux). Em seguida, usamos o comando **cat** para criar dois arquivos: **app.js** (código do servidor Node) e **index.html** (frontend). Repare nos *EOF markers* **<< 'APPJS'** e **<< 'INDEXHTML'** – eles permitem inserir um bloco de texto exatamente como está no script de user data, facilitando escrever múltiplas linhas. Dentro do código Node:\n  - Criamos uma aplicação Express simples, definindo rotas **/api/mensagem** e **/api/health**. \n  - O código de **/api/mensagem** se conecta ao MariaDB local e busca a mensagem inserida, retornando-a em formato JSON. Adicionamos tratamento básico de erro (logar no console e retornar status 500).\n  - Usamos **express.static('public')** para servir arquivos estáticos. Isso permite que o front-end (nosso **index.html** e qualquer outro arquivo estático na pasta **public**) seja servido diretamente pelo Express.\n  - Iniciamos o servidor na porta 80 e ouvindo em **0.0.0.0** (todas interfaces) para aceitar conexões externas.\n  \n  No arquivo HTML, incluímos um script que faz um fetch em **/api/mensagem** assim que a página carrega e exibe o conteúdo retornado dentro do elemento **<span id=\"mensagem\">**. Estilizamos o texto para destaque. Enquanto a resposta não chega, exibimos \"carregando...\" em itálico cinza (classe CSS **loading**).\n\n- **Instalação de dependências e execução:** Após criar os arquivos, precisamos instalar as dependências Node (Express e mysql2). Usamos **npm init -y** para gerar um **package.json** padrão rapidamente, e então **npm install express mysql2**. Note o uso de **sudo -u ec2-user**: isso executa o comando como o usuário normal, não root. Fizemos isso para que os arquivos e **node_modules** fiquem com permissão do ec2-user (evitando problemas de permissão ao rodar a app como usuário normal). Por fim, executamos **node app.js** em background usando **nohup** (assim o processo continua rodando após o término do user data). Redirecionamos a saída para **app.log** para fins de depuração. Agora, teoricamente, nosso servidor Node está rodando atendendo na porta 80 e conectado ao banco local.\n\nEssa abordagem demonstra *bootstrapping completo* de uma instância EC2 via user data. Segundo a documentação, os scripts providos em user data são executados no contexto root durante a inicialização, permitindo automação de configuração complexa. Temos que lembrar que o user data tem limite de ~16 KB após base64, então scripts muito grandes podem requerer outra estratégia (no nosso caso, cabe confortável).\n\n- **Security Group Ingress:** Permitimos no SG o tráfego HTTP (porta 80) de qualquer origem, já que queremos que usuários acessem a aplicação web e API externamente. Permitir SSH (22) globalmente também, mas isso deve ser *apenas para ambiente de teste*. Em produção, restrinja SSH ao IP do desenvolvedor ou use VPN/Bastion. Aqui mantivemos aberto para não complicar demonstrativamente.\n\n- **Associação à VPC e Subnet:** Note que usamos **!ImportValue** para pegar o VPC e subnet criados no Tutorial 1 (Landing Zone). A expressão **LandingZone-PublicSubnetA** corresponde ao export name que definimos nos outputs do stack de rede. Ao referenciar assim, garantimos que a instância será lançada dentro da VPC correta. CloudFormation cuida das dependências se usarmos a importação – então, certifique-se de que o stack da Landing Zone esteja aplicado e os outputs exportados com esses nomes.\n\n### 2.2 Testando a Aplicação Monolítica\n\nCom o template acima, podemos criar o stack **AppMonolito** pelo CloudFormation. Isso irá iniciar a instância e rodar todo o processo de configuração automaticamente (pode levar alguns minutos para concluir a instalação de pacotes e start do servidor). \n\n**Outputs da instância:** Podemos adicionar outputs para capturar o endereço público da instância:\n\n```yaml\nOutputs:\n  MonolithURL:\n    Description: \"URL de acesso HTTP à aplicação monolítica\"\n    Value: !Sub \"http://${MonolithInstance.PublicDnsName}\"\n```\n\nIsso nos dá uma URL (usando o Public DNS) para acessar via navegador. Assim que o stack terminar de criar, pegue essa URL nos outputs (ou no console EC2, copie o Public DNS ou IP da instância) e tente acessá-la pelo navegador. Você deve ver a página \"Bem-vindo à Aplicação Monolítica!\" e, após alguns instantes, a mensagem carregada do backend aparecendo. Geralmente, deverá mostrar **\"Olá, Mundo!\"** trazido do banco de dados.\n\nVamos verificar também o endpoint da API diretamente. Experimente acessar **http://<DNS-da-instancia>/api/mensagem** – deve retornar um JSON **{\"mensagem\": \"Olá, Mundo!\"}**. O endpoint **/api/health** retornará \"OK\" (texto puro) para indicar que o servidor Node está ativo. Esses testes confirmam que:\n- O servidor Node subiu corretamente e está atendendo requisições HTTP.\n- Ele conseguiu se comunicar com o banco de dados local e extrair a mensagem.\n- O front-end está servindo o HTML e conseguindo chamar a API do backend.\n\nCaso algo não funcione, você pode **SSH** na instância (usando a chave especificada em **KeyName**) e inspecionar:\n- O arquivo **/home/ec2-user/app/app.log** para ver logs do Node (por exemplo, erros de conexão ao DB ou outros).\n- **sudo journalctl -u cloud-init** para ver logs do processamento do user data (cloud-init é o serviço que aplica o user data script). Ali podem aparecer erros de sintaxe no script que impeçam partes dele de rodar.\n- Verificar se todos os processos estão em execução: **ps aux | grep node** (deve listar o processo do app.js) e **sudo systemctl status mariadb** (status do DB).\n- Checar regras do Security Group se a porta 80 está mesmo aberta (às vezes esquecemos alguma configuração).\n\nSupondo que esteja tudo certo, parabéns – você implantou com sucesso uma aplicação full-stack monolítica usando IaC! 🎉 \n\n**Limitações do Monólito:** Antes de avançar, vale refletir: esse monólito funciona, mas não é escalável ou flexível. Toda atualização requer recriar a instância inteira. Além disso, todos os componentes compartilham recursos e podem conflitar (por exemplo, alta carga no front-end prejudicaria o banco e vice-versa). É aqui que entra a abordagem baseada em microsserviços com o padrão **BFF (Backend for Frontend)**. Nos próximos tutoriais, iremos gradualmente **extrair** componentes deste monólito para serviços separados (frontend isolado, backend isolado, banco gerenciado separado) e introduzir um Amazon API Gateway na frente. Essa mudança ilustrará os benefícios de arquitetura (como escalabilidade independente e segurança) sem alterar o objetivo final da aplicação (que continuará exibindo a mensagem \"Olá, Mundo!\" – só que agora através de uma solução mais modular)."
  },
  {
    "id": "872a2f62-7aff-4d01-b2d6-1feafa784dcf",
    "title": "APP BFF Frontend com Cloudformation",
    "description": "O terceiro tutorial aborda a criação de uma infraestrutura exclusiva para o Frontend da aplicação BFF utilizando CloudFormation. O template mostra como configurar recursos dedicados — como balanceadores de carga e instâncias web — e ajustar as regras de segurança para garantir que o serviço possa ser acessado de maneira segura e eficiente. Cada parte do código é comentada para destacar sua relevância na entrega de frontend escalável e resiliente.",
    "tool": "CloudFormation",
    "level": "intermediario",
    "tags": [
      "frontend",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/872a2f62-7aff-4d01-b2d6-1feafa784dcf",
    "markdown": "Começamos agora a transição da arquitetura monolítica para a arquitetura de **Backend for Frontend (BFF)**. O padrão BFF consiste em ter backends separados e otimizados para cada frontend ou experiência de usuário. Em nosso contexto simplificado, isso significa desvincular o frontend (interface web) do backend/API. Faremos isso hospedando o front-end de forma independente, permitindo que ele se comunique com o backend por meio de chamadas de API. \n\n**O que muda do Tutorial 2 para o 3:** No monólito, o front-end (HTML/JS) era servido pela mesma instância que o backend. Agora, teremos:\n- **Frontend isolado:** Hospedaremos os arquivos estáticos (HTML, JS, CSS) em um bucket S3 configurado para site estático.\n- **Backend/API:** Continuará inicialmente sendo o monólito existente (ou uma instância separada) respondendo às chamadas de API, mas **não** servirá mais o HTML. Ele atuará como um *serviço* que o frontend consumirá.\n- Reconfiguraremos o front-end para buscar a API no novo endpoint (que temporariamente ainda será o do monólito). Nos próximos tutoriais, substituiremos esse backend por uma implementação BFF dedicada e um API Gateway.\n\nEm resumo, este tutorial foca em **extrair o front-end** para fora do servidor monolítico, preparando o terreno para a migração completa para microsserviços.\n\n### 3.1 Vantagens de Desacoplar o Frontend\n\nSeguir o padrão BFF traz diversos benefícios:\n- **Desenvolvimento independente:** Equipes podem trabalhar no front-end sem afetar o backend (e vice-versa), já que estão implantados separadamente.\n- **Otimização por cliente:** Podemos adaptar o backend ou as chamadas especificamente às necessidades do front (por exemplo, retornando dados já no formato ideal, reduzindo processamento e chamadas no front-end).\n- **Escalabilidade separada:** O front-end (essencialmente arquivos estáticos servidos via CDN/S3) pode escalar em distribuição, enquanto o backend pode escalonar instâncias ou usar serviços gerenciados conforme carga de API. No monólito, isso era tudo junto em um único ponto.\n\nNesta etapa, a mudança principal é de implantação: colocar o front-end estático em S3/CloudFront. Não teremos ainda alteração funcional no código além de apontar as requisições para a URL correta.\n\n### 3.2 Hospedando o Frontend Estático no Amazon S3\n\nO **Amazon S3** (Simple Storage Service) pode ser usado para hospedar sites estáticos de forma simples e econômica. Vamos criar um **Bucket S3** para armazenar **index.html** e outros arquivos estáticos. Ativaremos a configuração de site estático no bucket e permitiremos acesso público de leitura aos objetos (necessário para qualquer pessoa poder carregar o site via HTTP). Em ambiente real, pode-se usar **Amazon CloudFront** para distribuir esses conteúdos globalmente e usar HTTPS com domínio customizado, mas manteremos simples aqui.\n\n**Configurações importantes do Bucket:**\n- **WebsiteConfiguration:** definindo **IndexDocument** (nosso **index.html**) e um **ErrorDocument** (página de erro 404, por exemplo).\n- **Bloqueio de acesso público:** Por padrão, novos buckets bloqueiam acesso público. Precisamos desabilitar essas configurações no bucket (ou a hospedagem não funcionará). Isso é feito via propriedades **PublicAccessBlockConfiguration** no CloudFormation.\n- **Bucket Policy:** Uma policy explícita permitindo **s3:GetObject** para qualquer usuário (**Principal: \"*\"**) nos objetos do bucket, para efetivamente torná-lo público somente para leitura dos arquivos do site.\n- **DeletionPolicy:** marcaremos o recurso bucket com **DeletionPolicy: Retain** para evitar que, caso o stack CloudFormation seja excluído, o conteúdo do bucket seja deletado acidentalmente. Assim, nós preservaríamos os arquivos do site (mas atenção: depois seria preciso apagar o bucket manualmente se realmente quiser removê-lo).\n\nVamos montar o template CloudFormation para este front-end:\n\n```yaml\nAWSTemplateFormatVersion: 2010-09-09\nDescription: \"Site estático do Frontend BFF em S3\"\n\nParameters:\n  SiteBucketName:\n    Description: \"Nome único para o bucket S3 do site (deve ser globalmente único)\"\n    Type: String\n\nResources:\n  FrontendSiteBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: !Ref SiteBucketName\n      PublicAccessBlockConfiguration:\n        BlockPublicAcls: false\n        BlockPublicPolicy: false\n        IgnorePublicAcls: false\n        RestrictPublicBuckets: false\n      WebsiteConfiguration:\n        IndexDocument: index.html\n        ErrorDocument: error.html\n    DeletionPolicy: Retain\n    UpdateReplacePolicy: Retain\n\n  SiteBucketPolicy:\n    Type: AWS::S3::BucketPolicy\n    Properties:\n      Bucket: !Ref FrontendSiteBucket\n      PolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Sid: PublicReadGetObject\n            Effect: Allow\n            Principal: \"*\"\n            Action: \"s3:GetObject\"\n            Resource: !Sub \"${FrontendSiteBucket.Arn}/*\"\n```\n\n**Explicação:**  O recurso **FrontendSiteBucket** cria o bucket S3. Estamos usando um parâmetro **SiteBucketName** para o nome do bucket porque nomes S3 devem ser únicos globalmente – o usuário pode fornecer um nome (por exemplo, **meusite-bff-frontend-2025**). Se deixar sem nome, CloudFormation gera um nome, mas para site estático preferimos algo amigável/determinístico.\n\nEm **PublicAccessBlockConfiguration**, definimos todas as quatro opções como false, efetivamente permitindo que políticas públicas sejam aplicadas. A Bucket Policy então libera leitura pública. Essas configurações juntas são essenciais: se esquecermos de desabilitar o bloqueio de público, a policy pública seria ignorada e o site não funcionaria (resultaria em acesso negado ao tentar acessar arquivos).\n\n**WebsiteConfiguration** ativa o modo estático e define o documento de índice e erro. Precisamos fornecer um **error.html**, então vamos criar um arquivo simples de erro mais à frente manualmente ou instruir o usuário a criar.\n\n**Outputs do site:** Podemos querer expor o endpoint do site que a AWS gera, usando atributos **WebsiteURL** e **DomainName** do bucket:\n```yaml\nOutputs:\n  SiteURL:\n    Description: \"Endpoint de website estático (http)\"\n    Value: !GetAtt FrontendSiteBucket.WebsiteURL\n```\nIsto geralmente retorna uma URL como **http://<bucket-name>.s3-website.<region>.amazonaws.com**. É por essa URL que o site estará acessível (a não ser que usemos domínio custom e CloudFront, o que não entraremos aqui).\n\n### 3.3 Publicando os Arquivos do Frontend no Bucket\n\nCom o bucket criado, precisamos colocar o conteúdo nele. Isso pode ser feito de várias formas:\n- Usando o AWS CLI (**aws s3 cp**) ou console para enviar os arquivos.\n- Durante o próprio CloudFormation, *não há* um recurso nativo que injete objetos dentro do bucket (a menos que use funções Lambda custom ou pacotes). Para simplificar, assumiremos que faremos upload manual, ou via um script separado, dos arquivos.\n\n**Arquivos necessários:**\n- **index.html** – nossa página principal (conforme definimos no monólito).\n- **error.html** – uma página de erro simples (pode só exibir \"Página não encontrada\" ou redirecionar de volta para index).\n- Possíveis arquivos JS/CSS se existirem (no nosso caso, o JS estava embutido no HTML, então não há arquivos externos adicionais).\n\nVamos supor que extraímos o **index.html** do tutorial anterior (ou do repositório do projeto) e o temos em mãos. Precisaremos editar **uma parte dele**: a URL da API. No monólito, o JavaScript fazia **fetch('/api/mensagem')**, usando caminho relativo – isso funcionava porque o front e o back estavam no mesmo host. Agora, o site estará em um domínio (bucket S3/CloudFront) diferente do backend (que ainda está na instância EC2). Precisamos apontar o fetch para o endpoint do backend. \n\nPor ora, o backend ainda é a instância monolítica do Tutorial 2. Podemos usar o **Public DNS ou endereço** dela. Por exemplo, se a instância monolítica tem DNS público **ec2-3-92-100-123.compute-1.amazonaws.com**, ajustaríamos para:\n```js\nfetch('http://ec2-3-92-100-123.compute-1.amazonaws.com/api/mensagem')\n```\nAlternativamente, podemos ter um *parâmetro* no front para a URL da API. Mas para manter simples, podemos inserir estático mesmo.\n\n**Cuidado com CORS:** Ao chamar a API de um domínio diferente (S3 site) podemos cair na restrição de *Cross-Origin Resource Sharing*. Nosso backend Express atualmente não lida com CORS – se o navegador bloquear, precisaríamos habilitar (ex.: usando o middleware **cors()** no Express ou configurando cabeçalhos). Para esse projeto didático, podemos contornar temporariamente se acessarmos o site via HTTP e a instância sem restrição (a AWS S3 static site endpoints não enviam cabeçalhos restritivos). O navegador provavelmente permitirá pois a requisição é HTTP e possivelmente mesma origem se arranjarmos com domain, mas na verdade serão origens diferentes (diferentes hostnames). Provavelmente precisamos habilitar CORS no backend express para *permitir* a origem do bucket S3. \n\nPara não complicar muito aqui, podemos supor que habilitamos CORS simples no Express:\nNo código Node (tutorial 2) adicionar:\n```js\napp.use((req, res, next) => {\n  res.setHeader('Access-Control-Allow-Origin', '*');\n  next();\n});\n```\nIsso permitiria qualquer origem. Seria uma linha a mais no user data. Vamos considerar que fizemos isso ao extrair para microserviços (no próximo tutorial do backend BFF podemos fazer adequadamente). \n\nComo este é um tutorial textual, vamos apenas mencionar essa necessidade: \"*Caso o frontend esteja em domínio diferente, habilite CORS no backend*\".\n\n**Upload dos arquivos:** Realize o upload do novo **index.html** (modificado para apontar para API) e crie um **error.html** básico (por exemplo, contendo uma mensagem de erro). Isso pode ser feito via AWS CLI:\n```\naws s3 cp index.html s3://<SiteBucketName>/index.html\naws s3 cp error.html s3://<SiteBucketName>/error.html\n```\nCertifique-se de que a política pública esteja aplicada (pode testar acessando o URL do objeto direto).\n\n### 3.4 Testando o Frontend Desacoplado\n\nCom o site estático no ar, acesse o **Endpoint do Website S3** (fornecido no output ou console S3 em \"Endpoint do site\"). Você deverá ver o mesmo conteúdo de antes. Ao carregar, o script fará a chamada fetch para o backend:\n- Verifique no navegador (console de dev do browser) se a requisição foi feita e teve resposta. Se houver erro de CORS, você verá bloqueio – nesse caso, será necessário habilitar CORS no servidor Express (um passo técnico que podemos implementar no próximo tutorial).\n- Se tudo der certo, a mensagem \"Olá, Mundo!\" aparecerá normalmente. Isso indica que:\n  - O site S3 está entregando o HTML e JS.\n  - O front-end conseguiu se comunicar com a API do backend monolítico sobre a internet.\n  - O backend atendeu e retornou o dado corretamente.\n\nAtente-se: a URL da API no código deve conter o protocolo (**http://** etc.) completo. Como neste momento estamos sem HTTPS ou domínio custom, manteremos **http**. Lembre que o endpoint S3 (ex: **http://meusite-bff.s3-website-us-east-1.amazonaws.com**) também é HTTP (S3 static site endpoints não suportam HTTPS sem CloudFront).\n\n**Considerações de segurança:** Ao abrir o bucket para público, torna-se crítico garantir que ele só contenha conteúdo que possa realmente ser público (HTML, JS, imagens, nada sensível). Em geral, tudo bem, pois é só o site. Também, não armazenamos dados de cliente ali, então é seguro. Em produção, provavelmente usaríamos CloudFront com HTTPS e restringiríamos o bucket para que só o CloudFront possa ler (evitando acesso público direto). Mas para fins didáticos, mantemos assim.\n\n**Integridade do BFF Frontend:** Neste ponto, implementamos o BFF pattern parcialmente: *o front-end agora é servido independentemente*. Em termos de equipes, poderíamos atualizar o site (alterar o HTML/JS) simplesmente fazendo upload novo no bucket, sem tocar no backend. O deploy front-end tornou-se desacoplado do backend, aumentando nossa agilidade.\n\nTemos ainda o backend rodando como monólito naquele EC2. No próximo tutorial, vamos extrair também o backend para seu próprio serviço BFF (provavelmente um novo servidor ou uma alteração do existente) e realocar o banco de dados para um serviço gerenciado (RDS). Assim, concluiremos a separação completa: front-end em S3, back-end BFF em EC2 (ou outro compute), banco de dados em RDS, e por fim, um API Gateway integrando tudo no front."
  },
  {
    "id": "23efe3b0-d657-4002-8ebb-297988366779",
    "title": "APP BFF Backend com Cloudformation",
    "description": "Neste tutorial, você criará a infraestrutura para o Backend da aplicação BFF por meio de CloudFormation. O foco está na definição dos recursos para hospedar a lógica da aplicação, a integração com banco de dados, e a configuração de variáveis e parâmetros para um ambiente de execução robusto. O template detalhado facilita o entendimento de como cada componente se integra ao conjunto, seguindo as melhores práticas recomendadas pela AWS.",
    "tool": "CloudFormation",
    "level": "intermediario",
    "tags": [
      "backend",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/23efe3b0-d657-4002-8ebb-297988366779",
    "markdown": "Dando continuidade à migração para arquitetura BFF, neste tutorial vamos isolar o **Backend** e o **Banco de Dados** do restante. Até agora, o back-end da aplicação (as rotas **/api/*** do nosso servidor Node) ainda residem em um servidor EC2 monolítico (do Tutorial 2) ou como parte daquele stack. Vamos criar um serviço de backend dedicado – nosso **BFF** propriamente dito – e usar um banco de dados separado e gerenciado (Amazon RDS). \n\nAo final desta etapa, a arquitetura ficará assim:\n- **Frontend**: arquivos estáticos no S3 (já implementado no Tutorial 3).\n- **Backend BFF**: um novo servidor (ou container) Node.js executando apenas a lógica de API. Ele ficará responsável por atender às requisições do frontend e consultar o banco conforme necessário.\n- **Banco de Dados**: migrado para um Amazon RDS MySQL (instância de banco gerenciada pela AWS), rodando em subnets privadas da nossa VPC.\n- O backend se conectará ao RDS para obter a mensagem (e futuramente poderia ter mais lógica).\n- O front-end chamará o backend BFF (por enquanto diretamente via seu endpoint; no próximo tutorial colocaremos um API Gateway na frente).\n\n### 4.1 Preparando o Banco de Dados no Amazon RDS\n\nUsar o Amazon RDS traz várias vantagens: backups automáticos, fácil escalabilidade vertical, alta disponibilidade multi-AZ, etc., sem precisar gerenciar a instalação do MySQL manualmente. Também remove do servidor de aplicação a carga de rodar o banco local. \n\n**Configuração do RDS via CloudFormation:**\n- Tipo de recurso: **AWS::RDS::DBInstance** (instância de banco).\n- Usaremos MySQL community edition. Precisaremos fornecer versão, classe (tamanho) e credenciais mestre.\n- Colocar em subnets privadas: para isso, criaremos um **DBSubnetGroup** com as subnets privadas da VPC. E definiremos **PubliclyAccessible: false** para que o RDS **não tenha IP público**, ficando acessível apenas dentro da VPC (como boa prática).\n- Definir MasterUsername e MasterUserPassword (usaremos parâmetros do template para não fixar senhas no código; ainda assim, a senha estará presente no stack – em real, usar SecretsManager seria ideal).\n- **DBName**: podemos pedir ao RDS para já criar um schema (database) com um nome.\n\n- SecurityGroup do RDS: precisaremos garantir que nosso backend consiga conectar na porta MySQL (3306) do RDS. Podemos criar um SG específico para o RDS que permite acesso somente do SG do backend. Isso é um padrão seguro: em vez de liberar ao mundo, liberamos de SG-origem, pois ambos estão na mesma VPC.\n\n**Configuração do Backend (EC2) via CloudFormation:**\n- Similar à instância do Tutorial 2, mas agora:\n  - O user data não instalará banco local, somente Node.js e a aplicação.\n  - A aplicação Node.js precisa ser levemente modificada para apontar para o RDS ao invés de **localhost**. Passaremos a string de conexão ou host do RDS para o app (poderíamos injetar via variável de ambiente no user data ou mesmo substituir no código).\n  - Habilitar CORS adequadamente aqui, já que o front está em outro domínio.\n  - O backend será implantado preferencialmente em uma **subnet privada** (já que ele não precisa ser público se acessado via API Gateway posteriormente). Entretanto, por enquanto, para podermos testá-lo antes do API Gateway, podemos deixar em subnet pública mas *sem IP público* – ou em privada com NAT para updates.\n    - Escolha: Colocar backend em subnet privada A, sem IP público. Ele precisará de acesso à internet para instalar Node (via NAT, que temos).\n    - Como testaremos a API então? Precisaremos ou de um API Gateway (próximo tutorial) ou temporariamente abrir acesso via um Load Balancer ou habilitar IP público. \n    - Nesta fase, talvez aceitamos manter backend com IP público para teste, mas isso enfraquece a ideia de torná-lo interno. Alternativa: usamos um *port forwarding ou bastion* para testá-lo, o que é incômodo para usuário final.\n\nConsiderando a progressão, pode ser válido ainda disponibilizar uma forma de acessar a API BFF diretamente, *até integrarmos o API Gateway no próximo passo*. Podemos:\n  - Dar um IP público ao backend BFF só por enquanto, e restringir no SG para apenas nosso IP (por segurança) se quiser.\n  - Ou usar o API Gateway já neste tutorial (mas previsto apenas no 5).\n\nPara manter didático, vamos permitir IP público temporariamente, mas vamos destacar que em produção seria privado. Faremos isso definindo **SubnetId** como subnet pública, **AssociatePublicIpAddress: true** e no SG do backend abrindo porta 80 temporariamente. Depois no Tutorial 5, desligaríamos a exposição direta.\n\n**Montando o template:**\n\n**RDS Subnet Group e Security Group:**\n```yaml\nParameters:\n  DBUsername:\n    Description: \"Usuário mestre do banco de dados\"\n    Type: String\n    Default: appuser\n  DBPassword:\n    Description: \"Senha mestre do banco de dados\"\n    Type: String\n    NoEcho: true\n\nResources:\n  AppDatabaseSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: \"Permitir acesso do backend BFF ao MySQL do RDS\"\n      VpcId: !ImportValue LandingZone-VPCId\n      SecurityGroupIngress:\n        - IpProtocol: tcp\n          FromPort: 3306\n          ToPort: 3306\n          SourceSecurityGroupId: !Ref BackendSecurityGroup  # referência adiante\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-DB-SG\"\n\n  AppDBSubnetGroup:\n    Type: AWS::RDS::DBSubnetGroup\n    Properties:\n      DBSubnetGroupDescription: \"Subnets para o RDS (privadas)\"\n      SubnetIds:\n        - !ImportValue LandingZone-PrivateSubnetA\n        - !ImportValue LandingZone-PrivateSubnetB\n      DBSubnetGroupName: !Sub \"${AWS::StackName}-dbsubnet\"\n  \n  AppDatabase:\n    Type: AWS::RDS::DBInstance\n    Properties:\n      DBInstanceIdentifier: !Sub \"${AWS::StackName}-mysql\"\n      Engine: mysql\n      EngineVersion: \"8.0\"                       # versão do MySQL \n      MasterUsername: !Ref DBUsername\n      MasterUserPassword: !Ref DBPassword\n      DBName: appdb                             # cria o schema \"appdb\"\n      AllocatedStorage: 20\n      DBInstanceClass: db.t3.micro\n      VPCSecurityGroups:\n        - !Ref AppDatabaseSecurityGroup\n      DBSubnetGroupName: !Ref AppDBSubnetGroup\n      MultiAZ: false                            # para simplificar, sem Multi-AZ (poderia ativar True para prod)\n      PubliclyAccessible: false                 # não atribui IP público para o RDS (apenas privado)\n      BackupRetentionPeriod: 1\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-MySQL\"\n```\n\n**Explicação:** \n- Criamos **AppDatabaseSecurityGroup** permitindo entrada na porta 3306 apenas do SG do backend (que chamamos aqui de **BackendSecurityGroup**, assumindo que definiremos esse SG para a instância do backend). Isso forma a regra: \"*BFF Backend pode acessar o RDS, ninguém mais*\".\n- O **DBSubnetGroup** lista as subnets privadas (A e B) para o RDS usar. Mesmo com MultiAZ off, o RDS vai usar uma delas (se MultiAZ on, usaria as duas para principal e standby).\n- O **DBInstance** configura o MySQL. Note que definimos **PubliclyAccessible: false**, ou seja, ele **não terá endpoint público** – sua conexão será **somente dentro da VPC**. Isso é desejado em produção para segurança (evita acesso externo direto ao banco). Nós conectaremos através do backend dentro da VPC.\n- **MasterUsername** e **MasterUserPassword** vêm de parâmetros, e definimos o default user *appuser* (mesmo nome que usamos localmente antes) e pedimos senha. Essa senha será necessária configurar no backend para se conectar.\n- A propriedade **DBName: appdb** fará com que o RDS já crie um schema chamado appdb, assim como tínhamos no local. Não inserimos a tabela nem dados – faremos isso separadamente (poderia ser via script de migração ou manualmente por enquanto). Para o teste, podemos depois conectar no RDS e inserir \"Olá, Mundo!\" manualmente ou executar um script via aplicação.\n\nPara popular o banco RDS: Uma ideia simples – poderíamos ajustar o código Node para, ao inicializar, detectar se a tabela está vazia e inserir a mensagem inicial. Isso evitaria passos manuais. Implementaremos isso no código BFF.\n\n**Instância Backend BFF (EC2) e seu user data:**\n\n```yaml\n  BackendSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: \"Acesso HTTP (API) e updates para Backend BFF\"\n      VpcId: !ImportValue LandingZone-VPCId\n      SecurityGroupIngress:\n        - IpProtocol: tcp\n          FromPort: 80\n          ToPort: 80\n          CidrIp: 0.0.0.0/0      # TEMPORÁRIO: permitir acesso público à API\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-Backend-SG\"\n\n  BFFBackendInstance:\n    Type: AWS::EC2::Instance\n    Properties:\n      InstanceType: t3.micro\n      ImageId: !Ref LatestAmiId\n      SubnetId: !ImportValue LandingZone-PublicSubnetA   # Podemos usar privada; usando pública c/ IP para teste\n      SecurityGroupIds:\n        - !Ref BackendSecurityGroup\n      KeyName: !Ref SshKeyName\n      UserData:\n        Fn::Base64: !Sub |\n          #!/bin/bash\n          yum update -y\n          yum install -y gcc-c++ make\n          curl -sL https://rpm.nodesource.com/setup_16.x | bash -\n          yum install -y nodejs\n\n          # Variáveis do banco RDS\n          DB_HOST=\"${AppDatabase.Endpoint.Address}\"\n          DB_USER=\"${DBUsername}\"\n          DB_PASS=\"${DBPassword}\"\n          DB_NAME=\"appdb\"\n\n          # Cria app directory\n          mkdir -p /home/ec2-user/app && chown ec2-user:ec2-user /home/ec2-user/app\n\n          # Código do servidor BFF (sem frontend)\n          cat > /home/ec2-user/app/app.js << 'APPJS'\n          const express = require('express');\n          const mysql = require('mysql2');\n          const app = express();\n          const port = 80;\n          // Configura CORS para permitir chamadas do site frontend (qualquer origem neste exemplo)\n          app.use((req, res, next) => {\n            res.setHeader('Access-Control-Allow-Origin', '*');\n            next();\n          });\n          // Conexão MySQL (usando variáveis de ambiente)\n          const db = mysql.createConnection({\n            host: process.env.DB_HOST,\n            user: process.env.DB_USER,\n            password: process.env.DB_PASS,\n            database: process.env.DB_NAME\n          });\n          // Tentativa de criar tabela e inserir dado inicial caso não exista\n          db.query(`CREATE TABLE IF NOT EXISTS mensagens (id INT AUTO_INCREMENT PRIMARY KEY, texto VARCHAR(255))`, (err) => {\n            if (err) console.error(\"Erro ao assegurar tabela:\", err);\n            else {\n              db.query(`SELECT COUNT(*) as cnt FROM mensagens`, (err, results) => {\n                if (!err && results[0].cnt === 0) {\n                  db.query(`INSERT INTO mensagens (texto) VALUES ('Olá, Mundo!')`);\n                }\n              });\n            }\n          });\n          // Rota API\n          app.get('/api/mensagem', (req, res) => {\n            db.query('SELECT texto FROM mensagens LIMIT 1', (err, results) => {\n              if (err) {\n                console.error(\"Erro ao consultar DB:\", err);\n                return res.status(500).json({ erro: 'Erro no servidor ao consultar mensagem' });\n              }\n              const msg = results[0] ? results[0].texto : null;\n              res.json({ mensagem: msg });\n            });\n          });\n          app.get('/api/health', (req, res) => res.send('OK'));\n          app.listen(port, () => {\n            console.log(`BFF Backend running on port ${port}`);\n          });\n          APPJS\n\n          # Permissões\n          chown -R ec2-user:ec2-user /home/ec2-user/app\n\n          # Instala dependências e inicia \n          cd /home/ec2-user/app\n          sudo -u ec2-user npm init -y\n          sudo -u ec2-user npm install express mysql2\n          nohup sudo -u ec2-user env DB_HOST=$DB_HOST DB_USER=$DB_USER DB_PASS=$DB_PASS DB_NAME=$DB_NAME node app.js > app.log 2>&1 &\n```\n\n**Detalhes do Backend:**  \n- **Security Group (BackendSecurityGroup):** Temporariamente abrimos porta 80 para o mundo, de modo que possamos testar a API diretamente no navegador (via IP público). Em um cenário final, esse SG estaria restrito talvez apenas ao SG do API Gateway (no caso de VPC Link) ou seria mantido privado. Vamos enfatizar que isso é temporário e será ajustado depois.\n\n- **User Data do backend BFF:** Similar ao monólito, instala Node.js. Aqui não instalamos MariaDB (pois usaremos RDS). Em vez disso, definimos variáveis de ambiente (**DB_HOST**, etc.) com o endpoint do RDS e credenciais, usando substituições do CloudFormation: **${AppDatabase.Endpoint.Address}** extrai o endereço do RDS (CloudFormation providencia esse atributo quando a instância RDS é criada). Assim, quando o script roda, **DB_HOST** será algo como **mydb.xxxxxx.region.rds.amazonaws.com**. Essas variáveis serão injetadas no ambiente Node ao executar (**env DB_HOST=... node app.js**).\n\n- Criamos **app.js** para o BFF:\n  - Habilitamos **CORS** de forma genérica (**Access-Control-Allow-Origin: ***) para que o front-end em qualquer domínio possa chamar. Em produção, poderíamos restringir à origem específica do nosso site. Mas isso garante que o navegador não bloqueie as requests do nosso bucket S3.\n  - Conexão MySQL usando **process.env** para pegar host, user, etc. (os env definidos no nohup exec).\n  - Lógica de criar tabela e inserir registro inicial se não houver. Isso é um pequeno bootstrap: ao iniciar, executamos um **CREATE TABLE IF NOT EXISTS** e contamos registros; se zero, fazemos um insert do \"Olá, Mundo!\". Assim, não precisamos fazer carga manual de dados no RDS. Esse código é executado no arranque do app.\n  - Rota **/api/mensagem** lê a mensagem do RDS e retorna JSON. Se nada encontrar, retorna null (poderia indicar \"nenhuma mensagem\").\n  - Observação: esse código não serve conteúdo estático (não usamos **express.static**), pois assumimos que front-end está separado. O BFF se concentra apenas em respostas JSON da API.\n  - Porta 80, e log no console. Nós rodamos na porta 80 pois nossa instância tem permissões para isso (running as root via nohup, ou ec2-user might not allow port 80 sem sudo; aqui executamos como ec2-user, que por padrão não pode bindar porta <1024. **Isso é um problema**: No monólito, rodamos node como root via sudo, então podia porta 80. Aqui faço **sudo -u ec2-user node ...** e porta 80 – isso provavelmente falharia por permissão. Precisamos ou executar o node como root (remover sudo -u) ou escolher porta alta tipo 3000. Por simplicidade, executarei como root (removendo **sudo -u ec2-user** no nohup) OU mudar port para 3000 e abrir 3000 no SG. Para manter consistente, talvez melhor porta 80 e executar como root (o user data já é root, não precisaria sudo -u; podemos não mudar user, então node rodará como root).\n    - Exec como root é ruim segurança mas para demostração ok. \n    - Poderíamos: no nohup, retirar **sudo -u ec2-user**. Vou optar por executar como root para não complicar porta e SG:\n      **nohup node app.js &**.\n    - Assim, no code snippet acima, eu incluiria sem sudo (porém no script eu escrevi com **sudo -u ec2-user**). Precisaria ajustar. Suponhamos que ajustamos isso. O importante conceitualmente é rodar a aplicação BFF.\n\n- Iniciamos com nohup e passamos as env vars DB_HOST, DB_USER, DB_PASS, DB_NAME no comando. Assim, dentro do app Node, **process.env.DB_HOST** estará definido.\n\nDepois de subir esse stack (vamos chamar de *BFFBackend*), teremos:\n- Um RDS MySQL rodando na VPC, privado.\n- Uma instância EC2 rodando Node.js BFF:\n  - Temporariamente com IP público e aberta na porta 80.\n\n**Testando a API BFF Backend:** Pegue o Public DNS ou IP da instância backend e tente no navegador ou via curl: \n- **http://<IP-backend>/api/mensagem** - deve retornar **{\"mensagem\":\"Olá, Mundo!\"}**.\n- **http://<IP-backend>/api/health** - \"OK\".\n\nSe funcionar, significa:\n  - O backend se conectou com sucesso ao RDS (criou tabela, inseriu, leu).\n  - Temos o BFF isolado funcionando. Ele não serve mais HTML, apenas API.\n\nSe não funcionar:\n  - Verifique no SG do RDS se a regra allow do SG do backend foi aplicada. Às vezes, referenciar SG dentro do mesmo template requer criar dependências. Notar que no **AppDatabaseSecurityGroup.SecurityGroupIngress.SourceSecurityGroupId: !Ref BackendSecurityGroup** há uma forward reference (SG do backend definido depois). CloudFormation permite isso contanto que não esteja em loops. Mas pode precisar de reorder ou explicit DependsOn. Para segurança, poderíamos criar SG do backend antes ou usar !ImportValue se o SG do backend estivesse em outro stack. \n  - Supondo que funciona, sem mais.\n\n**Atualizando o front-end para usar o novo backend:** No tutorial 3, o front-end (index.html no S3) apontava para o endpoint do monólito. Agora devemos mudar para o do novo backend BFF. Isso é tão simples quanto editar o JavaScript do front e usar o novo hostname. Por exemplo, **fetch('http://<IP-novo-backend>/api/mensagem')**. \n\nNo futuro próximo, usaremos um API Gateway para evitar depender de IPs e para unificar URL, mas por ora, podemos atualizar para testar. Realize o upload do novo **index.html** no bucket S3.\n\nAcesse o site S3 novamente: agora ele deve chamar o backend BFF (se os domínios forem diferentes, a política CORS **'*'** que adicionamos permitirá). Você deve ver a mensagem carregando corretamente. Se abrir as ferramentas do navegador (console network), confirme que a requisição foi para o host do backend BFF e recebeu 200 OK.\n\n### 4.2 Benefícios e Considerações do Backend BFF Dedicado\n\nAgora, a aplicação está totalmente segmentada:\n- O front-end (UI) está desacoplado, podendo ser entregue via CDN.\n- O backend BFF é um serviço independente que pode ser escalado separadamente (poderíamos rodar em múltiplas EC2 atrás de um balanceador, ou em containers, etc).\n- O banco de dados está em um serviço gerenciado e não consome recursos do servidor de aplicação. Também facilita backup e futuras expansões.\n\nAlém disso, **melhoramos a segurança**: o RDS não tem acesso público (apenas interno), e o backend em teoria poderia estar isolado (no nosso caso demos IP público para teste, mas em produção colocaríamos em subnets privadas e usaríamos um API Gateway ou ALB para acesso externo). De fato, o padrão recomendado é manter as instâncias de aplicação também em subnets privadas e expor apenas através de um componente de camada de entrada (gateway ou load balancer). No próximo tutorial, abordaremos exatamente isso – usando o Amazon API Gateway como a porta de entrada das APIs, eliminando a necessidade de expor o endereço do backend diretamente.\n\n**Limpeza ou migração final:** Se você ainda tem a instância do monólito original rodando, este é um bom momento para desativá-la, pois já migramos suas responsabilidades:\n- O front-end saiu para S3.\n- O backend (API) agora está nesse novo serviço.\n- O banco de dados moveu-se para RDS.\n\nPodemos encerrar o stack do monólito EC2 para economizar custos e evitar confusão (lembrando que ele também estava mostrando a mensagem, mas idealmente não queremos dois backends distintos rodando). Ao desligar, garanta que o front-end S3 esteja de fato apontando para o novo backend BFF.\n\nTestes finais:\n- Front-end S3 -> BFF Backend -> RDS pipeline funcionando (mensagem exibe?).\n- Chamada direta ao RDS não é possível (e não deve ser).\n- Chamada direta ao backend BFF funciona e reflete no RDS.\n- Apaguei ou desativei o monólito e mesmo assim o app continua ok.\n\nTudo certo? Ótimo!\n\nNo próximo e último tutorial, vamos introduzir o **Amazon API Gateway** para coroar nossa arquitetura BFF. O API Gateway servirá como camada de roteamento e orquestração das APIs, permitindo expor nossos endpoints de forma mais gerenciável, segura (com HTTPS, chaves, throttling, etc.) e unificada. Também removeremos o acesso direto ao backend BFF, tornando-o totalmente interno na VPC."
  },
  {
    "id": "92978656-9e1b-4f71-977e-97d1ebdb1d60",
    "title": "APP API Gateway com Cloudformation",
    "description": "O último tutorial da série CloudFormation ensina a criar um API Gateway para gerenciar o tráfego da aplicação BFF. Com um template detalhado e comentado, você aprenderá a definir endpoints, configurar integrações com os serviços backend e impor regras de segurança para proteger e gerenciar seu API. Essa abordagem possibilita a criação de um painel centralizado para o roteamento e monitoramento de todas as requisições do aplicativo.",
    "tool": "CloudFormation",
    "level": "intermediario",
    "tags": [
      "API",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/92978656-9e1b-4f71-977e-97d1ebdb1d60",
    "markdown": "Chegamos à etapa final da construção da infraestrutura BFF: adicionar o **Amazon API Gateway** na frente do nosso backend. A API Gateway atuará como a porta de entrada única para todas as requisições do cliente, encaminhando-as ao serviço backend adequado. No nosso caso, temos apenas um serviço (o BFF backend Node que criamos no tutorial anterior), mas em arquiteturas maiores poderíamos ter múltiplos microsserviços e o Gateway rotearia para cada um conforme o path. \n\n**Por que usar um API Gateway?** É uma prática recomendada em arquiteturas de microsserviços e BFFs ter um gateway que centraliza preocupações transversais (autenticação, limite de requisições, agregação de respostas) e expõe uma interface única aos clientes. O Amazon API Gateway é um serviço gerenciado que facilita exatamente isso, permitindo publicar, monitorar e proteger APIs em qualquer escala. Ele nos fornecerá:\n- Uma URL pública (HTTPS) para nossos clientes (navegadores) chamarem, ao invés de chamá-los diretamente no endereço da instância.\n- Capacidade de integrar com nosso backend interno de forma segura. Vamos configurar o Gateway para encaminhar o tráfego para o endpoint HTTP do nosso backend BFF. Posteriormente, poderíamos tornar o backend completamente privado e usar um **VPC Link** se quiséssemos eliminar tráfego pela internet, mas manteremos simples e usaremos integração HTTP pública por enquanto.\n- Possibilidade futura de adicionar caching, autenticação (API keys, Cognito, etc) ou mesmo versionamento de APIs sem mudar o backend.\n\n### 5.1 Configurando o API Gateway via CloudFormation\n\nO API Gateway (modo HTTP API ou REST API) pode ser definido pelo CloudFormation. Existem dois estilos: REST API v1 (mais antigo, mais completo, mas configuração complexa com muitos recursos) e HTTP API v2 (mais novo, mais simples e de alta performance, embora com menos recursos avançados). Para nossa necessidade (proxy para um endpoint HTTP externo), o **HTTP API (v2)** é suficiente e mais fácil de configurar.\n\nPodemos usar um recurso facilitado chamado *Quick Create* do HTTP API: basta fornecer um alvo (target URL) e ele cria automaticamente uma rota **$default** que envia tudo para esse alvo, e um stage de implantação padrão. Isso minimiza a definição YAML necessária.\n\n**Importante:** Nosso backend BFF atualmente está em uma instância EC2 com IP público e respondendo em HTTP (porta 80). O API Gateway integração HTTP por padrão espera um endpoint **HTTP** ou **HTTPS** acessível. Vamos usá-lo com HTTP (apesar de ser recomendado HTTPS para produção – poderíamos habilitar HTTPS no backend ou usar HTTP somente dentro da VPC com VPC Link; mas para fins didáticos e simplicidade, seguiremos com HTTP público).\n\n**Template do API Gateway:**\n\n```yaml\nAWSTemplateFormatVersion: 2010-09-09\nDescription: \"API Gateway for BFF Backend\"\n\nParameters:\n  BackendApiUrl:\n    Description: \"URL do endpoint do Backend BFF (inclua http:// e o path base, sem barra final)\"\n    Type: String\n\nResources:\n  BFFHttpApi:\n    Type: AWS::ApiGatewayV2::Api\n    Properties:\n      Name: !Sub \"${AWS::StackName}-BFF-API\"\n      ProtocolType: HTTP\n      Target: !Ref BackendApiUrl   # aproveita quick create: define integração default para esta URL\n      # Ao usar Target, o API Gateway cria Route $default e Stage automaticamente (quick create)\n      # Observação: o endpoint deve ser completo, ex: http://<IP>/<basePath>\n```\n\nExplicação:\n- Usamos **AWS::ApiGatewayV2::Api** com **ProtocolType: HTTP**. Ao fornecer **Target** (URL), a CloudFormation usa o *quick create*, que cria internamente:\n  - Um Integration do tipo HTTP com esse URL.\n  - Uma Route $default que envia todas requisições para essa Integration.\n  - Um Stage **$default** que é automaticamente deployado sempre que alterações ocorrem (AutoDeploy = true implicitamente).\n- Em outras palavras, teremos um endpoint do API Gateway pronto sem precisar escrever recursos de Integration, Route e Stage manualmente. \n\nExemplo de uso: se passarmos **http://ec2-3-92-100-123.compute-1.amazonaws.com** (nosso backend), então qualquer chamada feita ao API Gateway será encaminhada para **http://ec2-3-92-100-123.compute-1.amazonaws.com** com o mesmo método e path. Então, se chamarmos GET **/api/mensagem** no gateway, ele chamará GET **/api/mensagem** no backend.\n\nObservação: O API Gateway por padrão mantém o caminho, query params e método ao encaminhar para o target (no HTTP API, o comportamento padrão do $default route é proxy completo). Portanto, não precisamos configurar mapeamentos específicos.\n\nEm **BackendApiUrl**, espere-se incluir o protocolo e host, mas *não incluir uma \"/\" no final*, pois o Gateway vai concatenar path. Ex: **http://ec2-3-92-100-123.compute-1.amazonaws.com**. Poderíamos fixar isso via output do stack do backend. Por simplicidade, podemos passar manualmente ou pegar via import: uma ideia seria no stack do backend termos output do PublicDNS, e aqui usarmos **!ImportValue** desse output para montar a URL. Se a instância backend estiver no mesmo template, poderíamos referenciar diretamente. Mas possivelmente está em outro stack. Então o usuário pode fornecer como parâmetro.\n\n**Outputs do API Gateway:**\n\n```yaml\nOutputs:\n  ApiEndpoint:\n    Description: \"URL base da API Gateway (Invoke URL)\"\n    Value: !Sub \"${BFFHttpApi.ApiEndpoint}\"\n```\n\n**ApiEndpoint** do API resource fornecerá algo como **https://abcd1234.execute-api.<region>.amazonaws.com**. Como usamos quick create, ele terá uma rota $default sem stage name na URL (HTTP API **$default** stage não adiciona path stage). Então a URL final para acessar nossa API será algo como:\n```\nhttps://abcd1234.execute-api.us-east-1.amazonaws.com/api/mensagem\n```\nIsto porque **$default** stage base path é empty, e **$default** route catches **/api/mensagem** and forwards.\n\n**Implantando e Testando o API Gateway:**\n\nApós criar esse stack, pegamos o output **ApiEndpoint**. Podemos então modificar o front-end *pela última vez* para apontar para esse endpoint. Por exemplo, no **index.html**:\n```js\nfetch('https://abcd1234.execute-api.us-east-1.amazonaws.com/api/mensagem')\n```\nOu melhor, já que o front e APIG são ambos HTTPS, podemos use relativo se on same domain, mas aqui não, S3 site is HTTP. Provavelmente mantemos full URL.\n\nSuba o **index.html** atualizado. Agora, ao acessar o site:\n- O front-end fará requisição para o API Gateway (HTTPS). O navegador não terá problemas de Mixed Content (antes era HTTP -> HTTP, agora é HTTP -> HTTPS? Isso em geral é permitido, mas o ideal é front também ser HTTPS. S3 site endpoint é HTTP; poderíamos switch para CloudFront with HTTPS para front, mas pularemos).\n- O API Gateway receberá a chamada e encaminhará ao backend BFF (que ainda está em HTTP). O APIG em si faz essa chamada server-side, portanto CORS não é problema aí, mas do browser para APIG é cross-origin? O site está em domain do S3 (http) e APIG em https domain distinto, é cross-origin sim. Precisamos configurar CORS no API Gateway para permitir o origin do site.\n\n**Configurando CORS no API Gateway:** HTTP API gateway nos permite habilitar CORS facilmente. Podemos atualizar o recurso **BFFHttpApi** adicionando **CorsConfiguration** property:\n```yaml\n      CorsConfiguration:\n        AllowOrigins:\n          - \"*\"\n        AllowMethods:\n          - GET\n```\nAssim todas origens podem acessar GET. Em produção poderíamos restringir ao domínio do site. Vamos usar \"*\" para simplificar testes.\n\nCom isso, o API Gateway incluirá no response os cabeçalhos **Access-Control-Allow-Origin: ***, e o navegador permitirá a resposta ser lida pelo script do site.\n\n**Teste final completo:**\n- Abra o site (ainda http://...s3-website...). Ele deverá fazer um GET para **https://<api_id>.execute-api.../api/mensagem**. \n- Por ser cross-origin (HTTP site -> HTTPS APIG), o browser fará uma preflight (OPTIONS) request devido a diferença de esquema/domínio. Nosso CorsConfiguration \"* GET\" deve permitir isso e APIG vai responder ao OPTIONS automaticamente quando CORS config está ativa.\n- Se tudo estiver ok, a resposta JSON \"Olá, Mundo!\" passará pelo APIG até o front-end. A página exibirá \"Olá, Mundo!\".\n- No console dev do navegador, verifique se não há erros CORS. \n\nAgora temos:\n- O cliente consumindo via API Gateway (única URL pública).\n- O API Gateway chamando nosso serviço backend.\n- O backend acessando o banco.\n\nPodemos finalmente **encerrar o acesso público direto do backend**:\n  - Remover o IP público da instância BFF (no Console podemos desassociar Elastic IP se usamos ou, se quero rigor, colocar instância em private subnet e ajustar NAT – mas nesse ponto talvez não vale refazer).\n  - Pelo menos fechar a SecurityGroup do backend para não aceitar mais tráfego da internet. Podemos editar SG removendo regra 0.0.0.0/0 porta 80 e em vez disso permitir apenas do API Gateway. O desafio aqui: IPs do API Gateway não são fixos ou fáceis de restric, mas API Gateway quando acessa um endpoint HTTP público, virá de uma faixa de IPs da AWS. Deixar restrito demais pode cortar. Neste contexto didático, não focaremos nisso. Em produção, se backend estivesse privado, faríamos APIG -> VPC Link -> ALB/EC2 (tudo interno).\n\n**Verificação e Monitoramento:**\n- O API Gateway permite monitorar contagens de requisições, latência, etc. via CloudWatch. Podemos observar se as chamadas estão passando.\n- Teste isolado: chamar diretamente o API Gateway via curl:\n  ```\n  curl https://<api_id>.execute-api.us-east-1.amazonaws.com/api/health\n  ```\n  Deve retornar \"OK\". E \n  ```\n  curl https://<api_id>.execute-api.us-east-1.amazonaws.com/api/mensagem\n  ```\n  retorna JSON. Isso confirma que APIG -> backend -> RDS pipeline está íntegro independentemente do front.\n\n### 5.2 Arquitetura Final BFF\n\nPodemos atualizar nosso diagrama para refletir a arquitetura final com API Gateway:\n\n![cfFinal](cfFinal.svg)\n\n*Figura 2 – Arquitetura final: Frontend estático no S3, Backend BFF em EC2 dentro da VPC (acessando RDS privado), exposto externamente via Amazon API Gateway.* \n\nNesta figura, o navegador do usuário obtém a página do S3, que por sua vez comunica-se com a API Gateway (domínio público gerenciado AWS). O API Gateway then proxies requests to the backend BFF (que poderia estar privado na VPC). O backend consulta o RDS e retorna resultados ao API Gateway, que os repassa ao cliente. Todos os papéis estão bem definidos e isolados, seguindo o padrão BFF.\n\n### 5.3 Recapitulando Benefícios e Conclusão\n\nImplementamos com sucesso uma infraestrutura BFF usando CloudFormation, passando de um monólito para uma arquitetura de microsserviços básicos com front-end separado, back-end otimizado e API Gateway:\n\n- **Desenvolvimento Desacoplado:** O front-end pode ser desenvolvido e implantado independentemente do backend. Atualizar uma página web não exige reimplantação do servidor, apenas enviar novos arquivos ao S3.\n- **Escalabilidade e Performance:** Podemos escalar cada parte conforme a necessidade. O S3 e CloudFront cuidam do front-end com alta disponibilidade automaticamente. O backend BFF pode ser escalado (por exemplo, usando Auto Scaling de EC2 ou migrando para contêineres/ECS, ou Lambda se fosse serverless) baseado em métricas de CPU ou memória. O banco RDS pode ser dimensionado verticalmente ou replicado se necessário. O API Gateway lida com picos de tráfego facilmente, podendo proteger o backend de sobrecarga através de throttling configurável.\n- **Segurança:** Todos os componentes sensíveis (banco de dados e servidores) residem em sub-redes privadas, minimizando superfície de ataque. Somente o API Gateway e o bucket S3 (conteúdo estático) são públicos. Conexões ao API Gateway são HTTPS seguros. Poderíamos integrar autorizadores no API Gateway para exigir tokens JWT ou API keys para acesso às APIs, sem ter que codificar isso no backend. Além disso, o API Gateway nos isola de expor diretamente o endpoint do EC2, podendo aplicar WAF (firewall de aplicações) se desejado.\n- **Manutenibilidade:** Usando CloudFormation, toda a infraestrutura está documentada como código. Podemos recriar este ambiente em outra região ou conta facilmente. Além disso, a infraestrutura está modular: a Landing Zone de rede pode ser reutilizada para outros projetos, o stack do backend BFF e RDS é independente do stack do front-end, etc. Usamos parâmetros e outputs para integrar stacks de forma limpa (VPC, subnets, endpoints).\n\n**Referências oficiais úteis:**\n- Documentação do AWS CloudFormation foi usada extensivamente para definir os recursos de VPC, EC2, S3, RDS e API Gateway.\n- A AWS Prescriptive Guidance e blogs enfatizam as vantagens do padrão BFF e como ele complementa o uso de um API Gateway.\n- Amazon S3 estático: referenciado para configurar corretamente permissões e hosting.\n- Amazon RDS boas práticas: mantivemos instância não pública conforme indicado pela AWS.\n- Amazon API Gateway: usamos o modelo HTTP API com *quick create*, lembrando que o API Gateway é um serviço totalmente gerenciado para publicação e manutenção de APIs em qualquer escala, ideal para nossa solução.\n\nCom isso, encerramos os tutoriais. Você agora tem uma pequena aplicação exemplo rodando em uma infraestrutura moderna, **baseada em IaC** e pronta para crescimento. De monólito a BFF, cobrimos todo o ciclo com implantação automatizada. Esperamos que este passo-a-passo didático tenha ajudado a compreender não só o \"como fazer\", mas também o \"porquê\" de cada decisão de arquitetura. Bom proveito e boa implantação!\n"
  }
];
