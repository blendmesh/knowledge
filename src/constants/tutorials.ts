export const mockTutorials = [
  {
    "id": "ff9e5d7d-005c-466c-8e32-b30f30a1eb5b",
    "title": "Preparar o ferramental e ambiente",
    "description": "Preparar o ambiente de desenvolvimento: instala√ß√£o do Terraform, configura√ß√£o do AWS CLI e organiza√ß√£o da estrutura de arquivos do projeto.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/ff9e5d7d-005c-466c-8e32-b30f30a1eb5b",
    "markdown": "A Infraestrutura como C√≥digo (IaC) √© o processo de gerenciar sua infraestrutura (servidores, redes, servi√ßos em nuvem etc.) usando configura√ß√£o declarativa em vez de procedimentos manuais. O Terraform, criado pela HashiCorp, √© uma das principais ferramentas de IaC dispon√≠veis. Ele permite **construir, mudar e versionar** a infraestrutura de forma segura e eficiente, suportando desde componentes de baixo n√≠vel (como inst√¢ncias EC2, buckets S3, redes) at√© componentes de alto n√≠vel (como DNS e recursos de SaaS) em v√°rios provedores de nuvem.\n\nAntes de come√ßarmos a usar o Terraform, precisamos preparar o ambiente de trabalho com as seguintes etapas:\n\n- **1. Criar uma conta na AWS:** Se ainda n√£o tiver, [crie uma conta AWS](/blog/3a60fae5-2431-4b18-8763-75815bfad331) (a maioria dos servi√ßos oferece um free tier). Em ambientes de produ√ß√£o, √© recomend√°vel **n√£o usar as credenciais root** da conta AWS para automa√ß√£o. Em vez disso, [crie um usu√°rio IAM espec√≠fico para o Terraform com as permiss√µes necess√°rias](/blog/206b42e6-6209-46e2-a86f-b4a4a815d6df). Para simplificar este tutorial, voc√™ pode atribuir temporariamente permiss√µes administrativas completas a esse usu√°rio IAM (n√£o √© boa pr√°tica em cen√°rios reais, mas facilita o aprendizado inicial).\n    \n- **2. Gerar credenciais de acesso (Access Key ID e Secret Access Key):** Ap√≥s criar o usu√°rio IAM, gere um par de chaves de acesso (Access Key ID e Secret Access Key) para ele. A Access Key ID funciona como um nome de usu√°rio e a Secret Access Key como uma senha para acesso program√°tico √† AWS. **Importante:** Salve essas credenciais em local seguro, pois a chave secreta s√≥ √© exibida no momento da cria√ß√£o e n√£o poder√° ser recuperada depois. A AWS permite no m√°ximo duas chaves por usu√°rio, e recomenda usar credenciais tempor√°rias quando poss√≠vel.\n    \n- **3. Instalar o AWS CLI (opcional, mas recomendado):** O AWS CLI facilita configurar credenciais e testar acesso. [Baixe e instale a AWS CLI (v2) para seu sistema operacional](/blog/3e3b870b-8fbb-4b1b-8998-6ab4e233b7f4). Em seguida, execute **aws configure** no terminal e insira a **Access Key ID**, **Secret Access Key**, regi√£o padr√£o (por exemplo, **us-east-1**) e formato de sa√≠da (por exemplo, **json**) quando solicitado. Isso salvar√° suas credenciais em **~/.aws/credentials** sob o perfil \"default\". O Terraform posteriormente poder√° detectar essas credenciais automaticamente neste arquivo.\n    \n- **4. Criar um par de chaves SSH (para acesso a inst√¢ncias EC2):** Caso planeje criar inst√¢ncias EC2, ser√° necess√°rio um par de chaves SSH para conect√°-las. No console AWS, navegue at√© _EC2 > Network & Security > Key Pairs_ e clique em **Create key pair**. Informe um nome (ex: \"terraform-key\") e salve o arquivo PEM baixado (este √© a chave privada) em local seguro. Essa chave ser√° usada para acesso SSH √†s m√°quinas e pelo Terraform para provisionamento remoto. _(Observa√ß√£o: voc√™ tamb√©m pode usar um par de chaves existente caso j√° tenha um.)_\n    \n- **5. Instalar o Terraform CLI:** A HashiCorp distribui o [Terraform como um bin√°rio execut√°vel √∫nico](/blog/32a6aac2-1188-4448-b838-e33fd0908a92). Baixe a vers√£o mais recente do Terraform no site oficial ([https://developer.hashicorp.com/terraform/install](https://developer.hashicorp.com/terraform/install)). Se estiver em **Windows**, baixe o zip, extraia o **terraform.exe** e coloque-o em uma pasta (por exemplo, **C:\\Terraform**). No **Linux ou macOS**, baixe o pacote, descompacte o bin√°rio e mova-o para um diret√≥rio do PATH do sistema (como **/usr/local/bin/**). Ap√≥s isso, verifique a instala√ß√£o abrindo um terminal e executando:\n    \n\n```bash\nterraform -version\n```\n\nEsse comando deve exibir a vers√£o do Terraform instalada. Por exemplo:\n\n```plaintext\nTerraform v1.12.1\non linux_amd64\n```\n\nSe o comando for reconhecido (mesmo que sua vers√£o seja diferente), o Terraform CLI est√° instalado corretamente. Se n√£o, verifique se o diret√≥rio do bin√°rio est√° no PATH ou repita a instala√ß√£o.\n\nAgora, com a ferramenta instalada e as credenciais configuradas, temos o ambiente pronto para criar nossa primeira infraestrutura com Terraform. Nos tutoriais seguintes, construiremos passo a passo uma configura√ß√£o Terraform, usando a AWS como provedor de nuvem escolhido."
  },
  {
    "id": "723dc4aa-284e-4cb9-9fc5-bc2fb61fb68f",
    "title": "Fun√ß√µes B√°sicas",
    "description": "Introduzir a constru√ß√£o de recursos utilizando HCL (HashiCorp Configuration Language). Vamos criar uma VPC, subnets e uma inst√¢ncia EC2 como exemplo.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Fundamentos",
      "VPC",
      "Subnet",
      "EC2",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/723dc4aa-284e-4cb9-9fc5-bc2fb61fb68f",
    "markdown": "Com o ambiente configurado, vamos criar uma infraestrutura simples na AWS usando o Terraform, para entender suas fun√ß√µes b√°sicas. Os **passos b√°sicos** de uso do Terraform s√£o: **escrever a configura√ß√£o**, **inicializar os plugins/provedores**, **planejar as mudan√ßas** e **aplicar (provisionar) a infraestrutura**. Vamos detalhar cada passo criando um pequeno exemplo de recurso AWS.\n\n**1. Escrever a configura√ß√£o Terraform (provider e resource):**\n\nCrie uma pasta de projeto (por exemplo, **terraform-projeto1**) e dentro dele um arquivo chamado **main.tf**. Arquivos **.tf** cont√™m a configura√ß√£o em HCL (HashiCorp Configuration Language). Vamos come√ßar definindo o provedor AWS e um recurso simples (uma inst√¢ncia EC2) neste arquivo:\n\n```hcl\n# main.tf\n\n# Define o provedor AWS e a regi√£o onde os recursos ser√£o criados\nprovider \"aws\" {\n  region = \"us-east-1\"        # Regi√£o AWS (ex: us-east-1)\n}\n\n# Recurso: Inst√¢ncia EC2\nresource \"aws_instance\" \"exemplo\" {\n  ami           = \"ami-0fc5d935ebf8bc3bc\"   # ID da imagem AMI para o servidor (Ubuntu 20.04 LTS us-east-1)\n  instance_type = \"t2.micro\"               # Tipo da inst√¢ncia (t2.micro √© eleg√≠vel ao free tier)\n  \n  key_name      = \"terraform-key\"          # Nome do par de chaves SSH para acesso (criado no passo anterior)\n  \n  tags = {\n    Name = \"ServidorExemplo\"              # Tag Name para identificar a inst√¢ncia no console AWS\n  }\n}\n```\n\n**Sobre o c√≥digo acima:** No bloco **provider \"aws\"**, especificamos qual provedor de nuvem usar e configuramos op√ß√µes como regi√£o. O Terraform usa _providers_ (provedores) para interagir com APIs de diferentes plataformas cloud. Para usar a AWS, utilizamos o provedor AWS, que sabe gerenciar recursos da AWS. A regi√£o **us-east-1** foi escolhida aqui apenas como exemplo; voc√™ pode ajust√°-la para outra regi√£o AWS de prefer√™ncia.\n\nNo bloco **resource \"aws_instance\" \"exemplo\"**, estamos declarando que desejamos criar um recurso do tipo AWS EC2 Instance. Cada recurso em Terraform √© identificado pelo **tipo** e um **nome interno** (no caso, tipo **aws_instance** e nome **exemplo**). Esses identificadores servem para o Terraform mapear e referenciar o recurso. Dentro do recurso, definimos os **atributos** necess√°rios:\n\n- **ami**: a Amazon Machine Image usada para instancia√ß√£o. Aqui usamos uma AMI p√∫blica de Ubuntu Server 20.04 em us-east-1 (cada regi√£o tem IDs pr√≥prios; esta AMI ID √© usada como exemplo e pode estar desatualizada no futuro).\n- **instance_type**: o tamanho da inst√¢ncia (t2.micro).\n- **key_name**: o nome do par de chaves para habilitar acesso SSH √† inst√¢ncia. Certifique-se de usar o nome do Key Pair que voc√™ criou anteriormente na AWS.\n- **tags**: bloco opcional para etiquetar o recurso. Colocamos uma tag ‚ÄúName‚Äù para identificar a inst√¢ncia pelo nome no console.\n\n_Observa√ß√£o:_ N√£o especificamos credenciais aqui no c√≥digo. O provedor AWS ir√° procurar credenciais de forma padr√£o (primeiro em vari√°veis de ambiente **AWS_ACCESS_KEY_ID** e **AWS_SECRET_ACCESS_KEY**, depois no arquivo de credenciais **~/.aws/credentials** perfil default, etc.). Como configuramos o AWS CLI no Tutorial 1, o Terraform dever√° encontrar as credenciais automaticamente no perfil _default_.\n\n> _Dica:_ Em projetos reais, √© recomendado **fixar a vers√£o do provedor** para evitar atualiza√ß√µes inesperadas. Isso pode ser feito com um bloco **terraform { required_providers { ... } }** especificando a fonte e vers√£o do provedor AWS. Aqui omitimos para simplificar, usando a vers√£o mais recente dispon√≠vel no momento da inicializa√ß√£o.\n\n**2. Inicializar o projeto Terraform:**\n\nAbra um terminal na pasta do projeto (**terraform-projeto1**) e execute o comando abaixo para inicializar o diret√≥rio de trabalho:\n\n```bash\nterraform init\n```\n\nEsse comando realiza a **inicializa√ß√£o** do Terraform no diret√≥rio atual. Ele ir√° baixar o plugin do provedor AWS necess√°rio (caso ainda n√£o esteja em cache) e configurar o ambiente local. Voc√™ dever√° ver mensagens indicando o download do provider AWS. O Terraform analisa a configura√ß√£o em busca de refer√™ncias a provedores e instala automaticamente os plugins necess√°rios. Ap√≥s o **init**, ser√° criada uma pasta oculta **.terraform/** com os plugins baixados, e um arquivo **terraform.lock.hcl** com as vers√µes usadas.\n\n**3. Revisar o plano de execu√ß√£o (terraform plan):**\n\nAntes de aplicar as mudan√ßas, √© uma boa pr√°tica visualizar o **plano de execu√ß√£o**. Execute:\n\n```bash\nterraform plan\n```\n\nO Terraform ir√° ler a configura√ß√£o e comparar com o estado atual da infraestrutura (no primeiro uso, n√£o h√° nada provisionado ainda). Como o recurso **aws_instance.exemplo** n√£o existe na AWS, o plano mostrar√° que esse recurso ser√° **criado**. A sa√≠da do **plan** lista as a√ß√µes que ser√£o tomadas sem efetuar mudan√ßas reais ainda. Voc√™ deve ver algo como:\n\n```plaintext\nPlan: 1 to add, 0 to change, 0 to destroy.\n```\n\nIsso indica que 1 recurso ser√° adicionado. O comando **terraform plan** permite revisar se a infraestrutura que ser√° criada corresponde ao desejado, servindo como uma etapa de valida√ß√£o antes da execu√ß√£o real.\n\n**4. Aplicar a configura√ß√£o (terraform apply):**\n\nEstando satisfeito com o plano, podemos provisionar de fato a infraestrutura:\n\n```bash\nterraform apply\n```\n\nO Terraform mostrar√° novamente o plano e perguntar√° **Do you want to perform these actions?** para confirmarmos. Responda **yes** para prosseguir. (Voc√™ pode adicionar **-auto-approve** para pular a confirma√ß√£o, mas por seguran√ßa n√£o faremos isso agora). O comando **apply** ent√£o **executa as a√ß√µes propostas**, criando os recursos na AWS. Isso incluir√° chamar a API da AWS para iniciar a inst√¢ncia EC2 com as caracter√≠sticas definidas.\n\nSe tudo ocorrer bem, ap√≥s alguns segundos/minutos, a sa√≠da ir√° indicar **Apply complete! Resources: 1 added, 0 changed, 0 destroyed.**. Voc√™ tamb√©m poder√° ver informa√ß√µes sobre o recurso criado, como o ID da inst√¢ncia EC2, IP p√∫blico, etc.\n\nAgora sua inst√¢ncia EC2 est√° criada na AWS. Voc√™ pode confirmar acessando o console AWS EC2 na regi√£o escolhida e verificando se h√° uma inst√¢ncia em execu√ß√£o com a tag Name \"ServidorExemplo\". Parab√©ns, voc√™ provisionou sua primeira infraestrutura com Terraform! üéâ\n\n**5. Limpar recursos (terraform destroy):**\n\nComo boa pr√°tica, vamos **destruir** o recurso criado para evitar custos desnecess√°rios e manter o ambiente limpo. O Terraform facilita isso com:\n\n```bash\nterraform destroy\n```\n\nSemelhante ao apply, ele mostrar√° um plano (desta vez indicando que o recurso ser√° destru√≠do) e pedir√° confirma√ß√£o. Digite **yes** para confirmar. O Terraform ent√£o chamar√° a API da AWS para terminar a inst√¢ncia EC2. Ao final, a mensagem deve indicar que o recurso foi removido: **Destroy complete! Resources: 1 destroyed.**. O comando **terraform destroy** **deprovisiona todos os objetos gerenciados pela configura√ß√£o Terraform**, ou seja, remove tudo que foi criado pelo apply, seguindo o estado conhecido. Use-o com cautela, especialmente em ambientes n√£o-desej√°veis de perder ‚Äì ele √© √∫til em dev/testes, mas em produ√ß√£o normalmente voc√™ gerenciaria remo√ß√µes de forma mais controlada.\n\n> **Recapitulando os comandos principais:**\n> \n> - **terraform init** ‚Äì inicializa o ambiente (baixar provedores, m√≥dulos).\n> - **terraform plan** ‚Äì mostra as mudan√ßas que ser√£o feitas (fase de revis√£o).\n> - **terraform apply** ‚Äì aplica as mudan√ßas, provisionando ou atualizando a infra.\n> - **terraform destroy** ‚Äì destr√≥i os recursos provisionados pelo Terraform.\n> \n> Esses comandos formam o n√∫cleo do workflow do Terraform. Em execu√ß√£o t√≠pica: voc√™ altera os arquivos **.tf** (config), roda **plan** para verificar e ent√£o **apply** para efetivar.\n\nCom esses passos b√°sicos, aprendemos a preparar uma configura√ß√£o Terraform e criar recursos na nuvem AWS de forma declarativa. No pr√≥ximo tutorial, vamos tornar nossa configura√ß√£o mais flex√≠vel usando **vari√°veis** de entrada e **outputs**."
  },
  {
    "id": "c0b2f47b-7dfb-440c-8276-8690d2a4a1bf",
    "title": "Vari√°veis e Outputs no Terraform",
    "description": "Parametrizar nossa infraestrutura usando vari√°veis de entrada e exibir resultados com outputs. Dessa forma, o c√≥digo se torna mais flex√≠vel e reutiliz√°vel.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Fundamentos",
      "Variable",
      "Output",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/c0b2f47b-7dfb-440c-8276-8690d2a4a1bf",
    "markdown": "Ao criar configura√ß√µes Terraform, √© comum evitar valores fixos (hardcodes) para torn√°-las reutiliz√°veis e parametriz√°veis. **Vari√°veis de entrada** permitem customizar aspectos da infraestrutura sem alterar o c√≥digo, enquanto **outputs** exp√µem valores √∫teis da infraestrutura provisionada. Vamos adicionar vari√°veis e outputs ao nosso exemplo.\n\n**1. Definindo vari√°veis de entrada:**\n\nNo Terraform, vari√°veis de entrada s√£o declaradas usando o bloco **variable**. Elas podem ter um tipo, um valor padr√£o, e uma descri√ß√£o. Vamos modificar nossa configura√ß√£o para usar vari√°veis:\n\n- Crie um arquivo separado para vari√°veis, por exemplo **variables.tf** (opcional, apenas organiza√ß√£o). Dentro dele, declare:\n\n```hcl\n# variables.tf\n\nvariable \"regiao\" {\n  description = \"Regi√£o AWS onde criar os recursos\"\n  default     = \"us-east-1\"\n}\n\nvariable \"ami_id\" {\n  description = \"ID da imagem AMI para a inst√¢ncia EC2\"\n  default     = \"ami-0fc5d935ebf8bc3bc\"\n}\n\nvariable \"instancia_tipo\" {\n  description = \"Tipo da inst√¢ncia EC2\"\n  default     = \"t2.micro\"\n}\n\nvariable \"nome_chave\" {\n  description = \"Nome do par de chaves SSH para acesso √† inst√¢ncia\"\n  default     = \"terraform-key\"\n}\n\nvariable \"nome_instancia\" {\n  description = \"Nome (Tag) da inst√¢ncia EC2\"\n  default     = \"ServidorExemplo\"\n}\n```\n\nAqui definimos 5 vari√°veis com valores padr√£o. O uso de **default** torna a vari√°vel **opcional**, pois caso nenhuma valor seja fornecido, o Terraform usa o default especificado. Por exemplo, definimos um AMI padr√£o e tipo de inst√¢ncia padr√£o. Em cen√°rios reais, voc√™ poderia omitir o default de algumas vari√°veis importantes para for√ßar o usu√°rio a fornecer (nesse caso, o Terraform pediria valor no **terraform apply** caso n√£o fosse passado nenhum).\n\n- Atualize o arquivo **main.tf** para usar essas vari√°veis em vez de valores fixos:\n\n```hcl\n# main.tf (atualizado para usar vari√°veis)\n\nprovider \"aws\" {\n  region = var.regiao     # usa a vari√°vel \"regiao\"\n}\n\nresource \"aws_instance\" \"exemplo\" {\n  ami           = var.ami_id          # usa a vari√°vel \"ami_id\"\n  instance_type = var.instancia_tipo  # usa a vari√°vel \"instancia_tipo\"\n  key_name      = var.nome_chave      # usa a vari√°vel \"nome_chave\"\n\n  tags = {\n    Name = var.nome_instancia        # usa a vari√°vel \"nome_instancia\"\n  }\n}\n```\n\nNote o prefixo **var.** para referenciar o valor de uma vari√°vel dentro do c√≥digo Terraform. Estamos interpolando as vari√°veis nas configura√ß√µes. Essa abordagem torna nosso c√≥digo mais gen√©rico e f√°cil de reutilizar. Por exemplo, se quisermos lan√ßar a inst√¢ncia em outra regi√£o ou com outro tipo, basta fornecer valores diferentes √†s vari√°veis, sem editar o c√≥digo em si.\n\n> _Compara√ß√£o com programa√ß√£o tradicional:_ Se voc√™ est√° familiarizado com fun√ß√µes, pode imaginar as **vari√°veis de entrada como par√¢metros de fun√ß√£o** e os **outputs como retorno**. Vari√°veis permitem passar argumentos para a ‚Äúfun√ß√£o‚Äù (m√≥dulo Terraform) sem precisar alterar seu interior.\n\n**2. Fornecendo valores para vari√°veis:**\n\nH√° diversas maneiras de definir os valores de vari√°veis no Terraform:\n\n- **Defaults no c√≥digo:** como fizemos acima, garante um valor padr√£o.\n- **Arquivo .tfvars:** voc√™ pode criar um arquivo (ex: **terraform.tfvars**) listando vari√°veis e valores, que o Terraform carrega automaticamente.\n- **Linha de comando:** usar a op√ß√£o **-var \"nome=valor\"** no **terraform apply/plan** para setar diretamente.\n- **Vari√°veis de ambiente:** exportar **TF_VAR_nome** no shell para cada vari√°vel.\n\nPor agora, podemos confiar nos valores default. Se quiser testar diferentes valores, crie um arquivo **terraform.tfvars** no projeto com conte√∫do, por exemplo:\n\n```hcl\nregiao = \"us-west-2\"\nnome_instancia = \"ServidorOeste\"\n```\n\nIsso mudaria a regi√£o para Oregon (us-west-2) e o nome da inst√¢ncia. Ao rodar **terraform apply**, o Terraform automaticamente l√™ esse arquivo e substitui nos defaults das vari√°veis correspondentes.\n\n**3. Declarando outputs:**\n\nOutputs s√£o sa√≠das que o Terraform pode exibir ap√≥s o apply, ou que podem ser consumidos por outros m√≥dulos. Eles s√£o √∫teis para mostrar informa√ß√µes importantes da infraestrutura provisionada (IP da m√°quina, URL gerada, ID de recurso, etc). Vamos adicionar um output para ilustrar:\n\nNo arquivo **outputs.tf** (pode criar um novo arquivo), declare, por exemplo:\n\n```hcl\n# outputs.tf\n\noutput \"ip_publico\" {\n  description = \"IP p√∫blico da inst√¢ncia EC2 criada\"\n  value       = aws_instance.exemplo.public_ip\n}\n```\n\nExplica√ß√£o:\n\n- O bloco **output** nomeia uma sa√≠da (**ip_publico**) e define seu valor. Aqui usamos a refer√™ncia **aws_instance.exemplo.public_ip** ‚Äì ou seja, estamos acessando o atributo **public_ip** do recurso que criamos. O Terraform, ao concluir o apply, capturar√° esse valor do estado e exibir√° para n√≥s.\n- Adicionamos uma descri√ß√£o para clareza (opcional, mas √∫til).\n\nVoc√™ pode adicionar m√∫ltiplos outputs se quiser. Por exemplo, poder√≠amos expor tamb√©m o ID da inst√¢ncia ou o DNS p√∫blico:\n\n```hcl\noutput \"id_instancia\" {\n  value = aws_instance.exemplo.id\n}\n```\n\n**4. Aplicar a configura√ß√£o com vari√°veis e outputs:**\n\nSalve os arquivos modificados/novos e execute novamente o ciclo:\n\n```bash\nterraform init        # (n√£o necess√°rio se j√° feito antes e n√£o se adicionou novos providers)\nterraform plan\nterraform apply\n```\n\nNo **plan**, agora o Terraform deve mostrar nenhuma mudan√ßa se voc√™ n√£o alterou valores (pois j√° criamos a inst√¢ncia no tutorial 2). Por√©m, note que adicionamos outputs ‚Äì e outputs n√£o alteram infraestrutura, s√£o apenas informativos. Assim, possivelmente o **plan** indicar√° **0 to add, 0 to change, 0 to destroy** (nenhum recurso mudou).\n\nAp√≥s um **terraform apply** (que reconhecer√° que nada precisa ser criado/destro√≠do), o Terraform exibir√° os valores de output. Por exemplo, dever√° aparecer algo como:\n\n```plaintext\nOutputs:\n\nid_instancia = \"i-0bb...123\"\nip_publico = \"3.85.XX.XX\"\n```\n\nAgora temos o IP p√∫blico facilmente vis√≠vel, sem precisar ir ao console AWS. O output tamb√©m fica acess√≠vel via comando CLI **terraform output** a qualquer momento ap√≥s um apply, ou pode ser utilizado por outros m√≥dulos Terraform se esta configura√ß√£o fosse consumida como m√≥dulo. Em resumo, **outputs exp√µem informa√ß√µes da infraestrutura** para o usu√°rio ou para outros componentes, de forma semelhante a valores de retorno de uma fun√ß√£o.\n\n**5. Refinando tipos e valida√ß√µes (opcional):**\n\nNas vari√°veis, podemos exigir tipos espec√≠ficos. Por padr√£o, se n√£o definirmos **type**, qualquer tipo √© aceito (string, number, bool, lista, etc.). No exemplo acima, todas as vari√°veis acabaram sendo strings. Poder√≠amos definir explicitamente, por exemplo:\n\n```hcl\nvariable \"instancia_tipo\" {\n  type        = string\n  default     = \"t2.micro\"\n  description = \"Tipo da inst√¢ncia EC2\"\n}\n```\n\nO Terraform suporta tipos complexos como **list(string)**, **map(number)**, **object({...})** etc., o que permite estruturas de dados mais ricas. Tamb√©m podemos adicionar restri√ß√µes (validation rules) e marcar vari√°veis como **sensitive** (para que valores n√£o apare√ßam em logs).\n\nNo contexto deste tutorial, n√£o aprofundaremos nessas op√ß√µes, mas saiba que elas existem para tornar m√≥dulos robustos e seguros contra entradas inv√°lidas.\n\n**Resumo:** As **vari√°veis de entrada** tornam o c√≥digo Terraform flex√≠vel e reutiliz√°vel, evitando a necessidade de editar o c√≥digo para cada contexto. J√° os **outputs** fornecem um jeito organizado de obter dados resultantes da infraestrutura criada. Juntos, eles facilitam o uso de um mesmo conjunto de configura√ß√µes em diferentes ambientes (dev, hml, prod, cada um com par√¢metros diferentes) sem duplica√ß√£o de c√≥digo.\n\nNo pr√≥ximo tutorial, entenderemos como o Terraform gerencia o **state (estado)** da infraestrutura e porque ele √© crucial no funcionamento da ferramenta.\n"
  },
  {
    "id": "f82224ce-d909-4fea-9bcc-8a353b1b05f7",
    "title": "Terraform State (Estado do Terraform)",
    "description": "Compreender como o Terraform gerencia o estado dos recursos, a import√¢ncia do arquivo de state e como trabalhar com backends remotos.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Backend",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/f82224ce-d909-4fea-9bcc-8a353b1b05f7",
    "markdown": "O **Terraform State** √© o cora√ß√£o do funcionamento do Terraform. Toda vez que aplicamos uma configura√ß√£o, o Terraform **armazena informa√ß√µes sobre os recursos provisionados** em um arquivo de estado. Esse arquivo atua como a fonte da verdade (_single source of truth_) que o Terraform usa para acompanhar o que foi criado e mapear para os recursos reais na nuvem.\n\nVamos entender o state por partes:\n\n**1. O que √© o arquivo de estado?**\n\nAp√≥s executar **terraform apply** pela primeira vez, voc√™ ver√° que um arquivo chamado **terraform.tfstate** foi gerado no diret√≥rio do projeto. Esse arquivo (no formato JSON) cont√©m a representa√ß√£o da infraestrutura que o Terraform gerenciou. Ele inclui todos os recursos e seus atributos conhecidos. Por exemplo, para nossa inst√¢ncia EC2, o state armazenar√° seu ID (**i-...**), IP, AMI usada, tags, etc. O state **mapeia os recursos definidos no c√≥digo com os objetos reais na AWS**.\n\n- O Terraform usa esse arquivo em execu√ß√µes futuras para saber o que j√° existe. Quando rodamos **terraform plan** ou **apply** subsequente, ele carrega o estado e compara com a configura√ß√£o desejada para determinar quais altera√ß√µes s√£o necess√°rias (adi√ß√£o, mudan√ßa ou destrui√ß√£o de recursos).\n- **N√£o edite manualmente** o arquivo de estado. Altera√ß√µes manuais podem corromper o mapeamento e levar a inconsist√™ncias. O Terraform oferece comandos pr√≥prios para manipular o estado se necess√°rio (como **terraform state rm**, **mv**, etc.), mas essa √© uma funcionalidade avan√ßada para casos especiais.\n\nPara visualizar o conte√∫do do state de forma leg√≠vel, voc√™ pode usar o comando:\n\n```bash\nterraform show terraform.tfstate\n```\n\nque exibir√° os recursos e atributos atuais conforme registrados. Por exemplo, uma se√ß√£o do state para nossa inst√¢ncia EC2 pode parecer assim (resumido):\n\n```json\n{\n    \"resources\": [{\n        \"type\": \"aws_instance\",\n        \"name\": \"exemplo\",\n        \"provider\": \"provider[\\\"registry.terraform.io/hashicorp/aws\\\"]\",\n        \"instances\": [{\n            \"attributes\": {\n                \"id\": \"i-0bb...123\",\n                \"ami\": \"ami-0fc5d935ebf8bc3bc\",\n                \"instance_type\": \"t2.micro\",\n                \"public_ip\": \"3.85.xx.xx\",\n                \"tags\": {\n                    \"Name\": \"ServidorExemplo\"\n                },\n                // ... outros atributos ...\n            }\n        }]\n    }]\n}\n```\n\nIsso mostra o recurso **aws_instance.exemplo** com seus atributos atuais. O Terraform compara esses valores com o que est√° nos arquivos **.tf** para detectar diferen√ßas.\n\n**2. Import√¢ncia do state:**\n\n- **Fonte de verdade:** O state √© considerado pela ferramenta como o estado real conhecido da sua infra. Se algu√©m modificar manualmente a infraestrutura fora do Terraform (ex: mudar uma configura√ß√£o da inst√¢ncia via console AWS), o state do Terraform ficar√° desatualizado em rela√ß√£o ao mundo real. √â poss√≠vel ent√£o que um **terraform plan** indique mudan√ßas (pois ao refrescar ele percebe diverg√™ncias). Portanto, mantenha a disciplina de usar o Terraform para mudar recursos, ou se alterar externamente, sincronize (via **terraform refresh** ou import) para evitar surpresas.\n- **Performance:** Com o state, o Terraform n√£o precisa consultar a API de todos os recursos toda vez ‚Äì ele j√° sabe o que existe e s√≥ consulta mudan√ßas relevantes, tornando as opera√ß√µes mais eficientes em infraestruturas grandes.\n- **Tracking metadata e depend√™ncias:** O state guarda metadados e permite ao Terraform entender as depend√™ncias entre recursos. Por exemplo, se um recurso depende de outro (implicitamente por refer√™ncia ou explicitamente via **depends_on**), o Terraform usa o state para orquestrar a ordem de cria√ß√£o/remo√ß√£o corretamente.\n\n**3. Backend do state (armazenamento local vs remoto):**\n\nPor padr√£o, o Terraform salva o arquivo **terraform.tfstate** localmente (no mesmo diret√≥rio). Isso funciona bem para uso individual ou ambientes de teste. Por√©m, em contextos colaborativos ou de equipe, **√© problem√°tico compartilhar o state via arquivo local**, pois apenas uma pessoa teria as informa√ß√µes atualizadas. Al√©m disso, se dois membros aplicarem mudan√ßas ao mesmo tempo usando estados separados, podem ocorrer conflitos e infra inconsistente.\n\nPara resolver isso, o Terraform suporta **backends remotos** para o state:\n\n- Voc√™ pode configurar o Terraform para armazenar o state em um local remoto centralizado, como um bucket S3 na AWS, um armazenamento no Azure, Terraform Cloud/Enterprise, entre outros. Por exemplo, usando um backend S3, todos os membros da equipe apontam para o mesmo arquivo de state na nuvem. Assim, quando algu√©m faz um apply, o arquivo remoto √© atualizado e outro usu√°rio obt√©m essas atualiza√ß√µes automaticamente no pr√≥ximo run.\n- Backends remotos frequentemente suportam **bloqueio de estado (state locking)** para prevenir condi√ß√µes de corrida. No caso do AWS S3, normalmente configura-se um bloqueio usando DynamoDB junto com o bucket S3 (o Terraform adquire um lock no DynamoDB durante opera√ß√µes para que dois processos n√£o alterem o state simultaneamente).\n\nConfigurar backend: √â feito adicionando um bloco **backend** dentro do bloco **terraform {}**. Exemplo de configura√ß√£o para S3 (sintaxe ilustrativa):\n\n```hcl\nterraform {\n  backend \"s3\" {\n    bucket = \"meu-bucket-terraform-state\"\n    key    = \"dev/state.tfstate\"    # caminho/nome do arquivo no bucket\n    region = \"us-east-1\"\n    dynamodb_table = \"terraform-locks\"  # tabela DynamoDB para lock (j√° deve existir)\n    encrypt = true\n  }\n}\n```\n\nAp√≥s configurar, rodamos **terraform init** novamente para migrar o state local para o remoto. A partir da√≠, o state reside no S3.\n\n_Para prop√≥sitos deste tutorial, manteremos o backend local default._ Mas fique ciente de que em projetos profissionais quase sempre usa-se backends remotos para **compartilhar o state** de forma segura.\n\n**4. Comandos √∫teis de state:** O Terraform oferece subcomandos para inspecionar ou modificar o state:\n\n- **terraform state list** ‚Äì lista todos os recursos presentes no estado atual (√∫til para ver o que Terraform est√° gerenciando).\n- **terraform state show <RESOURCE>** ‚Äì mostra os atributos espec√≠ficos de um recurso do state.\n- **terraform import** ‚Äì permite importar um recurso criado manualmente na AWS para dentro do state do Terraform (avan√ßado, para ado√ß√£o gradual de Terraform em infra existente).\n- **terraform state mv** / **rm** ‚Äì para mover ou remover manualmente entradas do state (casos especiais de refatora√ß√£o).\n\nEm geral, manipular state manualmente requer cautela. A necessidade de fazer isso indica frequentemente que queremos reorganizar configura√ß√µes ou corrigir algo fora do comum. Nas opera√ß√µes cotidianas, **o Terraform gerencia o state automaticamente**.\n\n**5. Protegendo o state:** O arquivo state cont√©m informa√ß√µes sens√≠veis ‚Äì por exemplo, dados de recursos podem incluir IPs, IDs, e possivelmente segredos (se voc√™ gerencia senhas/keys via Terraform, elas podem aparecer no state em texto plano). Portanto:\n\n- Adicione o **terraform.tfstate** (e o diret√≥rio **.terraform/**) ao **.gitignore** caso use Git, **n√£o commit** esse arquivo em controle de vers√£o.\n- Ao usar backends remotos, restrinja o acesso ao bucket ou armazenamento para apenas as pessoas/processos necess√°rios.\n- Considere ativar criptografia no backend remoto (ex: S3 com SSE ou usar Vault/Terraform Cloud que criptografam automaticamente).\n\n**Resumo:** O **state** do Terraform √© o mecanismo que permite que ele saiba o que est√° acontecendo na sua infraestrutura. Ele **mapeia** as configura√ß√µes declarativas aos recursos reais e registra o estado atual deles. Esse conceito possibilita planos precisos e aplica√ß√µes idempotentes. Entender e gerenciar o state (especialmente em equipe, usando backends remotos e locks) √© fundamental para um uso seguro do Terraform em ambientes profissionais.\n\nTendo coberto o state, seguimos em frente para recursos mais avan√ßados. No pr√≥ximo tutorial, veremos os **Provisioners**, que permitem executar a√ß√µes adicionais (scripts/comandos) durante a cria√ß√£o ou destrui√ß√£o de recursos."
  },
  {
    "id": "1ca38694-1270-4b9f-b46f-e16b2847a9db",
    "title": "Provisioners (Executando Scripts e Comandos)",
    "description": "Ensinar como executar comandos e scripts durante a cria√ß√£o dos recursos, utilizando provisioners como remote-exec e local-exec.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Fundamentos",
      "Provisioner",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/1ca38694-1270-4b9f-b46f-e16b2847a9db",
    "markdown": "Normalmente, o Terraform se limita a declarar infraestrutura (inst√¢ncias, redes, buckets etc.). Mas e se quisermos executar alguma configura√ß√£o extra, como rodar um script dentro de uma m√°quina criada ou executar um comando local ap√≥s criar um recurso? Para esses casos, o Terraform oferece os **Provisioners**.\n\nProvisioners s√£o trechos de configura√ß√£o que permitem executar scripts _depois_ que um recurso √© criado ou _antes/depois_ de ser destru√≠do. Eles podem agir **localmente** (na m√°quina onde o Terraform est√° sendo rodado) ou **remotamente** (dentro de uma inst√¢ncia provisionada, via SSH/WinRM). √â uma feature de \"configura√ß√£o p√≥s-provisionamento\".\n\n**‚ö†Ô∏è Aviso:** Provisioners devem ser usados como **√∫ltimo recurso**. A HashiCorp recomenda, quando poss√≠vel, usar m√©todos nativos de provisionamento (por exemplo, _user data_ no EC2, scripts de inicializa√ß√£o, ou ferramentas de ger√™ncia de configura√ß√£o como Ansible, Chef) em vez de provisioners do Terraform. Isso porque provisioners podem introduzir depend√™ncias impl√≠citas e falhas n√£o determin√≠sticas (se um script falha, o Terraform marca o recurso como \"tainted\"). Ainda assim, s√£o √∫teis em casos espec√≠ficos e vamos demonstr√°-los aqui na cria√ß√£o de uma inst√¢ncia EC2.\n\n**1. Provisioner remote-exec (executando comandos na inst√¢ncia):**\n\nVamos estender nossa configura√ß√£o da inst√¢ncia EC2 para instalar o Nginx automaticamente quando a m√°quina subir. Usaremos o provisioner **remote-exec** para isso.\n\nNo recurso **aws_instance.exemplo**, adicione dentro do bloco resource:\n\n```hcl\nresource \"aws_instance\" \"exemplo\" {\n  # ... outros atributos (ami, instance_type, etc) ...\n\n  provisioner \"remote-exec\" {\n    inline = [\n      \"sudo apt-get update -y\",\n      \"sudo apt-get install -y nginx\"\n    ]\n\n    # Configura√ß√µes de conex√£o SSH para a inst√¢ncia\n    connection {\n      type        = \"ssh\"\n      host        = self.public_ip         # IP p√∫blico da inst√¢ncia\n      user        = \"ubuntu\"               # Usu√°rio SSH (Ubuntu AMI usa 'ubuntu'; Amazon Linux usaria 'ec2-user')\n      private_key = file(\"~/.ssh/terraform-key.pem\")\n    }\n  }\n}\n```\n\n**Entendendo o c√≥digo:**\n\n- Definimos um **provisioner \"remote-exec\"** dentro do recurso EC2. Isso informa que, ap√≥s criar a inst√¢ncia, o Terraform deve **se conectar via SSH** a ela e executar alguns comandos.\n- A op√ß√£o **inline** nos permite especificar uma lista de comandos shell que ser√£o executados sequencialmente na m√°quina remota. Aqui usamos dois comandos: update dos pacotes e instala√ß√£o do nginx (considerando que a AMI √© Ubuntu/Debian). Poder√≠amos alternativamente usar **script = \"caminho/para/script.sh\"** para enviar e executar um script pronto.\n- O bloco **connection** especifica _como_ se conectar na inst√¢ncia:\n    - **host = self.public_ip** significa usar o IP p√∫blico da pr√≥pria inst√¢ncia (**self** refere-se ao recurso em contexto). Poder√≠amos usar **aws_instance.exemplo.public_ip** igualmente.\n    - **user** √© o usu√°rio SSH apropriado. **Aten√ß√£o:** varia conforme a AMI. Para Ubuntu usamos **ubuntu**, para Amazon Linux seria **ec2-user**, para RedHat **ec2-user** ou **root**, etc. Certifique-se de usar o correto ou voc√™ ter√° falha de autentica√ß√£o.\n    - **private_key** usamos a fun√ß√£o built-in **file()** para ler o conte√∫do de um arquivo de chave privada. Aqui apontamos para o arquivo PEM do par de chaves \"terraform-key\" que baixamos no Tutorial 1. Ajuste o caminho conforme onde sua chave est√° armazenada. Alternativamente, se seu agente SSH estiver carregado com a chave, poder√≠amos usar **agent = true** em vez de especificar a chave diretamente.\n\nQuando rodarmos **terraform apply** com esse provisioner adicionado, o fluxo ser√°:\n\n1. Terraform cria a inst√¢ncia EC2.\n2. Quando a inst√¢ncia estiver pronta, o Terraform tentar√° conectar via SSH (pode esperar alguns segundos at√© a porta SSH responder).\n3. Executar√° os comandos **sudo apt-get update -y** e depois **sudo apt-get install -y nginx**.\n4. Se tudo executar com sucesso, o provisioner termina com sucesso; caso algum comando retorne erro (status diferente de 0), o Terraform marcar√° o recurso como _tainted_ (com falha) e voc√™ poderia tentar aplicar novamente ap√≥s corrigir o script.\n\nAp√≥s a conclus√£o, voc√™ poderia testar acessando o IP p√∫blico da inst√¢ncia no navegador (porta 80) para ver a p√°gina padr√£o do Nginx, verificando que a instala√ß√£o ocorreu.\n\n**2. Provisioner local-exec (executando comando localmente):**\n\nO **local-exec** roda um comando no **m√°quina local** (onde o Terraform est√° rodando). Por exemplo, podemos usar para acionar alguma ferramenta ou script ap√≥s criar um recurso. Um caso de uso comum: chamar o AWS CLI local para algo n√£o suportado diretamente pelo Terraform, ou simplesmente exibir uma mensagem customizada.\n\nExemplo simples: adicionar um provisioner local-exec que s√≥ imprime uma mensagem:\n\n```hcl\nresource \"aws_instance\" \"exemplo\" {\n  # ... atributos e remote-exec ...\n  \n  provisioner \"local-exec\" {\n    command = \"echo 'Inst√¢ncia ${self.id} criada com IP ${self.public_ip}'\"\n  }\n}\n```\n\nAqui usamos **${self.id}** e **${self.public_ip}** para inserir o ID e IP da inst√¢ncia na mensagem (essa sintaxe dentro de strings √© para interpola√ß√£o de express√µes do Terraform). Esse comando ser√° executado localmente, ent√£o voc√™ ver√° essa mensagem no console do Terraform durante o apply. O **local-exec** roda assim que o recurso √© criado e n√£o depende de conex√£o SSH.\n\n**3. Provisioners em a√ß√µes de destrui√ß√£o:**\n\nTamb√©m existem provisioners que executam em destrui√ß√£o de recurso, usando a sintaxe **provisioner \"local-exec\" { when = destroy ... }** por exemplo. Isso poderia, por exemplo, rodar um script de backup antes de apagar um servidor, ou limpar algum registro. S√≥ mencionando para refer√™ncia ‚Äì n√£o usuaremos no exemplo atual.\n\n**4. Boas pr√°ticas com provisioners:**\n\n- Tente manter os scripts idempotentes. Se voc√™ aplicar novamente o Terraform, o provisioner **remote-exec** n√£o ser√° reexecutado a menos que o recurso em si precise ser recriado (ou se voc√™ for√ßar via **taint**). Ent√£o, idealmente, o script deveria poder rodar m√∫ltiplas vezes sem problemas (no nosso exemplo, reinstalar nginx repetidamente n√£o quebra nada, apt-get handle corretamente).\n- Se a configura√ß√£o da inst√¢ncia √© complexa (v√°rios pacotes, confs), considere usar _user data_ do EC2. Voc√™ pode fornecer **user_data** no recurso **aws_instance** com um script shell que a AWS executa na inicializa√ß√£o. Isso muitas vezes substitui a necessidade de um **remote-exec**, com a vantagem do script ficar registrado no ec2 e executar mesmo sem o Terraform conectado.\n- _Debug:_ erros em provisioners podem ser dif√≠ceis de diagnosticar de primeira. Use **terraform apply -auto-approve -parallelism=1** se precisar ver a sa√≠da em ordem (embora para um recurso √∫nico n√£o importe). O Terraform mostrar√° o stdout/stderr dos comandos provisionados. Em caso de falha, ele marca o recurso com erro; voc√™ pode usar **terraform taint <resource>** para for√ßar recria√ß√£o e tentar novamente ap√≥s ajustar o script.\n\n**5. Alternativa: null_resource + triggers:**\n\nUma estrat√©gia para executar a√ß√µes sem atrelar a um recurso espec√≠fico √© usar o recurso especial **null_resource**. Um **null_resource** n√£o provisiona nada na nuvem, mas voc√™ pode anexar provisioners a ele e usar o argumento **triggers** para definir depend√™ncias arbitr√°rias. Por exemplo:\n\n```hcl\nresource \"null_resource\" \"exec_depois\" {\n  triggers = {\n    instancia_id = aws_instance.exemplo.id\n  }\n\n  provisioner \"local-exec\" {\n    command = \"echo Inst√¢ncia ${trigger.instancia_id} pronta.\"\n  }\n}\n```\n\nAqui, o **null_resource** ter√° como _trigger_ o ID da inst√¢ncia EC2. Sempre que esse ID mudar (ou seja, a inst√¢ncia for recriada), o Terraform recria esse null_resource, acionando o provisioner. Isso garante que o comando rode ap√≥s a inst√¢ncia ser (re)criada. Essa t√©cnica √© √∫til para sequenciar provisioners de forma controlada. Contudo, para iniciantes, entender que isso existe √© suficiente.\n\n**Resumo:** Os **Provisioners** d√£o flexibilidade para executar a√ß√µes complementares √† cria√ß√£o de recursos:\n\n- **remote-exec** para configurar dentro de m√°quinas (via SSH/WinRM).\n- **local-exec** para executar coisas localmente (scripts, notifica√ß√µes, etc.). Eles devem ser usados com crit√©rio, pois introduzem considera√ß√µes de ordem de execu√ß√£o e poss√≠veis instabilidades. No entanto, s√£o muito poderosos em cen√°rios onde precisamos integrar Terraform com processos de configura√ß√£o.\n\nNo nosso exemplo, vimos como instalar automaticamente um servidor web em uma inst√¢ncia criada. Nos pr√≥ximos tutoriais, focaremos em como estruturar melhor nosso c√≥digo Terraform, primeiro com **M√≥dulos** para reuso e organiza√ß√£o, e depois explorando **Meta-argumentos** e **Fun√ß√µes/Express√µes** avan√ßadas da linguagem."
  },
  {
    "id": "d60862af-f596-493e-aa07-46d1ebbff335",
    "title": "M√≥dulos no Terraform",
    "description": "Ensinar a cria√ß√£o e utiliza√ß√£o de m√≥dulos no Terraform para modularizar a infraestrutura e melhorar a manuten√ß√£o do c√≥digo.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Fundamentos",
      "Modules",
      "AWS",
      "VPC"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/d60862af-f596-493e-aa07-46d1ebbff335",
    "markdown": "Conforme sua infraestrutura cresce, manter tudo em um √∫nico conjunto de arquivos pode ficar dif√≠cil. O Terraform permite **organizar e reutilizar c√≥digo** atrav√©s de **M√≥dulos**. Um m√≥dulo nada mais √© do que um conjunto de configura√ß√µes Terraform em uma pasta, que pode ser referenciado por outras configura√ß√µes. Qualquer pasta com arquivos **.tf** pode ser considerada um m√≥dulo (inclusive a pasta raiz do seu projeto, chamada de _root module_).\n\n**1. Por que usar m√≥dulos?**\n\n- **Reutiliza√ß√£o:** Escreva um m√≥dulo uma vez e use-o em v√°rios lugares. Por exemplo, voc√™ pode criar um m√≥dulo que representa um servidor web com certas configura√ß√µes (inst√¢ncia + seguran√ßa + attach de volume, etc.) e ent√£o reutiliz√°-lo para lan√ßar m√∫ltiplos servidores iguais em vez de duplicar c√≥digo.\n- **Organiza√ß√£o:** Separar a infraestrutura em componentes l√≥gicos. Por exemplo, um m√≥dulo para _networking_, outro para _servidores_, outro para _banco de dados_, facilita a manuten√ß√£o.\n- **Abstra√ß√£o:** Voc√™ pode encapsular detalhes complexos dentro de um m√≥dulo e expor apenas vari√°veis e outputs necess√°rios, simplificando o uso para outros (ou equipe).\n\nEm suma, m√≥dulos tornam a configura√ß√£o **compon√≠vel e reutiliz√°vel** ‚Äì princ√≠pios importantes conforme os projetos escalam.\n\n**2. Criando um m√≥dulo local:**\n\nVamos criar um m√≥dulo simples a partir do que j√° temos. Suponha que queiramos reutilizar a configura√ß√£o da inst√¢ncia EC2 (com suas vari√°veis) para criar v√°rias inst√¢ncias semelhantes. Podemos transformar nossa configura√ß√£o em um m√≥dulo gen√©rico de _inst√¢ncia EC2_.\n\n- Crie uma pasta chamada **modules/instancia_ec2**. Dentro dela, crie tr√™s arquivos: **main.tf**, **variables.tf** e **outputs.tf** (estrutura comum para m√≥dulos).\n\n**modules/instancia_ec2/main.tf:**\n\n```hcl\n# Recurso EC2 definido dentro do m√≥dulo\nresource \"aws_instance\" \"this\" {\n  ami           = var.ami_id\n  instance_type = var.instance_type\n  key_name      = var.key_name\n\n  tags = {\n    Name = var.instance_name\n  }\n}\n```\n\n**modules/instancia_ec2/variables.tf:**\n\n```hcl\nvariable \"ami_id\" {\n  description = \"ID da AMI da inst√¢ncia\"\n}\n\nvariable \"instance_type\" {\n  description = \"Tipo da inst√¢ncia\"\n  default     = \"t2.micro\"\n}\n\nvariable \"key_name\" {\n  description = \"Nome do par de chaves SSH\"\n}\n\nvariable \"instance_name\" {\n  description = \"Valor da tag Name da inst√¢ncia\"\n}\n```\n\n**modules/instancia_ec2/outputs.tf:**\n\n```hcl\noutput \"instance_id\" {\n  description = \"ID da inst√¢ncia criada\"\n  value       = aws_instance.this.id\n}\n\noutput \"public_ip\" {\n  description = \"IP p√∫blico da inst√¢ncia\"\n  value       = aws_instance.this.public_ip\n}\n```\n\nExplica√ß√£o:\n\n- Dentro do m√≥dulo, usamos o recurso **aws_instance.this** (optamos pelo nome interno \"this\" por conven√ß√£o, j√° que o m√≥dulo pode ser instanciado v√°rias vezes, assim cada inst√¢ncia do m√≥dulo ter√° seu pr√≥prio **aws_instance.this** separado).\n- Vari√°veis do m√≥dulo: definimos **ami_id**, **instance_type**, **key_name** e **instance_name** como entradas do m√≥dulo. Note que n√£o colocamos **default** para **ami_id**, **key_name** e **instance_name** ‚Äì isso os torna obrigat√≥rios, for√ßando quem chamar o m√≥dulo a fornecer esses valores (o tipo default any se nenhum type √© especificado, aqui inferimos string para estes).\n- Outputs do m√≥dulo: repassamos o ID e IP p√∫blico para quem usar o m√≥dulo, caso queira acessar.\n\nTemos ent√£o um m√≥dulo que cria 1 inst√¢ncia EC2 de acordo com os par√¢metros fornecidos.\n\n**3. Usando (chamando) um m√≥dulo:**\n\nNo arquivo principal do nosso projeto (por exemplo, **main.tf** na raiz), vamos **chamar o m√≥dulo** para criar inst√¢ncias. Podemos substituir nosso recurso expl√≠cito anterior por chamadas de m√≥dulo. Exemplo:\n\n```hcl\nmodule \"web1\" {\n  source          = \"./modules/instancia_ec2\"\n  ami_id          = \"ami-0fc5d935ebf8bc3bc\"\n  instance_type   = \"t2.micro\"\n  key_name        = \"terraform-key\"\n  instance_name   = \"WebServer-1\"\n}\n\nmodule \"web2\" {\n  source          = \"./modules/instancia_ec2\"\n  ami_id          = \"ami-0fc5d935ebf8bc3bc\"\n  instance_type   = \"t2.micro\"\n  key_name        = \"terraform-key\"\n  instance_name   = \"WebServer-2\"\n}\n```\n\nCada bloco **module \"<nome>\" { ... }** instanciar√° nosso m√≥dulo. Os atributos dentro do bloco correspondem √†s vari√°veis do m√≥dulo que declaramos:\n\n- **source** indica onde est√° o c√≥digo do m√≥dulo. No caso, usamos um path relativo apontando para a pasta do m√≥dulo local. Poderia ser um endere√ßo de reposit√≥rio Git, registro p√∫blico, etc., se fosse um m√≥dulo externo.\n- Fornecemos valores para as vari√°veis **ami_id**, **instance_type**, etc. (nesse caso usamos os mesmos para ambos, exceto o **instance_name** que diferenciamos para fins de identifica√ß√£o).\n\nAgora, quando rodarmos **terraform apply**, o Terraform vai entrar na pasta do m√≥dulo, ler o **main.tf** dele e criar os recursos definidos ali, para cada inst√¢ncia do m√≥dulo.\n\nTeremos ent√£o 2 inst√¢ncias EC2 criadas, com nomes \"WebServer-1\" e \"WebServer-2\". Os outputs definidos no m√≥dulo tamb√©m ficam dispon√≠veis. Voc√™ pode acess√°-los via **module.web1.public_ip** por exemplo, em um output no root module, ou via o comando output se voc√™ exp√¥s no root. Se quis√©ssemos, poder√≠amos no root module criar outputs para consolidar, ex:\n\n```hcl\noutput \"ips_dos_webs\" {\n  value = [ module.web1.public_ip, module.web2.public_ip ]\n}\n```\n\nAssim, ap√≥s o apply, ver√≠amos a lista de IPs das duas inst√¢ncias.\n\n**4. M√≥dulos do Terraform Registry (reutiliza√ß√£o compartilhada):**\n\nAl√©m de m√≥dulos locais, o Terraform possui um **Registry p√∫blico** (registro) onde a comunidade e a HashiCorp publicam m√≥dulos reutiliz√°veis. Em vez de escrever tudo do zero, voc√™ pode usar m√≥dulos prontos. Por exemplo, m√≥dulos populares:\n\n- **terraform-aws-vpc** que cria uma VPC completa (subnets, gateways, etc.).\n- **terraform-aws-ec2-instance** para inst√¢ncias com configura√ß√µes pr√©-determinadas.\n- Muitos outros para bancos de dados, redes, Kubernetes, etc.\n\nPara usar um m√≥dulo do registry, o **source** do m√≥dulo usa uma nota√ß√£o como **terraform-aws-modules/ec2-instance/aws** (nome do m√≥dulo e provedor). O Terraform baixar√° automaticamente o m√≥dulo do registry no init. Por exemplo:\n\n```hcl\nmodule \"servers\" {\n  source  = \"terraform-aws-modules/ec2-instance/aws\"\n  version = \"4.1.0\"  # √© bom pinar vers√£o\n\n  # Par√¢metros exigidos pelo m√≥dulo (conforme documenta√ß√£o do m√≥dulo)\n  name = \"MyServer\"\n  instance_count = 2\n  ami = \"ami-083654bd07b5da81d\"\n  instance_type = \"t2.micro\"\n  # ... e muitos outros poss√≠veis ...\n}\n```\n\nOs m√≥dulos oficiais costumam ter documenta√ß√£o no registry com exemplos de uso. Us√°-los pode poupar muito trabalho, mas certifique-se de entender o que fazem e adequar √†s suas necessidades.\n\n**5. Boas pr√°ticas de m√≥dulos:**\n\n- **Coes√£o:** um m√≥dulo deve ter uma responsabilidade clara. Por exemplo, n√£o misture no mesmo m√≥dulo recursos de rede e de inst√¢ncia. Mantenha com prop√≥sito espec√≠fico.\n- **Vari√°veis e Outputs bem definidos:** exponha o que faz sentido para reutiliza√ß√£o e mantenha detalhes internos encapsulados. Forne√ßa descri√ß√µes e padr√µes l√≥gicos.\n- **Vers√µes:** se publicar m√≥dulos (ou mesmo internamente), controle vers√µes para evitar quebrar setups existentes ao atualizar.\n- **Organiza√ß√£o de pastas:** comum termos uma estrutura de reposit√≥rio como:\n    \n    ```\n    /terraform\n      /modules\n        /modulo1/...\n        /modulo2/...\n      /envs\n        /dev/main.tf  (root module usando m√≥dulos)\n        /prod/main.tf ...\n    ```\n    \n    Ou seja, separar defini√ß√£o de m√≥dulos reutiliz√°veis e as configura√ß√µes espec√≠ficas de cada ambiente que consomem esses m√≥dulos.\n\nNo nosso caso, fizemos um m√≥dulo simples de inst√¢ncia EC2. Imagine que poder√≠amos t√™-lo parametrizado ainda mais (quantidade de discos, se executa provisioner ou n√£o, etc.) e ent√£o reutiliz√°-lo para diferentes tipos de servidor definindo vari√°veis diferentes.\n\n**Resumo:** M√≥dulos s√£o elementos chave para escrever **infraestrutura como c√≥digo** de forma DRY (_Don't Repeat Yourself_). Eles permitem estruturar o c√≥digo em unidades reutiliz√°veis e compartilhar configura√ß√µes entre projetos e equipes. Com m√≥dulos, podemos criar \"cat√°logos\" de componentes de infra (rede b√°sica, servidor padr√£o, cluster XYZ, etc.) que aceleram a composi√ß√£o de novos ambientes.\n\nNos pr√≥ximos tutoriais, vamos continuar aprimorando nosso uso do Terraform explorando os **Meta-argumentos** (par√¢metros especiais que recursos aceitam) e as **Fun√ß√µes & Express√µes** da linguagem para l√≥gica e transforma√ß√µes de valores."
  },
  {
    "id": "1d94e151-8ae0-4ab7-b402-313f531d7bd0",
    "title": "Meta-argumentos: count, for_each, depends_on e lifecycle",
    "description": "Explorar os meta argumentos do Terraform, como  count, for_each, depends_on e lifecycle, que permitem maior controle e l√≥gica na cria√ß√£o de recursos.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Fundamentos",
      "Meta_arguments",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/1d94e151-8ae0-4ab7-b402-313f531d7bd0",
    "markdown": "No Terraform, al√©m das propriedades normais de recursos (definidas pelo provedor), existem alguns argumentos especiais ‚Äì os **meta-argumentos** ‚Äì que podem ser usados em **qualquer recurso** (ou m√≥dulo) para controlar seu comportamento. Os principais meta-argumentos s√£o: **count**, **for_each**, **depends_on** e o bloco **lifecycle** (que inclui flags como **create_before_destroy**, **prevent_destroy**, etc.). Vamos entender cada um e ver exemplos.\n\n### count\n\nO meta-argumento **count** permite criar m√∫ltiplas inst√¢ncias de um recurso usando uma √∫nica declara√ß√£o. Ele espera um valor inteiro. Se **count = 3**, por exemplo, aquele recurso ser√° criado 3 vezes, e voc√™ poder√° referenci√°-los atrav√©s de √≠ndices (0,1,2).\n\n**Exemplo:** Suponha que queremos 3 inst√¢ncias EC2 id√™nticas (como fizemos chamando m√≥dulo duas vezes, mas poder√≠amos usar count tamb√©m). Podemos modificar nosso recurso (ou m√≥dulo) assim:\n\n```hcl\nresource \"aws_instance\" \"worker\" {\n  count         = 3\n  ami           = var.ami_id\n  instance_type = var.instance_type\n  key_name      = var.key_name\n\n  tags = {\n    Name = \"Worker-${count.index}\"\n  }\n}\n```\n\n- Definindo **count = 3**, o Terraform criar√° **aws_instance.worker[0]**, **aws_instance.worker[1]** e **aws_instance.worker[2]**. O **count.index** √© uma vari√°vel interna dispon√≠vel no contexto do recurso, indicando o √≠ndice atual (0,1,2) durante a cria√ß√£o de cada.\n- No exemplo, as inst√¢ncias receber√£o tags Name \"Worker-0\", \"Worker-1\" e \"Worker-2\" respectivamente gra√ßas ao **${count.index}**.\n- Para referenciar atributos de um recurso contado, usamos sintaxe de √≠ndice. Por exemplo, **aws_instance.worker[1].public_ip** seria o IP da segunda inst√¢ncia (√≠ndice 1).\n- **count** √© √∫til para recursos homog√™neos e quando a quantidade √© conhecida ou controlada por vari√°vel. Por exemplo, poder√≠amos ter uma vari√°vel **quantidade_workers** e usar **count = var.quantidade_workers**.\n\n### for_each\n\nEnquanto **count** trabalha com um n√∫mero e √≠ndices, o meta-argumento **for_each** permite iterar sobre uma cole√ß√£o (lista ou mapa) de valores, criando uma inst√¢ncia do recurso para cada elemento. Ele d√° mais controle sobre identificar cada inst√¢ncia por uma chave (ao inv√©s de apenas √≠ndices num√©ricos).\n\n**Exemplo:** Criar m√∫ltiplos buckets S3 com nomes definidos em lista:\n\n```hcl\nvariable \"nomes_buckets\" {\n  type = list(string)\n  default = [\"logs\", \"assets\", \"backup\"]\n}\n\nresource \"aws_s3_bucket\" \"bucket\" {\n  for_each = toset(var.nomes_buckets)\n  bucket   = each.value   # each.value ser√° um dos nomes da lista\n  acl      = \"private\"\n}\n```\n\n- Aqui, **for_each = toset(var.nomes_buckets)**. Usamos **toset(...)** para converter a lista em um conjunto (set) de valores √∫nicos adequados para o for_each. Dessa forma, **each.value** iterar√° sobre \"logs\", \"assets\", \"backup\".\n- O Terraform criar√° 3 buckets com esses nomes. Em vez de √≠ndices, a identifica√ß√£o ser√° pelas chaves do set (nesse caso as pr√≥prias strings dos nomes):\n    - **aws_s3_bucket.bucket[\"logs\"]**, **aws_s3_bucket.bucket[\"assets\"]**, etc., ser√£o as refer√™ncias.\n- Podemos acessar atributos por chave: ex: **aws_s3_bucket.bucket[\"assets\"].id** referenciaria o bucket com nome \"assets\".\n- Vantagem sobre count: se os nomes forem √∫nicos, o for_each permite adicionar/remover elementos da cole√ß√£o e o Terraform consegue identificar precisamente qual recurso corresponde a qual elemento, minimizando recria√ß√µes desnecess√°rias. Com count, remover um item no meio da lista poderia recriar outros por mudan√ßa de √≠ndice. Com for_each usando chaves, isso √© mais est√°vel.\n\n**Dica:** **for_each** tamb√©m aceita maps. Se us√°ssemos um map, **each.key** e **each.value** estariam dispon√≠veis dentro do recurso. Por exemplo, suponha um map de usu√°rios para grupos, voc√™ poderia iterar e usar key e value em diferentes campos.\n\nTanto **count** quanto **for_each** **n√£o podem ser usados simultaneamente** em um mesmo recurso. Voc√™ escolhe um ou outro de acordo com a necessidade de iterar.\n\n### depends_on\n\nO Terraform normalmente determina automaticamente as depend√™ncias entre recursos lendo refer√™ncias (por exemplo, se um recurso A usa um atributo de B, ent√£o A depende de B). Por√©m, √†s vezes √© preciso declarar depend√™ncias **explicitamente**, quando n√£o h√° refer√™ncia direta no c√≥digo mas queremos for√ßar ordem de cria√ß√£o.\n\nO meta-argumento **depends_on** aceita uma lista de recursos que este recurso depende. Isso garante que, ao aplicar, o Terraform crie esses recursos dependentes antes.\n\n**Exemplo t√≠pico:** Suponha que temos um recurso que executa um comando local que precisa que _dois_ recursos diferentes estejam prontos, mas ele n√£o referencia nenhum diretamente. Podemos usar **depends_on** para especificar ambos.\n\nOutro exemplo concreto:\n\n```hcl\nresource \"aws_instance\" \"app\" { ... }\n\nresource \"aws_eip\" \"app_eip\" {\n  vpc      = true\n  depends_on = [aws_instance.app]\n}\n```\n\nNesse caso, um Elastic IP **aws_eip.app_eip** que, digamos, √© associado via user-data ou script, talvez n√£o tenha liga√ß√£o expl√≠cita no Terraform com a inst√¢ncia. Mas usando depends_on, garantimos que a inst√¢ncia **app** seja criada antes de alocar o EIP (mesmo que a associa√ß√£o n√£o ocorra aqui via Terraform).\n\nEm geral, use **depends_on** quando o Terraform n√£o consegue inferir a ordem correto por conta pr√≥pria. Um exemplo de uso real dado anteriormente foi:\n\n```hcl\nresource \"aws_elb\" \"web_elb\" {\n  depends_on = [aws_instance.web]\n  instances  = [aws_instance.web.id]\n}\n```\n\nAqui o **depends_on** √© redundante porque referenciar **aws_instance.web.id** j√° estabelece a depend√™ncia. Mas ilustra a sintaxe: uma lista de recursos dos quais este depende.\n\n### lifecycle\n\nO bloco **lifecycle** dentro de um recurso permite ajustar o comportamento do ciclo de vida de cria√ß√£o/atualiza√ß√£o/remo√ß√£o de um recurso. Ele suporta algumas op√ß√µes:\n\n- **create_before_destroy**: Em casos de substitui√ß√£o (recrea√ß√£o) de um recurso, essa flag indica para **criar o novo antes de destruir o antigo**. Muito √∫til para evitar downtime em certos recursos. Por exemplo, se voc√™ estiver alterando um recurso que n√£o pode coexistir e por padr√£o o Terraform destru√≠ria primeiro e depois criaria, isso evitaria interrup√ß√£o.\n    \n- **prevent_destroy**: Protege o recurso contra destrui√ß√£o. Se algu√©m tentar **terraform destroy** ou remover o recurso da config, o Terraform acusar√° erro em vez de destru√≠-lo. Use isso para recursos cr√≠ticos que nunca devem ser deletados acidentalmente (por exemplo, banco de dados de produ√ß√£o). Ex:\n    \n    ```hcl\n    resource \"aws_s3_bucket\" \"dados_importantes\" {\n      lifecycle {\n        prevent_destroy = true\n      }\n      # ... demais config ...\n    }\n    ```\n    \n    Com isso, mesmo um **terraform destroy** geral falhar√° se tentar remover esse bucket, a n√£o ser que o flag seja removido ou sobrescrito com **-target** e **-force** muito intencionalmente.\n    \n- **ignore_changes**: Serve para ignorar mudan√ßas em alguns atributos de um recurso. √Äs vezes, algum campo pode mudar fora do Terraform e voc√™ quer evitar que o Terraform tente reverter. Ou h√° atributos que s√£o definidos automaticamente ap√≥s cria√ß√£o e se forem colocados na config podem causar recria√ß√µes desnecess√°rias. Com **ignore_changes**, o Terraform n√£o reagir√° a drift nesses atributos espec√≠ficos. Exemplo:\n    \n    ```hcl\n    resource \"aws_instance\" \"test\" {\n      # ... \n      lifecycle {\n        ignore_changes = [ tags[\"LastDeployed\"] ]\n      }\n    }\n    ```\n    \n    Se algum processo externo atualizar a tag LastDeployed na inst√¢ncia, o Terraform n√£o vai detect√°-la como mudan√ßa a ser corrigida.\n    \n- **replace_triggered_by**: (mais avan√ßado) lista de recursos ou atributos cuja altera√ß√£o deve for√ßar a substitui√ß√£o (destroy/create) deste recurso. Por exemplo, se um recurso A n√£o depende diretamente de B, mas se B mudar voc√™ quer recriar A, pode usar isso.\n    \n\n**Exemplo de create_before_destroy:** Suponha que temos um recurso AWS Launch Configuration que n√£o permite atualiza√ß√£o in-place e sempre requer recria√ß√£o. Poder√≠amos usar:\n\n```hcl\nresource \"aws_launch_configuration\" \"lc\" {\n  name_prefix = \"app-lc-\"\n  image_id    = \"ami-XYZ\"\n  instance_type = \"t3.small\"\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n```\n\nAssim, ao trocar **image_id**, o Terraform primeiro criaria a nova Launch Config antes de destruir a antiga. Isso evita problemas no Auto Scaling (que exige sempre uma LC ativa).\n\n**Recapitulando:**\n\n- **count**: m√∫ltiplos recursos indexados por n√∫mero.\n- **for_each**: m√∫ltiplos recursos indexados por chave (elementos de set/map).\n- **depends_on**: define depend√™ncias extras manualmente.\n- **lifecycle**: controle fino do processo de cria√ß√£o/remo√ß√£o:\n    - create_before_destroy, prevent_destroy, ignore_changes etc.\n\nUsar esses meta-argumentos efetivamente permite escrever configura√ß√µes mais din√¢micas e seguras. Por exemplo, a combina√ß√£o de **for_each** com estruturas de dados permite montar recursos de forma declarativa a partir de listas/objetos complexos; **count** e condicionais possibilitam habilitar/desabilitar recursos opcionalmente (√†s vezes vemos padr√µes como **count = var.enabled ? 1 : 0** para criar ou n√£o um recurso baseado em flag booleana).\n\n**Exemplo pr√°tico integrando l√≥gica condicional:** imagine que quis√©ssemos que a cria√ß√£o de um recurso fosse opcional:\n\n```hcl\nvariable \"criar_bucket_backup\" {\n  type    = bool\n  default = false\n}\n\nresource \"aws_s3_bucket\" \"backup\" {\n  count  = var.criar_bucket_backup ? 1 : 0\n  bucket = \"meu-backup-bucket-123\"\n}\n```\n\nAqui usamos um operador tern√°rio (express√£o condicional) **condicao ? valor_se_true : valor_se_false** para o count. Se **criar_bucket_backup** for true, count =1 (o recurso ser√° criado); se false, count =0 (nenhum recurso criado). Isso d√° toggle de cria√ß√£o.\n\nNo pr√≥ximo tutorial, vamos explorar mais dessas **express√µes e fun√ß√µes** dispon√≠veis no Terraform, como o tern√°rio que acabamos de usar e muitas fun√ß√µes built-in que ajudam a manipular strings, listas e outros valores."
  },
  {
    "id": "1ea403f4-0d4d-4b0f-9281-87d2cf00328b",
    "title": "Fun√ß√µes e Express√µes no Terraform",
    "description": "Demonstrar o uso das fun√ß√µes embutidas do Terraform e express√µes para manipular dados, realizar transforma√ß√µes e lidar com l√≥gicas condicionais.",
    "tool": "Terraform",
    "level": "iniciante",
    "tags": [
      "Terraform",
      "Fundamentos",
      "AWS",
      "Functions"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/1ea403f4-0d4d-4b0f-9281-87d2cf00328b",
    "markdown": "A linguagem do Terraform (HCL - HashiCorp Configuration Language) √© declarativa, mas oferece diversos recursos para inserir l√≥gica e transformar dados: **express√µes condicionais, interpola√ß√µes e uma biblioteca de fun√ß√µes built-in**. Isso permite que voc√™ calcule valores dinamicamente em seus arquivos **.tf**.\n\nVamos cobrir os principais elementos:\n\n### Interpola√ß√£o de express√µes\n\nJ√° vimos nos exemplos o uso de sintaxe como **\"${var.nome}\"** ou **\"${count.index}\"** dentro de strings, ou mesmo direto **var.instancia_tipo**. Em Terraform 0.12+, muitas vezes voc√™ pode usar a vari√°vel diretamente (sem **${}**) quando o contexto espera uma express√£o. Por exemplo, **ami = var.ami_id** √© equivalente a **ami = \"${var.ami_id}\"** (caso seja string). Dentro de strings, para concatenar vari√°veis e texto, voc√™ usa a sintaxe **${ ... }**.\n\nEx:\n\n```hcl\ntags = {\n  Name = \"${var.env}-${var.app}\" \n}\n```\n\nAqui, se **env = \"dev\"** e **app = \"loja\"**, o resultado ser√° Name = \"dev-loja\". Voc√™ tamb√©m pode usar a **fun√ß√£o** **format** para formata√ß√£o:\n\n```hcl\nName = format(\"%s-%s\", var.env, var.app)\n```\n\nteria o mesmo efeito, usando placeholder do estilo Go (onde **%s** insere strings).\n\n### Express√µes condicionais (tern√°rio)\n\nA express√£o condicional em Terraform usa o operador **? :** e tem a forma:\n\n```\ncondi√ß√£o ? resultado_se_true : resultado_se_false\n```\n\nPor exemplo, do tutorial anterior:\n\n```hcl\ncount = var.tipo == \"prod\" ? 3 : 1\n```\n\nIsso configuraria **count** para 3 se o tipo for \"prod\", caso contr√°rio 1. Voc√™ pode usar condicionais para definir valores de par√¢metros ou decidir quantidades. √ötil tamb√©m em outputs, por exemplo:\n\n```hcl\noutput \"url\" {\n  value = var.env == \"prod\" ? aws_alb.prod.dns_name : \"http://staging.example.local\"\n}\n```\n\nAssim, em produ√ß√£o daria o DNS real do load balancer, em outros ambientes talvez um endere√ßo placeholder.\n\nLembre que tanto o resultado true quanto false devem ser do **mesmo tipo** (ou convers√≠veis) porque o Terraform precisa determinar o tipo da express√£o.\n\n### Fun√ß√µes built-in\n\nO Terraform possui [diversas fun√ß√µes built-in](https://developer.hashicorp.com/terraform/language/functions) para operar com strings, n√∫meros, cole√ß√µes e estruturas. Vamos citar algumas das mais comuns por categoria:\n\n- **Strings:**\n    \n    - **lower(\"TXT\")** - \"txt\" (min√∫sculas)\n    - **upper(\"abc\")** - \"ABC\" (mai√∫sculas)\n    - **concat(\"a\",\"b\")** - \"ab\" (concatenar strings; embora normalmente interpola√ß√£o simples ou **format** sejam preferidos)\n    - **substr(\"abcdef\", 1, 3)** - \"bcd\" (substring da posi√ß√£o 1 com comprimento 3)\n    - **replace(\"filename.txt\", \".txt\", \".bak\")** - \"filename.bak\" (substitui√ß√£o simples)\n    - **file(\"path/to/file\")** - l√™ o conte√∫do de um arquivo local e retorna como string (usamos no provisioner).\n- **N√∫meros e math:**\n    \n    - **max(5, 3, 9)** -> 9 (maior)\n    - **min(5, 3, 9)** -> 3 (menor)\n    - Operadores aritm√©ticos diretos: voc√™ pode usar **a + b**, **a * b**, etc., dentro de express√µes tamb√©m, ex: **count = var.num_servers * 2**.\n- **Booleanos:**\n    \n    - L√≥gicos padr√£o: **&&** (AND), **||** (OR), e **!** (NOT) funcionam dentro de **${ }**.\n    - Ex: **var.env == \"prod\" && var.desplegar == true ? 1 : 0** combinas condi√ß√µes.\n    - **can** e **coalesce**: **coalesce(var.opt1, var.opt2, \"default\")** retorna o primeiro valor n√£o-nulo (bom para defaults entre vari√°veis); **can()** verifica se uma express√£o √© v√°lida sem lan√ßar erro.\n- **Listas e Maps:**\n    \n    - **length(list or string)** - retorna comprimento. Ex: **length(var.nomes)** retorna quantos nomes na lista.\n    - **element([\"a\",\"b\",\"c\"], 1)** - \"b\" (pega elemento pelo √≠ndice).\n    - **slice(list, startIndex, endIndex)** - extrai sub-lista.\n    - **tolist(...)** e **toset(...)** convertendo tipos.\n    - **keys(map)** - lista de chaves do map\n    - **values(map)** - lista de valores.\n    - **lookup(map, key, default)** -> obt√©m valor pela chave ou default se n√£o existir. Ex:\n        \n        ```hcl\n        variable \"amis\" {\n          type = map(string)\n          default = {\n            \"us-east-1\" = \"ami-aaaa\"\n            \"us-west-2\" = \"ami-bbbb\"\n          }\n        }\n        ami = lookup(var.amis, var.regiao, \"ami-default\")\n        ```\n        \n        Aqui escolhemos a AMI conforme a regi√£o, e se a regi√£o n√£o estiver no mapa, usamos \"ami-default\".\n    - **merge(map1, map2, ...)** - combina maps.\n    - **zipmap(keys_list, values_list)** - cria um map a partir de duas listas (um de keys, outra de valores correspondentes).\n- **Outras √∫teis:**\n    \n    - **terraform.workspace** n√£o √© bem fun√ß√£o mas uma vari√°vel global que diz o workspace atual (ambiente) se usar Terraform Workspaces.\n    - **cidrsubnet(base_cidr, new_prefix, subnet_index)** - calcula subnets de um CIDR base (muito √∫til em automa√ß√£o de rede).\n    - **base64encode(str)** / **base64decode(str)** para trabalhar com base64.\n    - **sha256(str)** para gerar hash (√†s vezes para digitar um material ou gerar senhas tempor√°rias).\n    - **templatefile(\"file.tpl\", { var1 = val1, ... })** - carrega um arquivo de template e insere as vari√°veis fornecidas (substitui placeholders). √ötil para criar arquivos de configura√ß√£o ou scripts dentro do Terraform e passar para provisioners.\n\n**Exemplos combinando fun√ß√µes:**\n\n- Gerar um nome √∫nico concatenando prefixo, ambiente e um sufixo calculado:\n    \n    ```hcl\n    locals {\n      prefix = \"myapp\"\n      env    = var.env  # ex: \"prod\"\n      rand   = substr(replace(uuid(), \"-\", \"\"), 0, 4)  # 4 hex chars aleat√≥rios\n    }\n    resource \"aws_s3_bucket\" \"unique\" {\n      bucket = \"${local.prefix}-${local.env}-${local.rand}\"\n      acl    = \"private\"\n    }\n    ```\n    \n    Aqui usamos **uuid()** (gera um UUID), tiramos os **-** com replace, e pegamos os 4 primeiros caracteres para um sufixo aleat√≥rio curto. Isso ajuda a garantir que o nome do bucket seja √∫nico (bucket names globais precisam ser √∫nicos).\n    \n- Uso de condicionais com lista:\n    \n    ```hcl\n    resource \"aws_security_group\" \"example\" {\n      # ... regras ingress/egress ...\n      tags = merge(\n        var.common_tags,\n        var.env == \"prod\" ? { \"Critical\" = \"Yes\" } : {}\n      )\n    }\n    ```\n    \n    Neste snippet hipot√©tico, usamos **merge** para unir tags comuns com uma tag adicional \"Critical\" somente se for ambiente de produ√ß√£o (caso contr√°rio, merge com um map vazio n√£o adiciona nada). Isso demonstra que a express√£o condicional pode resultar em um mapa vazio ou com conte√∫do, e **merge** combina com o outro mapa de tags.\n    \n\n**Looping dentro da configura√ß√£o:**\n\nAl√©m de **count** e **for_each**, h√° tamb√©m **express√µes for** para construir listas ou maps dinamicamente:\n\n```hcl\nlocals {\n  nomes_upper = [ for nome in var.nomes: upper(nome) ]\n}\n```\n\nIsso iteraria **var.nomes** (lista de strings) e produziria uma nova lista **nomes_upper** em que cada nome √© transformado para mai√∫sculas. As express√µes **for** suportam tamb√©m cl√°usula **if** para filtrar:\n\n```hcl\nlocals {\n  buckets_com_backup = { for k, v in var.bucket_configs : k => v if v.backup }\n}\n```\n\nAcima, sup√µe **var.bucket_configs** √© um map de configura√ß√µes de bucket, e filtramos apenas os que t√™m backup habilitado, criando um novo map s√≥ com eles.\n\nEssas constru√ß√µes avan√ßadas permitem manipular dados declarativamente dentro do HCL, sem precisar de um script externo.\n\n### Resumo e dicas finais:\n\n- **Fun√ß√µes built-in:** Terraform tem muitas; consulte a [documenta√ß√£o oficial de fun√ß√µes](https://developer.hashicorp.com/terraform/language/functions) para ver todas. Sempre que voc√™ se pegar pensando \"preciso transformar este valor de tal forma\", h√° boa chance de j√° existir uma fun√ß√£o (split, join, trim, formatdate, etc.).\n- **Limita√ß√µes de linguagem:** Lembre-se que o Terraform n√£o √© uma linguagem de programa√ß√£o imperativa completa. N√£o h√° loops tradicionais do tipo \"for i in 1..10\" (em vez disso usa-se **count**/**for_each** para repeti√ß√£o de recursos ou for expressions para listas/mapas). N√£o h√° if/else campos como num script, mas o condicional tern√°rio supre a maioria das l√≥gicas condicionais.\n- **Locals:** Utilizamos brevemente **locals**. Os **local values** s√£o vari√°veis locais dentro da configura√ß√£o para armazenar resultados de express√µes complexas ou simplificar repeti√ß√£o de valores. Eles n√£o aparecem no output nem s√£o inputs ‚Äì s√£o s√≥ para organiza√ß√£o interna mesmo.\n- **Exprimir l√≥gica com clareza:** Embora seja poss√≠vel aninhar muitas fun√ß√µes e express√µes em uma linha, pondere usar **locals** ou dividir passos para manter o c√≥digo leg√≠vel. Por exemplo, criar um **local** para um c√°lculo complexo e depois usar o local no recurso pode facilitar a leitura e manuten√ß√£o.\n\nCom isso conclu√≠mos nossa s√©rie de tutoriais de Terraform. üöÄ Voc√™ aprendeu a:\n\n- Preparar o ambiente e entender o b√°sico de IaC e Terraform.\n- Criar recursos na AWS com Terraform e usar os comandos fundamentais (**init/plan/apply/destroy**).\n- Usar vari√°veis e outputs para generalizar configura√ß√µes e obter informa√ß√µes.\n- Compreender o Terraform state e boas pr√°ticas de armazenamento/compartilhamento.\n- Utilizar provisioners para a√ß√µes p√≥s-provisionamento (embora com modera√ß√£o).\n- Modularizar sua infraestrutura com m√≥dulos reutiliz√°veis, inclusive usando m√≥dulos de fonte externa.\n- Aplicar meta-argumentos para criar m√∫ltiplos recursos, controlar depend√™ncias e o ciclo de vida de recursos.\n- Aproveitar fun√ß√µes e express√µes da linguagem para tornar as configura√ß√µes mais din√¢micas e poderosas.\n\nCom esses conhecimentos, voc√™ est√° bem equipado para come√ßar a construir infraestruturas mais complexas e robustas como c√≥digo. A documenta√ß√£o oficial do Terraform e dos provedores (como AWS) ser√° sua aliada para descobrir novos recursos e aprofundar-se em detalhes espec√≠ficos. Boa sorte na sua jornada com Terraform!"
  },
  {
    "id": "72a0de7a-ad07-4797-86d1-477e1581e0cc",
    "title": "Preparar o ferramental e ambiente",
    "description": "Configura√ß√£o do ambiente de desenvolvimento, incluindo cria√ß√£o da conta AWS, instala√ß√£o da AWS CLI, configura√ß√£o de credenciais e escolha de um editor de c√≥digo para escrever templates CloudFormation.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Cloudformation",
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/72a0de7a-ad07-4797-86d1-477e1581e0cc",
    "markdown": "Antes de come√ßar a usar AWS CloudFormation, precisamos preparar o ambiente de trabalho com as ferramentas necess√°rias. Vamos garantir que voc√™ tenha acesso √† AWS e as ferramentas instaladas para escrever e implantar templates do CloudFormation. Siga os passos abaixo:\n\n1. **Criar uma conta AWS:** Caso ainda n√£o tenha, acesse o site da AWS e crie uma conta. √â necess√°rio um cart√£o de cr√©dito para validar a conta, mas a AWS oferece um n√≠vel gratuito (_free tier_) que permite experimentar muitos servi√ßos sem custos por 12 meses. Ap√≥s criar a conta, fa√ßa login no [Console de Gerenciamento da AWS](https://aws.amazon.com/console/).\n    \n2. **Configurar credenciais de acesso:** Para usar o CloudFormation e outros servi√ßos, voc√™ precisar√° de credenciais de acesso (Access Key ID e Secret Access Key) de um usu√°rio AWS com permiss√µes adequadas. Uma pr√°tica comum √© criar um usu√°rio IAM espec√≠fico para voc√™ com permiss√µes de administrador ou permiss√µes restritas apenas ao CloudFormation e recursos necess√°rios. No console AWS, navegue at√© **IAM (Identity and Access Management)** > **Users (Usu√°rios)** e crie um usu√°rio se ainda n√£o tiver. Ao criar, marque a op√ß√£o de **Acesso program√°tico** para obter a Access Key ID e Secret. Guarde essas credenciais em local seguro.\n    \n3. **Instalar a AWS CLI:** A AWS Command Line Interface (CLI) √© uma ferramenta de linha de comando que permite interagir com os servi√ßos AWS, inclusive criar stacks do CloudFormation via terminal. Baixe e instale a AWS CLI v2 (dispon√≠vel para Windows, Linux e macOS) do site oficial da AWS. Ap√≥s a instala√ß√£o, verifique no terminal com `aws --version` se tudo foi instalado corretamente.\n    \n4. **Configurar a AWS CLI:** Execute `aws configure` no terminal para configurar suas credenciais e regi√£o padr√£o. Esse comando solicitar√° seu **AWS Access Key ID**, **Secret Access Key**, **Default region name** (regi√£o AWS padr√£o, por exemplo `us-east-1` para Norte Virg√≠nia) e **Default output format** (formato de sa√≠da, como `json`). Insira os valores quando solicitado. Este processo cria um _profile_ padr√£o com suas credenciais e configura√ß√µes. Por exemplo, ao rodar o comando voc√™ ver√° prompts interativos:\n    \n    ```plaintext\n    $ aws configure\n    AWS Access Key ID [None]: <SEU_ACCESS_KEY_ID>\n    AWS Secret Access Key [None]: <SEU_SECRET_ACCESS_KEY>\n    Default region name [None]: us-east-1\n    Default output format [None]: json\n    ```\n    \n    Isso definir√° a regi√£o padr√£o (neste caso, us-east-1) e o formato JSON para respostas. Voc√™ pode alterar essas configura√ß√µes depois editando os arquivos de configura√ß√£o (`~/.aws/credentials` e `~/.aws/config` no Linux/macOS ou `%UserProfile%\\.aws\\...` no Windows).\n    \n5. **Escolher uma regi√£o AWS:** Durante os tutoriais, usaremos uma regi√£o AWS para criar os recursos (por exemplo, **us-east-1** ‚Äì Norte da Virg√≠nia). Prefira regi√µes pr√≥ximas geograficamente ou onde sua conta tenha _quotas_ dispon√≠veis. Lembre-se de que os recursos do CloudFormation s√£o criados dentro da regi√£o selecionada. Voc√™ pode mudar a regi√£o nas configura√ß√µes da CLI (passo anterior) ou selecionando no menu superior direito do Console da AWS.\n    \n6. **Ferramenta para editar templates (opcional):** CloudFormation templates s√£o arquivos de texto (JSON ou YAML). Voc√™ pode usar qualquer editor de texto ou IDE para escrev√™-los. √â recomend√°vel um editor com realce de sintaxe para YAML/JSON e suporte a c√≥digo, como VS Code, Atom ou Sublime Text. A extens√£o **cfn-lint** (CloudFormation Linter) est√° dispon√≠vel para v√°rios editores, ajudando a validar templates e apontar erros de sintaxe ou op√ß√µes inv√°lidas enquanto voc√™ digita. O cfn-lint √© uma ferramenta de linha de comando que verifica templates CloudFormation e ajuda a identificar erros e m√°s pr√°ticas. Por exemplo, ele checa se voc√™ usou nomes de propriedades v√°lidos, tipos corretos e valores permitidos, garantindo que seu template siga as normas do CloudFormation antes mesmo de tentar executar. Embora opcional, utilizar o linter ou mesmo o comando `aws cloudformation validate-template` para valida√ß√£o b√°sica pode evitar frustra√ß√µes com erros de formata√ß√£o.\n    \n7. **Verificar o ambiente:** Com a CLI configurada, teste listando os buckets S3 ou stacks existentes. Por exemplo, execute `aws s3 ls` (que lista buckets S3) ou `aws cloudformation list-stacks` para verificar se voc√™ recebe uma resposta (mesmo que vazia, sem erros de autoriza√ß√£o). Se o comando executar com sucesso, seu ambiente est√° pronto. Em caso de erro de credenciais ou regi√£o, revise os passos anteriores.\n    \n\nCom esses passos, voc√™ ter√° uma conta AWS pronta e o ambiente configurado com as ferramentas necess√°rias. No pr√≥ximo tutorial, come√ßaremos a explorar o AWS CloudFormation em si."
  },
  {
    "id": "a5f7fbf7-3162-479d-a81c-04492f64c4d4",
    "title": "Introdu√ß√£o ao AWS Cloudformation",
    "description": "Explica√ß√£o dos conceitos fundamentais: templates, stacks e infraestrutura como c√≥digo, abordando os benef√≠cios de automa√ß√£o e orquestra√ß√£o de recursos na AWS.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Cloudformation",
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/a5f7fbf7-3162-479d-a81c-04492f64c4d4",
    "markdown": "Agora que o ambiente est√° pronto, vamos entender o que √© o AWS CloudFormation e por que us√°-lo. O CloudFormation √© um servi√ßo de **Infraestrutura como C√≥digo (IaC)** que permite modelar e provisionar recursos de nuvem AWS por meio de templates. Em vez de criar manualmente recursos pelo console, voc√™ define tudo em um arquivo de texto (template) e o CloudFormation automatiza a cria√ß√£o e configura√ß√£o seguindo aquele modelo. Isso traz diversos benef√≠cios:\n\n### O que √© e por que usar o CloudFormation?\n\n- **Infraestrutura como c√≥digo declarativo:** Com CloudFormation, voc√™ declara em um arquivo todos os recursos AWS desejados (como inst√¢ncias EC2, buckets S3, etc.) e suas configura√ß√µes. O servi√ßo ent√£o provisiona e configura esses recursos para voc√™, respeitando depend√™ncias e ordem automaticamente. Isso significa menos tempo clicando em interfaces e mais consist√™ncia ‚Äî o template descreve exatamente o estado da infraestrutura.\n    \n- **Gerenciamento de grupo de recursos como unidade √∫nica:** Recursos definidos juntos em um template formam uma **stack** (pilha). Voc√™ pode criar, atualizar ou deletar todos aqueles recursos como uma √∫nica unidade (a stack). Por exemplo, um aplicativo web pode ter EC2, Load Balancer e banco de dados; todos s√£o criados juntos e, se voc√™ deletar a stack, todos s√£o removidos juntos. O CloudFormation facilita tratar recursos relacionados como um grupo coeso para gerenciamento.\n    \n- **Reprodu√ß√£o r√°pida de ambientes:** Como os templates s√£o arquivos, voc√™ pode reutiliz√°-los para criar c√≥pias id√™nticas da infraestrutura em outras regi√µes ou contas. Se voc√™ precisa levantar a mesma aplica√ß√£o em _staging_ e _production_, basta rodar o mesmo template em ambas as regi√µes, garantindo consist√™ncia. Isso ajuda em escalabilidade multi-regi√£o e recupera√ß√£o de desastres ‚Äî o mesmo template pode criar o mesmo conjunto de recursos repetidamente de forma confi√°vel.\n    \n- **Controle de mudan√ßas e versionamento:** Templates sendo c√≥digo texto podem ser versionados (ex: usando Git). Altera√ß√µes na infraestrutura s√£o feitas editando-se o template e fazendo _deploy_ (atualiza√ß√£o da stack). Voc√™ pode rastrear no hist√≥rico de vers√µes o que mudou, quando e por quem, assim como faria com c√≥digo de software. Em caso de problema ap√≥s uma mudan√ßa, voc√™ pode reverter para um template anterior e reimplantar a infraestrutura est√°vel. Essa capacidade de auditar e reproduzir mudan√ßas aumenta a confiabilidade.\n    \n- **Automa√ß√£o de implanta√ß√µes e seguran√ßa:** CloudFormation se integra a fluxos de CI/CD, permitindo automatizar a implanta√ß√£o de infraestrutura juntamente com aplica√ß√µes. Al√©m disso, por padr√£o todos os recursos s√£o criados de forma segura e com as configura√ß√µes definidas ‚Äî reduzindo erros humanos de configura√ß√£o. Se algo falhar durante a cria√ß√£o ou atualiza√ß√£o, o CloudFormation pode reverter automaticamente (rollback) para o estado anterior, ajudando a manter a estabilidade.\n    \n\nEm resumo, o CloudFormation habilita pr√°ticas de DevOps na infraestrutura: automa√ß√£o, consist√™ncia, rapidez na cria√ß√£o de ambientes e melhor governan√ßa.\n\n### Conceitos b√°sicos: Templates e Stacks\n\nDois conceitos fundamentais no CloudFormation s√£o **template** e **stack**:\n\n- **Template:** √© o arquivo (JSON ou YAML) que define os recursos a serem criados e suas configura√ß√µes. Ele cont√©m se√ß√µes como Resources, Parameters, Outputs, etc. (vamos detalhar adiante). √â como o ‚Äúplano de constru√ß√£o‚Äù da infraestrutura desejada. Por exemplo, um template pode dizer \"criar uma inst√¢ncia t2.micro no Amazon EC2, um bucket no S3 e um t√≥pico no SNS\".\n    \n- **Stack (Pilha):** √© a inst√¢ncia de um template em execu√ß√£o. Quando voc√™ usa um template para criar recursos, o CloudFormation cria uma stack, que nada mais √© do que o conjunto real de recursos provisionados conforme o template. A stack mant√©m o rastreamento de todos esses recursos. Se mais tarde voc√™ precisar atualizar a infraestrutura, voc√™ atualiza a stack fornecendo um template modificado (CloudFormation calcular√° as diferen√ßas e aplicar√° as mudan√ßas). Cada stack tem um estado e um hist√≥rico de eventos das opera√ß√µes (cria√ß√£o, atualiza√ß√µes, etc). No console AWS, voc√™ ver√° as stacks listadas e pode inspecionar os recursos dentro de cada stack.\n    \n\nUma analogia simples: o template √© a receita, a stack √© o prato pronto feito a partir daquela receita. Voc√™ pode usar a mesma receita para fazer v√°rios pratos (p. ex., criar v√°rias stacks de um mesmo template).\n\n**Custo:** Utilizar o AWS CloudFormation em si n√£o tem custo adicional ‚Äî o servi√ßo √© gratuito. Voc√™ paga apenas pelos recursos AWS que forem criados pelo CloudFormation dentro da sua conta. Por exemplo, se seu template cria uma inst√¢ncia EC2 e um banco de dados RDS, voc√™ ser√° cobrado pelo EC2 e RDS nos pre√ßos normais, mas nada pelo CloudFormation orquestrar isso.\n\n### Estrutura de um template CloudFormation\n\nUm template CloudFormation √© um arquivo de texto estruturado em se√ß√µes. Cada **se√ß√£o** cumpre uma finalidade espec√≠fica dentro do template. Algumas das principais se√ß√µes s√£o:\n\n- **Resources (Recursos):** se√ß√£o **obrigat√≥ria** em todos os templates. √â o cora√ß√£o do template, onde voc√™ especifica cada recurso AWS que deseja criar e suas propriedades. Cada recurso recebe um nome l√≥gico √∫nico no template e um tipo (por exemplo, AWS::EC2::Instance) que corresponde a um servi√ßo AWS. Nesta se√ß√£o voc√™ descreve detalhes como tamanho de uma inst√¢ncia, nome de um bucket, etc.\n    \n- **Parameters (Par√¢metros):** se√ß√£o opcional onde voc√™ pode declarar entradas personaliz√°veis para o template. Par√¢metros permitem passar valores na hora de criar/atualizar a stack, tornando o template reutiliz√°vel em diferentes contextos. Por exemplo, voc√™ pode ter um par√¢metro para o tipo da inst√¢ncia EC2, em vez de fixar no template. Assim o mesmo template pode criar um servidor t2.micro em dev ou m5.large em produ√ß√£o, conforme o par√¢metro. Durante a cria√ß√£o da stack, o CloudFormation pede os valores desses par√¢metros (ou usa valores padr√£o se definidos).\n    \n- **Outputs (Sa√≠das):** tamb√©m opcional, define valores de sa√≠da que o CloudFormation retorna ap√≥s criar a stack. Outputs s√£o usados para destacar informa√ß√µes importantes dos recursos criados, como o URL de um site, o ID de uma inst√¢ncia ou nomes gerados. Essas sa√≠das aparecem no console (aba Outputs da stack) e tamb√©m podem ser referenciadas por outras stacks (cross-stack reference) se voc√™ export√°-las, permitindo compartilhar recursos entre stacks.\n    \n- **Mappings (Mapeamentos):** se√ß√£o opcional que funciona como uma tabela de consulta est√°tica no template. Voc√™ pode mapear uma chave para diferentes valores dependendo de alguma condi√ß√£o predefinida, como regi√£o AWS ou ambiente. Por exemplo, um mapping poderia mapear nomes de regi√£o para IDs de AMI espec√≠ficos daquela regi√£o. Usando a fun√ß√£o intr√≠nseca `Fn::FindInMap`, seu template pode buscar o valor correto no mapping conforme a regi√£o atual, sem precisar de v√°rios par√¢metros ou l√≥gica complexa.\n    \n- **Conditions (Condi√ß√µes):** se√ß√£o opcional para declarar condi√ß√µes l√≥gicas que controlam a cria√ß√£o de recursos ou atribui√ß√£o de propriedades. Voc√™ pode decidir criar ou n√£o certos recursos com base em valores de par√¢metros ou pseudo-par√¢metros. Por exemplo, criar duas inst√¢ncias EC2 somente se o par√¢metro _Environment_ for \"prod\", caso contr√°rio criar apenas uma em dev. As condi√ß√µes permitem usar um √∫nico template para cen√°rios diferentes (produ√ß√£o vs teste) sem modifica-lo, ativando/desativando partes conforme necess√°rio.\n    \n\nAl√©m dessas, existem se√ß√µes menos usadas como **Metadata** (metadados do template), **Description** (descri√ß√£o do template), **Rules** (para validar combinac√µes de par√¢metros) e **Transform** (para macros e inclus√£o de macros como AWS::Serverless transform). N√£o entraremos em detalhes sobre elas neste momento, pois n√£o ser√£o foco em nossos tutoriais iniciais.\n\nEm resumo, **todo template tem, no m√≠nimo, a se√ß√£o Resources**. As outras se√ß√µes enriquecem o template com flexibilidade (Parameters, Conditions), sa√≠da de informa√ß√µes √∫teis (Outputs) e l√≥gica auxiliar (Mappings). Nos pr√≥ximos tutoriais, exploraremos cada uma dessas se√ß√µes com exemplos pr√°ticos.\n\n### Criando sua primeira stack com CloudFormation\n\nVamos agora praticar criando um recurso simples via CloudFormation para ver o processo completo. Faremos um exemplo m√≠nimo: criar um bucket S3 usando um template b√°sico. A ideia √© entender a din√¢mica de escrever um template e execut√°-lo.\n\n**Exemplo 1: Template YAML para criar um Bucket S3**\n\nAbaixo est√° um pequeno template em YAML. Ele cont√©m apenas a se√ß√£o **Resources**, definindo um recurso do tipo S3 Bucket. H√° coment√°rios explicando cada parte do template:\n\n```yaml\n# Template CloudFormation de exemplo - Cria um bucket Amazon S3\nAWSTemplateFormatVersion: '2010-09-09'  # Vers√£o do formato do template (opcional, boa pr√°tica incluir)\nDescription: \"Exemplo de template criando um bucket S3\"  # Descri√ß√£o do template (opcional)\n\nResources:\n  MeuBucket:\n    Type: AWS::S3::Bucket  # Especifica o tipo de recurso AWS (Bucket S3)\n    Properties:\n      # Nenhuma propriedade espec√≠fica definida.\n      # Sem nome fornecido, o CloudFormation gera um nome √∫nico automaticamente.\n```\n\n**Explica√ß√£o:** No template acima, definimos um recurso chamado `MeuBucket` do tipo `AWS::S3::Bucket`. N√£o especificamos nenhuma propriedade, ent√£o o bucket ser√° criado com configura√ß√µes padr√£o. Note que n√£o definimos um nome para o bucket ‚Äì quando n√£o fornecido, o CloudFormation ir√° gerar um nome √∫nico automaticamente para evitar conflitos de nomes globais em S3. Inclu√≠mos tamb√©m campos opcionais no topo: `AWSTemplateFormatVersion` (que indica a vers√£o do esquema de template, aqui usamos a mais comum `2010-09-09`) e `Description` para documentar o que o template faz.\n\n**Criando a stack na AWS:** Agora que temos o template, vamos criar a stack:\n\n- **Via Console AWS:** Entre no Console AWS e v√° para o servi√ßo _CloudFormation_. Clique em **Create stack (Criar pilha)** e escolha **\"With new resources (com novos recursos)\"**. Na se√ß√£o _Specify template (Especificar template)_, selecione o arquivo YAML do template acima (salve-o como, por exemplo, `bucket.yaml` no seu computador). Prossiga para a p√°gina seguinte, d√™ um nome para a stack (ex: \"StackExemploBucket\"), n√£o h√° par√¢metros neste template para preencher. Na pr√≥xima etapa, voc√™ pode deixar as op√ß√µes padr√£o. Avance at√© **Review** e ent√£o clique em **Create stack (Criar pilha)**. A stack come√ßar√° a ser criada. Voc√™ pode ir acompanhando na coluna de status at√© aparecer **CREATE_COMPLETE**, indicando sucesso.\n    \n- **Via AWS CLI:** Como alternativa, voc√™ pode usar a CLI para criar a stack, j√° que configuramos as credenciais. Por exemplo, rode:\n    \n    ```shell\n    aws cloudformation create-stack --stack-name StackExemploBucket \\\n      --template-body file://bucket.yaml\n    ```\n    \n    Isso enviar√° o template para o CloudFormation criar a stack chamada \"StackExemploBucket\". Voc√™ pode adicionar `--region <regiao>` se n√£o quiser usar a regi√£o padr√£o configurada. Use `aws cloudformation describe-stacks --stack-name StackExemploBucket` para acompanhar o status ou cheque no Console AWS.\n    \n\nAp√≥s a cria√ß√£o, v√° ao console S3 e verifique se um novo bucket apareceu. Ele ter√° um nome gerado automaticamente (algo como `stackexemplobucket-meubucket-1a2b3c4d`). Essa √© a **Physical ID** do recurso, gerada pelo CloudFormation, distinta do nome l√≥gico `MeuBucket` que usamos no template. Essa distin√ß√£o √© importante: o nome l√≥gico identifica o recurso dentro do template/stack para refer√™ncias e depend√™ncias, enquanto o nome f√≠sico √© o identificador real no servi√ßo AWS. Voc√™ pode ver ambos no console do CloudFormation, na aba **Resources** dentro da stack, listando o recurso com seu Logical ID e Physical ID.\n\nParab√©ns, voc√™ criou sua primeira infraestrutura atrav√©s de um template CloudFormation! üéâ Vimos como um template simples se traduz em um recurso real na nuvem. Nos pr√≥ximos tutoriais, construiremos sobre esses conceitos, adicionando par√¢metros, outputs e outros recursos ao template para explorar mais funcionalidades do CloudFormation."
  },
  {
    "id": "25e43b39-c9d0-4fda-b38a-5d79d76b7825",
    "title": "Parametriza√ß√£o",
    "description": "Uso da se√ß√£o Parameters para criar templates flex√≠veis e reutiliz√°veis, permitindo configurar valores como tipo de inst√¢ncia EC2, nomes de recursos e configura√ß√µes sem editar o c√≥digo.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Ambiente",
      "Fundamentos"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/25e43b39-c9d0-4fda-b38a-5d79d76b7825",
    "markdown": "Nesta se√ß√£o, vamos abordar a **Parametriza√ß√£o** em CloudFormation, ou seja, como tornar templates mais din√¢micos e reutiliz√°veis usando **Parameters**. Par√¢metros permitem que voc√™ personalize a inst√¢ncia de uma stack sem alterar o c√≥digo do template ‚Äì valores diferentes podem ser passados em cada cria√ß√£o/atualiza√ß√£o da stack. Isso √© essencial para n√£o precisar duplicar templates para cen√°rios diferentes.\n\n### Por que usar par√¢metros?\n\nSem par√¢metros, todos os valores dentro de um template s√£o fixos. Par√¢metros introduzem flexibilidade. Com eles, voc√™ consegue:\n\n- Reutilizar o mesmo template em contextos diferentes passando valores customizados. Por exemplo, um √∫nico template pode servir para dev e produ√ß√£o, bastando fornecer par√¢metros distintos (tamanho de servidor, quantidade de n√≥s, etc.) em cada ambiente.\n- **Evitar duplica√ß√£o de templates:** sem par√¢metros, voc√™ talvez mantivesse v√°rios templates quase id√™nticos diferindo apenas em configura√ß√µes menores (por exemplo, um template para cada regi√£o ou para cada tamanho de VM). Com par√¢metros, um template generalizado cobre todos os casos.\n- **Facilitar implanta√ß√µes automatizadas:** em pipelines CI/CD, voc√™ pode definir os valores dos par√¢metros para cada deploy (por exemplo, usar uma inst√¢ncia menor nos testes e maior em produ√ß√£o), sem editar o template em si.\n- **Compartilhar templates publicamente ou internamente:** outras pessoas podem usar seu template passando valores apropriados. Por exemplo, um template publicado pela AWS no GitHub pode pedir como par√¢metro um **KeyName** para acesso SSH, ao inv√©s de ter um valor fixo que n√£o funcionaria para todos.\n\nEm resumo, par√¢metros tornam o template **personaliz√°vel e adapt√°vel** a m√∫ltiplos cen√°rios, aumentando sua vida √∫til e abrang√™ncia.\n\n### Definindo par√¢metros no template\n\nPar√¢metros s√£o definidos em uma se√ß√£o **Parameters** no template. Cada par√¢metro tem um nome l√≥gico (ID), um tipo e opcionalmente metadados como descri√ß√£o e valores padr√£o ou permitidos. A sintaxe geral em YAML √© assim:\n\n```yaml\nParameters:\n  NomeDoParametro:\n    Type: DataType\n    Description: Texto explicativo (opcional)\n    Default: valor_padrao (opcional)\n    AllowedValues: [opcional lista de valores permitidos]\n    AllowedPattern: <regex opcional para validar formato>\n    MinLength:  ... (opcionais para Strings)\n    MaxLength:  ... \n    MinValue:   ... (opcionais para Numbers)\n    MaxValue:   ... \n    ConstraintDescription: \"Mensagem de erro personalizada se violar constraints\"\n```\n\nO √∫nico campo obrigat√≥rio para cada par√¢metro √© **Type**, que indica o tipo de dado esperado. Os tipos b√°sicos s√£o **String** (cadeia de caracteres), **Number** (n√∫mero) e **List<type>** (lista de valores separados por v√≠rgula). Al√©m desses, o CloudFormation define **AWS-specific parameter types** ‚Äì tipos especiais que correspondem a identificadores de recursos AWS existentes (veremos a seguir).\n\nVamos aplicar isso em nosso exemplo pr√°tico evoluindo o template anterior. Suponha que queremos permitir definir o **nome do bucket S3** em vez de aceitar um nome gerado automaticamente. Podemos introduzir um par√¢metro para o nome do bucket. Al√©m disso, vamos tamb√©m parametrizar o tipo da inst√¢ncia EC2 (que adicionaremos no pr√≥ximo tutorial), para j√° praticar com m√∫ltiplos par√¢metros.\n\n**Exemplo 2: Adicionando par√¢metros**\n\nVamos expandir o template do bucket para incluir par√¢metros de _BucketName_ e _InstanceType_, e ent√£o usar um deles no recurso. O trecho abaixo mostra a se√ß√£o de Parameters e como referenciar o par√¢metro no recurso:\n\n```yaml\nParameters:\n  BucketName:\n    Description: \"Nome √∫nico para o bucket S3 (deve ser globalmente √∫nico)\"\n    Type: String\n    MinLength: 3\n    MaxLength: 63\n    AllowedPattern: \"^[a-z0-9-]+$\"\n    ConstraintDescription: \"O nome pode conter letras min√∫sculas, n√∫meros e hifens.\"\n    # Nenhum Default definido => tornar√° obrigat√≥rio fornecer um valor na cria√ß√£o da stack.\n\n  InstanceType:\n    Description: \"Tipo da inst√¢ncia EC2\"\n    Type: String\n    Default: t2.micro\n    AllowedValues:\n      - t2.micro\n      - t3.micro\n      - t3.small\n    ConstraintDescription: \"Deve ser um tipo de inst√¢ncia v√°lido e dispon√≠vel.\"\n\nResources:\n  MeuBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: !Ref BucketName  # Usa o valor fornecido pelo par√¢metro como nome do bucket\n```\n\n**Explica√ß√£o:** Definimos dois par√¢metros:\n\n- **BucketName** do tipo String, sem valor default (logo, obrigat√≥rio). Inclu√≠mos algumas restri√ß√µes: tamanho m√≠nimo 3 e m√°ximo 63 (limites dos nomes de bucket S3) e um padr√£o regex que exige apenas letras min√∫sculas, d√≠gitos e hifens (padr√£o de nomes S3). A propriedade **ConstraintDescription** fornece uma mensagem amig√°vel caso o usu√°rio insira um valor inv√°lido. Esse par√¢metro ser√° usado para dar nome ao bucket S3. **Aten√ß√£o:** nomes de bucket S3 s√£o globais, ent√£o o valor passado **deve ser √∫nico em toda AWS** (n√£o apenas na sua conta). Escolha um nome pouco prov√°vel (por exemplo, incluindo seu nome ou algo exclusivo) para evitar conflitos.\n    \n- **InstanceType** do tipo String tamb√©m, mas aqui colocamos um Default (**t2.micro**) e uma lista de AllowedValues. Isso significa que se o usu√°rio n√£o especificar nada, o tipo de inst√¢ncia ser√° **t2.micro**. Se quiser customizar, s√≥ poder√° escolher entre **t2.micro**, **t3.micro** ou **t3.small** no nosso exemplo. Usamos poucos valores apenas para demonstra√ß√£o; na pr√°tica, voc√™ n√£o precisaria listar todos os tipos poss√≠veis, poderia permitir qualquer valor ou usar um tipo espec√≠fico da AWS (veremos adiante). A ideia √© mostrar como restringir op√ß√µes e definir padr√£o.\n    \n\nNo recurso **MeuBucket** (um S3 Bucket), adicionamos a propriedade **BucketName** e usamos **!Ref BucketName**. **!Ref** (abrevia√ß√£o de fun√ß√£o intr√≠nseca Ref) retorna o valor do par√¢metro _BucketName_ fornecido pelo usu√°rio. Assim, o nome do bucket criado ser√° exatamente o valor passado para o par√¢metro na hora da cria√ß√£o da stack.\n\n> **Dica:** A fun√ß√£o intr√≠nseca **Ref** serve para referenciar o valor de um par√¢metro ou o identificador de um recurso. No caso de par√¢metros, **Ref** retorna o valor inserido. No caso de recursos, **Ref** retorna o ID f√≠sico prim√°rio do recurso (por exemplo, para AWS::S3::Bucket retornaria o nome do bucket; para AWS::EC2::Instance, retorna o Instance ID). Usaremos **Ref** e outras fun√ß√µes intr√≠nsecas conforme avan√ßamos nos exemplos.\n\n**Criando a stack com par√¢metros:** Ao criar ou atualizar uma stack com esse template, o CloudFormation ir√° solicitar valores para **BucketName** (j√° que n√£o tem default) e **InstanceType** (opcional, tem default). No Console AWS, ao chegar na tela \"Specify stack details (Especificar detalhes da pilha)\", voc√™ ver√° campos para inserir cada par√¢metro com suas descri√ß√µes. No caso do **InstanceType**, ter√° at√© um menu suspenso com as op√ß√µes permitidas (t2.micro, t3.micro, etc), j√° que definimos AllowedValues. Isso facilita a sele√ß√£o e previne erros de digita√ß√£o. Para **BucketName**, voc√™ ter√° um campo livre, mas se digitar algo fora do padr√£o (como letras mai√∫sculas), o CloudFormation mostrar√° um erro com a **ConstraintDescription** fornecida quando tentar criar.\n\nSe usar a CLI, voc√™ deve passar os par√¢metros via linha de comando. Exemplo usando o AWS CLI:\n\n```shell\naws cloudformation create-stack --stack-name StackComParams \\\n  --template-body file://template-com-parametros.yaml \\\n  --parameters ParameterKey=BucketName,ParameterValue=meu-bucket-unico-123 \\\n               ParameterKey=InstanceType,ParameterValue=t3.micro\n```\n\nNo comando acima, passamos dois **ParameterKey** com seus **ParameterValue** correspondentes. A CLI ent√£o envia esses valores para o CloudFormation usar durante a cria√ß√£o.\n\n**AWS-Specific Parameter Types:** No exemplo usamos par√¢metros do tipo **String**. Por√©m, o CloudFormation disponibiliza tipos especiais para facilitar a entrada de recursos existentes. Por exemplo, ao inv√©s de pedir um texto livre para o nome de uma Key Pair para acesso SSH, voc√™ pode declarar o par√¢metro com Type **AWS::EC2::KeyPair::KeyName**. Assim, ao criar a stack, o console AWS **listar√Å automaticamente** as Key Pairs v√°lidas da sua conta para sele√ß√£o, evitando erros de digita√ß√£o e garantindo que o valor existe. Existem v√°rios AWS::... Parameter Types, como VPC IDs (**AWS::EC2::VPC::Id**), Subnet IDs, Security Group IDs, etc. Esses tipos ajudam a capturar _IDs de recursos existentes_ de forma amig√°vel: o console preenche as op√ß√µes e valida que o ID existe na conta/regi√£o. No nosso caso, poder√≠amos definir **InstanceType** como **AWS::EC2::InstanceType** (se existisse este tipo) para listar automaticamente tipos dispon√≠veis, mas esse tipo espec√≠fico n√£o existe. Em vez disso, usamos AllowedValues manual. Fique ciente dessa funcionalidade ‚Äì por exemplo, quando fizermos um par√¢metro de KeyName, usaremos **AWS::EC2::KeyPair::KeyName** para ajudar o usu√°rio.\n\nAgora temos um template parametrizado. Se implantarmos este template (certificando-se de fornecer um BucketName √∫nico), o bucket ter√° o nome escolhido e teremos flexibilidade para escolher outros tipos de inst√¢ncia no futuro (a m√°quina EC2 em si adicionaremos no tutorial de **Recursos** a seguir).\n\n**Recapitulando:**\n\n- Declaramos par√¢metros em **Parameters** com Type e outras restri√ß√µes.\n- **Referenciamos** par√¢metros dentro de **Resources** (ou Outputs, etc.) usando **!Ref NomeDoParametro**.\n- Par√¢metros permitem que quem cria a stack insira valores, tornando o template adapt√°vel.\n- Evite usar par√¢metros para informa√ß√µes sens√≠veis (como senhas) sem necessidade absoluta, pois eles podem aparecer em logs ou outputs; para senhas, o ideal √© usar servi√ßos como AWS Secrets Manager ou Parameter Store com tipos seguros.\n\nNos pr√≥ximos tutoriais, continuaremos a evoluir nosso template ‚Äì vamos adicionar mais **Recursos** e ver como utilizar esses par√¢metros e novos recursos na pr√°tica."
  },
  {
    "id": "70600cf9-b332-41c9-a2ed-eeb74a7ab512",
    "title": "Recursos",
    "description": "Estrutura da se√ß√£o Resources, onde os recursos AWS s√£o declarados. Aprendemos a definir inst√¢ncias EC2, buckets S3, bancos de dados e a referenci√°-los corretamente dentro do template.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/70600cf9-b332-41c9-a2ed-eeb74a7ab512",
    "markdown": "A se√ß√£o **Resources** √© o n√∫cleo de qualquer template CloudFormation. √â onde declaramos os **recursos AWS** que queremos que o CloudFormation crie e gerencie. Cada recurso √© definido por um tipo (por exemplo, **AWS::S3::Bucket** para um bucket S3, **AWS::EC2::Instance** para uma inst√¢ncia EC2) e um conjunto de propriedades espec√≠ficas daquele recurso. Nesta parte, vamos aprofundar na sintaxe de recursos, adicionar novos recursos ao nosso exemplo e entender como eles interagem.\n\n### Entendendo a defini√ß√£o de recursos\n\nCada recurso em um template possui tr√™s atributos principais:\n\n- **Logical ID (Nome l√≥gico):** √â o identificador usado dentro do template para referenciar o recurso. Pode ser qualquer string alfanum√©rica sem espa√ßos (diferente de todos os outros recursos). Por exemplo, usamos **MeuBucket** e poderemos usar **MinhaInstancia**. Esse nome n√£o √© enviado para a AWS; ele serve apenas no escopo do template/stack.\n    \n- **Type (Tipo de recurso):** Especifica o servi√ßo e tipo do recurso que queremos criar. A sintaxe √© sempre **AWS::<Produto>::<Tipo>**. Por exemplo: **AWS::EC2::Instance**, **AWS::S3::Bucket**, **AWS::DynamoDB::Table**. A AWS fornece [documenta√ß√£o de refer√™ncia](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html) para cada tipo, listando quais propriedades s√£o suportadas e quais s√£o obrigat√≥rias.\n    \n- **Properties (Propriedades):** S√£o os par√¢metros espec√≠ficos daquele recurso que voc√™ pode configurar. Variam conforme o Type. Por exemplo, uma inst√¢ncia EC2 possui propriedades como InstanceType, ImageId, KeyName, SecurityGroups, etc., enquanto um bucket S3 tem propriedades como BucketName, AccessControl, VersioningConfiguration. Na defini√ß√£o do recurso, voc√™ lista as propriedades relevantes como chaves YAML aninhadas sob o recurso.\n    \n\nAl√©m disso, recursos podem ter metadados adicionais ou atributos substitutos:\n\n- **DependsOn:** um campo opcional que voc√™ pode usar para for√ßar a cria√ß√£o de um recurso ap√≥s outro, caso o CloudFormation n√£o consiga deduzir automaticamente a depend√™ncia. Geralmente, se um recurso refere outro (via **Ref** ou outras fun√ß√µes), a depend√™ncia √© impl√≠cita e voc√™ n√£o precisa usar DependsOn.\n- **DeletionPolicy:** uma propriedade especial que n√£o faz parte das propriedades \"normais\" do recurso, mas voc√™ pode adicionar para controlar o que acontece ao **deletar a stack**. Ex: **DeletionPolicy: Retain** em um bucket S3 impede que ele seja deletado automaticamente junto com a stack (√∫til para preservar dados).\n- **UpdatePolicy/UpdateReplacePolicy:** controlam comportamentos em atualiza√ß√µes, dependendo do recurso.\n\nVamos manter nosso foco no essencial: definir recursos com propriedades b√°sicas.\n\n### Adicionando uma inst√¢ncia EC2 ao template\n\nNo template em constru√ß√£o, j√° temos um recurso S3 Bucket (**MeuBucket**). Agora, vamos adicionar uma inst√¢ncia EC2 para simular uma pequena aplica√ß√£o web, por exemplo. Nossa inst√¢ncia EC2 ter√°:\n\n- Um tipo configur√°vel (usaremos o par√¢metro **InstanceType** j√° definido).\n- Uma AMI (imagem de m√°quina) espec√≠fica para a regi√£o. Usaremos, por enquanto, um ID de AMI est√°tico v√°lido para nossa regi√£o (N. Virg√≠nia, us-east-1) e posteriormente melhoraremos isso com Mappings.\n- Sem par de chaves (KeyName) e security groups customizados por enquanto, para simplificar (a inst√¢ncia ficar√° no default VPC e default security group). \n\n**Observa√ß√£o:** Sem um KeyName, n√£o ser√° poss√≠vel acessar via SSH essa inst√¢ncia, mas nosso prop√≥sito aqui √© apenas demonstrar a cria√ß√£o. Se quiser acessar a inst√¢ncia, voc√™ pode criar um Key Pair na AWS e adicionar uma propriedade KeyName referenciando-o via par√¢metro.\n\nVamos incluir o recurso EC2 no template. Segue o trecho relevante:\n\n```yaml\nResources:\n  MeuBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: !Ref BucketName\n\n  MinhaInstancia:\n    Type: AWS::EC2::Instance\n    Properties:\n      InstanceType: !Ref InstanceType  # utiliza o par√¢metro InstanceType para o tamanho da inst√¢ncia\n      ImageId: ami-0ff8a91507f77f867   # ID da AMI Amazon Linux (HVM 64 bits) para us-east-1\n      # KeyName: !Ref KeyNameParam    # (opcional) se desejar SSH, adicione um par√¢metro de KeyName e referencia aqui\n```\n\n**Explica√ß√£o:** Adicionamos um recurso l√≥gico chamado **MinhaInstancia** do tipo **AWS::EC2::Instance**. Nas propriedades, definimos:\n\n- **InstanceType** como **!Ref InstanceType** para usar o valor do par√¢metro que j√° configuramos (por padr√£o, t2.micro, ou outro se especificado). Isso torna f√°cil mudar o tamanho da VM.\n    \n- **ImageId** com um valor fixo **ami-0ff8a91507f77f867**. Este √© o ID de uma Amazon Machine Image para Amazon Linux 2 (HVM, x86_64) na regi√£o **us-east-1**. **Importante:** AMI IDs variam por regi√£o. Essa AMI exata existe em us-east-1; em outras regi√µes, o ID ser√° diferente. Ou seja, se voc√™ tentar usar este template em **sa-east-1** (S√£o Paulo) ou **eu-west-3** (Paris), por exemplo, ele falhar√° dizendo que a AMI n√£o foi encontrada. Por enquanto, assumiremos que estamos implantando em us-east-1. No **Tutorial 6 (Mapeamentos)** vamos solucionar esse problema permitindo que o template escolha a AMI correta conforme a regi√£o automaticamente, mas aqui mantemos fixo para simplicidade.\n    \n- N√£o definimos **KeyName** ou **SecurityGroups** explicitamente. Sem **SecurityGroups**, a inst√¢ncia ser√° associada ao security group padr√£o da VPC default (que normalmente permite todo tr√°fego de sa√≠da e nenhum de entrada). E sem **KeyName**, n√£o haver√° chave SSH associada. Estamos focando apenas na cria√ß√£o do recurso e suas depend√™ncias no CloudFormation, n√£o na sua acessibilidade. Em um cen√°rio real, provavelmente voc√™ adicionaria um par√¢metro para **KeyName** (usando Type **AWS::EC2::KeyPair::KeyName** para listar as chaves dispon√≠veis) e talvez definisse regras de seguran√ßa. Mas isso adicionaria complexidade que fugiria do escopo de fundamentos do CloudFormation ‚Äì nosso objetivo aqui √© entender a mec√¢nica do template.\n    \n\nAgora nosso template tem dois recursos. O CloudFormation, ao criar a stack, identificar√° automaticamente que n√£o h√° depend√™ncia direta entre o bucket e a inst√¢ncia, ent√£o pode tentar criar em paralelo. Isso n√£o √© um problema. Se houvesse depend√™ncia (por exemplo, se a inst√¢ncia dependesse de algum valor do bucket ou vice-versa), ele garantiria a ordem correta ou voc√™ usaria **DependsOn**.\n\n### Referenciando recursos e atributos\n\nUma vantagem de definir recursos no CloudFormation √© poder referenci√°-los entre si. Por exemplo, se a inst√¢ncia EC2 precisasse do nome do bucket para, digamos, fazer algo no user data script (n√£o vamos cobrir user data aqui), poder√≠amos passar como metadado usando **Ref MeuBucket**. Como mencionado, **Ref MeuBucket** retornaria o nome f√≠sico do bucket criado. Outras fun√ß√µes intr√≠nsecas como **Fn::GetAtt** permitem obter atributos espec√≠ficos do recurso, caso ele tenha (ex: o bucket S3 tem um atributo **DomainName**, uma inst√¢ncia EC2 tem atributos como **PublicIp** ou **AvailabilityZone**). Veremos **GetAtt** no pr√≥ximo tutorial de Outputs.\n\nNo nosso exemplo atual, os recursos s√£o independentes, ent√£o n√£o usamos nenhuma refer√™ncia entre eles. Mas √© importante saber que se us√°ssemos, CloudFormation entenderia essa **depend√™ncia impl√≠cita**. Por exemplo, se fiz√©ssemos:\n\n```yaml\nSomeProperty: !Ref MeuBucket\n```\n\nem **MinhaInstancia**, automaticamente o CloudFormation criaria o **MeuBucket** primeiro, pois √© referenciado, e s√≥ depois a **MinhaInstancia**. Essa gest√£o autom√°tica simplifica muito arquiteturas com m√∫ltiplos componentes ‚Äì voc√™ n√£o precisa sequenciar manualmente, apenas conecta as pe√ßas e o CloudFormation cuida da ordem.\n\n### Dicas ao definir recursos\n\n- Consulte a documenta√ß√£o do tipo de recurso para saber quais propriedades s√£o obrigat√≥rias. Por exemplo, para **AWS::EC2::Instance**, **InstanceType** e **ImageId** s√£o obrigat√≥rias, mas **KeyName** n√£o (apesar de √∫til), por isso colocamos as m√≠nimas.\n- Utilize nomes l√≥gicos descritivos e coesos. Isso ajuda a entender o template e, ao ver eventos no CloudFormation, √© mais f√°cil identificar qual recurso deu erro/sucesso. Nomes l√≥gicos como **WebServer** ao inv√©s de **MinhaInstancia** poderiam ser mais sem√¢nticos dependendo do caso.\n- Lembre-se que muitos recursos AWS t√™m limites ou requisitos. N√≥s colocamos um bucket S3 e uma inst√¢ncia EC2, que n√£o t√™m uma interdepend√™ncia. Se voc√™ definir algo como uma associa√ß√£o de Elastic IP a inst√¢ncia, por exemplo, voc√™ teria que garantir que a inst√¢ncia existe antes de associar o IP (tipicamente CloudFormation sabe pela refer√™ncia).\n- **Recursos obrigat√≥rios:** Em CloudFormation, a se√ß√£o Resources deve ter pelo menos um recurso. Se voc√™ n√£o tiver nada l√° (ex: s√≥ par√¢metros e outputs), a stack vai falhar pois n√£o tem o que criar. Por outro lado, √© poss√≠vel criar recursos l√≥gicos que n√£o correspondem a recursos f√≠sicos dependendo de condi√ß√µes (ver tutorial 7) ou usar transforma√ß√µes macros que geram recursos.\n\n### Atualizando a stack com novo recurso\n\nSe voc√™ j√° criou a stack no tutorial anterior (com apenas o bucket) e quer **atualiz√°-la** para adicionar a inst√¢ncia EC2, basta aplicar o novo template. No Console CloudFormation, escolha **Update (Atualizar)** na stack existente e forne√ßa o template modificado. O CloudFormation ver√° que um novo recurso **MinhaInstancia** foi adicionado e criar√° apenas ele, mantendo o bucket intacto. Esse √© o poder do gerenciamento de stack: ele calcula diferen√ßas de estado e executa apenas o necess√°rio (nesse caso, um _Create_ adicional).\n\nSe estiver fazendo do zero, pode criar j√° com ambos recursos. Lembre de fornecer todos par√¢metros necess√°rios (BucketName e InstanceType se quiser diferente do default).\n\nAgora nosso template acumulou v√°rias partes importantes: par√¢metros (BucketName, InstanceType), recursos (bucket S3 e inst√¢ncia EC2) e uso de refer√™ncias (**!Ref**). Estamos progredindo bastante! No pr√≥ximo tutorial, exploraremos a se√ß√£o de **Outputs**, que permitir√° extrair informa√ß√µes √∫teis desses recursos (como o ID da inst√¢ncia EC2 criada, ou o nome do bucket) e apresent√°-las para uso externo."
  },
  {
    "id": "a3e233b2-822b-4a62-b4f6-61f36fc10f4a",
    "title": "Outputs",
    "description": "Como expor informa√ß√µes importantes sobre os recursos criados na stack, como ID de inst√¢ncia, URL de um Load Balancer ou ARN de um servi√ßo, facilitando integra√ß√µes e documenta√ß√µes.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/a3e233b2-822b-4a62-b4f6-61f36fc10f4a",
    "markdown": "Nesta etapa, aprenderemos a usar a se√ß√£o **Outputs** do CloudFormation. Outputs s√£o sa√≠das da stack ‚Äì voc√™ pode pensar neles como \"resultados\" que a stack divulga ap√≥s ser criada ou atualizada. Eles servem para destacar informa√ß√µes importantes dos recursos criados ou passar dados para outras stacks.\n\n### Por que usar Outputs?\n\n- **Facilitar acesso a informa√ß√µes-chave:** Muitas vezes, os recursos que criamos t√™m identificadores ou endpoints que precisamos anotar ou reutilizar. Por exemplo, se seu template criou um Bucket S3, voc√™ pode querer saber o nome real do bucket; se criou uma inst√¢ncia EC2, pode querer o ID ou IP p√∫blico; se criou um load balancer, o DNS dele, etc. Em vez de procurar manualmente cada recurso, voc√™ pode definir Outputs no template para exibir essas informa√ß√µes de forma organizada. No console do CloudFormation, existe uma aba \"Outputs\" na stack onde esses valores aparecem, facilitando a visualiza√ß√£o ap√≥s a cria√ß√£o.\n    \n- **Referenciar em outras stacks (Cross-stack reference):** Outputs podem ser **exportados** e importados por outras stacks. Suponha que voc√™ tenha uma stack de rede (criando VPC, subnets) e quer usar a VPC em outra stack de aplica√ß√£o. Voc√™ pode exportar o ID da VPC na stack de rede e, na stack de aplica√ß√£o, usar **Fn::ImportValue** para obter esse ID a partir do nome de Export. Isso permite compor infraestruturas em m√≥dulos. (Nota: para exportar, voc√™ deve fornecer um nome √∫nico global na propriedade **Export** do Output). .\n    \n- **Documenta√ß√£o e Debug:** Outputs tamb√©m funcionam como documenta√ß√£o viva ‚Äì quem implanta a stack v√™ claramente quais recursos principais foram criados e seus valores. E durante desenvolvimento, √© √∫til expor valores para conferir se est√£o corretos (p. ex., ver se uma fun√ß√£o Lambda retornou algo ou se uma condicional escolheu o recurso certo).\n    \n\n**Importante:** Evite expor dados sens√≠veis em Outputs. **Tudo que aparece em Outputs fica vis√≠vel para quem tiver acesso √† stack**. Por exemplo, nunca coloque senhas, chaves privadas ou secrets como output. CloudFormation n√£o oculta esses valores. Para segredos, prefira armazenar no AWS Secrets Manager ou Parameter Store e n√£o exibir no CloudFormation.\n\n### Definindo Outputs no template\n\nA sintaxe da se√ß√£o Outputs √© simples. Podemos declarar at√© 200 outputs por template, cada um com um Nome l√≥gico, Value e opcionalmente Description e Export. Exemplo b√°sico:\n\n```yaml\nOutputs:\n  OutputName:\n    Description: \"Descri√ß√£o do valor de sa√≠da\"\n    Value: <valor a retornar>\n    Export:\n      Name: <nome_de_exportacao_opcional>\n    Condition: <opcional, s√≥ produzir output se condi√ß√£o true>\n```\n\nVamos adicionar alguns outputs ao nosso template para ilustrar. Queremos ver, por exemplo:\n\n- O nome do bucket S3 criado (que no nosso template √© dado pelo par√¢metro BucketName, ent√£o ser√° o valor que o usu√°rio forneceu).\n- O ID da inst√¢ncia EC2 criada.\n- (Opcionalmente) talvez o IP p√∫blico da inst√¢ncia EC2, para mostrar uso de **Fn::GetAtt**.\n\nAdicionaremos esses outputs e tamb√©m vamos demonstrar como condicionar outputs (no pr√≥ximo tutorial, condicionaremos a cria√ß√£o do bucket, e podemos condicionar o output do bucket tamb√©m). Por ora, vamos adicion√°-los sem condi√ß√µes.\n\n**Exemplo 3: Outputs para bucket e inst√¢ncia**\n\n```yaml\nOutputs:\n  BucketNameOutput:\n    Description: \"Nome do bucket S3 criado pela stack\"\n    Value: !Ref MeuBucket   # Ref do bucket retorna seu nome (se BucketName property foi setada, retorna aquele nome)\n    Export:\n      Name: !Sub \"${AWS::StackName}-BucketName\"  # (Opcional) exporta o nome do bucket, usando o nome da stack como parte do export\n\n  InstanceId:\n    Description: \"ID da Inst√¢ncia EC2 criada\"\n    Value: !Ref MinhaInstancia  # Ref da inst√¢ncia EC2 retorna o InstanceId\n    Export:\n      Name: !Sub \"${AWS::StackName}-InstanceId\"\n\n  InstancePublicIP:\n    Description: \"IP p√∫blico da inst√¢ncia EC2\"\n    Value: !GetAtt MinhaInstancia.PublicIpAddress  # Obt√©m o atributo IP p√∫blico da inst√¢ncia\n```\n\n**Explica√ß√£o:** Definimos tr√™s outputs:\n\n- **BucketNameOutput:** Pega o nome do bucket. Usamos **!Ref MeuBucket**. Para recursos do tipo Bucket, **Ref** devolve o nome do bucket. Note que poder√≠amos tamb√©m ter usado diretamente o par√¢metro **BucketName** (porque no nosso template, o bucket name f√≠sico = valor do par√¢metro). Mas usar **Ref MeuBucket** √© mais geral, pois mesmo que o nome fosse gerado automaticamente (no caso de n√£o termos passado BucketName), ele retornaria o nome gerado. Colocamos tamb√©m um **Export** com um nome constru√≠do dinamicamente usando **!Sub**. Aqui concatenamos o nome da stack (**${AWS::StackName}**) com \"-BucketName\" para formar um nome de exporta√ß√£o √∫nico. Isso permitiria outra stack importar pelo mesmo nome. O uso de `!Sub` no Export nos deixa incluir o nome da stack (que √© √∫nico em sua conta/regi√£o) para que o export tamb√©m seja √∫nico (j√° que exports precisam ser √∫nicos por regi√£o/conta). Export √© opcional ‚Äì inclu√≠mos para ilustrar, mas se n√£o precisar compartilhar, voc√™ pode omiti-lo.\n    \n- **InstanceId:** Similar, com **!Ref MinhaInstancia** para obter o ID da inst√¢ncia EC2 criada. Exportamos tamb√©m com um nome √∫nico. Assim, outra stack poderia importar esse ID se necess√°rio (por exemplo, para criar um alarm no CloudWatch atrelado a essa inst√¢ncia).\n    \n- **InstancePublicIP:** Demonstra uso de **!GetAtt** (Get Attribute). Muitos recursos do CloudFormation possuem atributos consult√°veis listados na documenta√ß√£o. Para EC2 Instance, um dos atributos √© **PublicIpAddress** (IP p√∫blico IPv4). Usando **!GetAtt MinhaInstancia.PublicIpAddress**, pegamos esse valor atual ap√≥s a cria√ß√£o. Lembrando: a inst√¢ncia EC2 s√≥ ter√° PublicIp se estiver em uma subnet com auto-assign public IP habilitado (por padr√£o, subnets da VPC default sim t√™m). Ent√£o assumindo default VPC, este output retornar√° o IP p√∫blico. Caso contr√°rio, pode vir vazio. De qualquer forma, exemplifica como pegar atributos al√©m do ID padr√£o. N√£o exportamos esse, √© s√≥ informativo.\n    \n\nAgora, quando a stack for criada, voc√™ poder√° ir no CloudFormation, selecionar a stack e ver esses outputs na aba **Outputs**. O nome do bucket ser√° exibido, o InstanceId e o IP. Esses valores tamb√©m podem ser obtidos via CLI:\n\n```shell\naws cloudformation describe-stacks --stack-name NomeDaStack --query \"Stacks[0].Outputs\"\n```\n\nEsse comando consultaria os outputs via API.\n\n### Considera√ß√µes sobre Outputs e mudan√ßas\n\nOutputs podem depender de recursos ou par√¢metros. Se voc√™ tentar atualizar uma stack e remover um output que estava exportado e sendo importado por outra stack, o CloudFormation n√£o permitir√° a menos que voc√™ remova a importa√ß√£o primeiro ‚Äì h√° um acoplamento. Fora isso, outputs s√£o f√°ceis de adicionar ou remover. No nosso caso, s√£o simples e n√£o afetam a l√≥gica de recursos.\n\nRefor√ßando: n√£o coloque senhas ou segredos nos outputs, pois ficam vis√≠veis em texto plano.\n\nAgora nosso template tem outputs √∫teis. Ao criar/atualizar a stack com o template contendo outputs, veremos:\n\n- _BucketNameOutput_: exibindo o nome do bucket.\n- _InstanceId_: exibindo o ID da inst√¢ncia.\n- _InstancePublicIP_: exibindo o IP p√∫blico (se houver).\n\nEsses outputs confirmam que tudo foi criado corretamente e nos d√£o informa√ß√µes para eventualmente usar fora do CloudFormation (por exemplo, acessar o bucket ou inst√¢ncia). Tamb√©m mostramos como exportar outputs para reuso.\n\nNo pr√≥ximo tutorial, vamos explorar **Mapeamentos (Mappings)** para melhorar nosso template, tornando-o ciente de diferen√ßas entre regi√µes. Especificamente, resolveremos o problema do ID de AMI fixo usando um mapping por regi√£o."
  },
  {
    "id": "63a64568-78d9-45ae-b448-ca9188480966",
    "title": "Mapeamentos",
    "description": "Uso da se√ß√£o Mappings para definir tabelas de configura√ß√£o que variam por regi√£o, ambiente ou tipo de recurso, garantindo valores adequados para cada contexto sem a necessidade de par√¢metros.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/63a64568-78d9-45ae-b448-ca9188480966",
    "markdown": "Nesta se√ß√£o, vamos apresentar a funcionalidade de **Mappings** (Mapeamentos) nos templates CloudFormation. Mappings permitem definir dados est√°ticos no template como se fossem tabelas de consulta, para usar posteriormente com base em alguma chave. Eles s√£o √∫teis quando certos valores precisam variar de acordo com a regi√£o, ambiente ou outro crit√©rio, e voc√™ n√£o quer expor isso via par√¢metros ao usu√°rio (at√© porque poderiam ser muitos valores ou detalhes internos). Com mappings, a l√≥gica de sele√ß√£o do valor apropriado fica dentro do template.\n\n### O que s√£o Mappings e quando us√°-los?\n\nUm Mapping √© basicamente uma lista de pares chave-valor aninhados. Voc√™ pode imagin√°-lo como um dicion√°rio de dois n√≠veis. Voc√™ fornece duas chaves para buscar um valor: uma chave de primeiro n√≠vel e uma de segundo n√≠vel. Por exemplo, um mapping cl√°ssico √© **RegionMap** que mapeia regi√µes para algum valor espec√≠fico daquela regi√£o:\n\n- Chave de primeiro n√≠vel: nome da regi√£o (ex: \"us-east-1\")\n- Chave de segundo n√≠vel: algum atributo ou categoria (ex: \"AMI\")\n- Valor: por exemplo, o ID da AMI daquela regi√£o.\n\nUma vez definido um mapping, usamos a fun√ß√£o intr√≠nseca **Fn::FindInMap** (ou em YAML a forma curta **!FindInMap**) para recuperar o valor, dado um mapping name e duas keys.\n\n**Quando usar:**\n\n- **Varia√ß√µes por Regi√£o:** AMIs, por exemplo, diferem por regi√£o. Em vez de pedir ao usu√°rio para fornecer a AMI correta via par√¢metro (o que exige que ele saiba ou calcule isso), voc√™ pode manter internamente um mapping **Region -> AMI**. Assim, ao criar em us-west-2 versus us-east-1, o template automaticamente pega a AMI adequada.\n- **Varia√ß√µes por Ambiente:** Suponha que sua infra em Dev use inst√¢ncias menores ou menos n√≥s que em Prod. Voc√™ poderia ter um mapping `Environment -> { param1: X, param2: Y }` e escolher valores conforme um par√¢metro Environment. Em alguns casos, Conditions podem fazer algo parecido; mapeamentos s√£o √∫teis se h√° muitas varia√ß√µes combinat√≥rias.\n- **Tabelas de constantes:** Qualquer conjunto de valores fixos que voc√™ n√£o quer recalcular. Exemplo: taxas de cobran√ßa ou IDs de ARNs fixos por regi√£o. Mappings deixam o template autocontido com esses dados.\n\n**Diferen√ßa de Parameters vs Mappings:** Par√¢metros exp√µem escolhas para quem cria a stack; Mappings mant√™m as diferen√ßas encapsuladas no template. Use par√¢metros para entradas que o usu√°rio deve controlar (como nomes, contagens, tipos). Use mappings quando o template pode decidir sozinho com base em informa√ß√µes conhecidas (regi√£o, ambiente) sem incomodar o usu√°rio.\n\n### Sintaxe de Mappings\n\nA se√ß√£o **Mappings** no template √© escrita assim (YAML):\n\n```yaml\nMappings:\n  NomeDoMapping:\n    ChavePrimaria1:\n      ChaveSecundaria1: Valor\n      ChaveSecundaria2: Valor\n    ChavePrimaria2:\n      ChaveSecundaria1: Valor\n      ChaveSecundaria2: Valor\n```\n\nN√£o h√° limite pr√°tico muito baixo (s√£o suportados at√© centenas de entradas). Voc√™ **n√£o pode** usar fun√ß√µes intr√≠nsecas ou refer√™ncias dentro dos Mappings ‚Äì eles s√£o est√°ticos. As chaves devem ser strings literais (n√£o podem ser via Ref). Os valores podem ser strings ou listas de strings.\n\nVamos aplicar isso ao nosso exemplo. O problema identificamos: o ID da AMI para a inst√¢ncia EC2 varia por regi√£o. N√≥s fixamos para us-east-1, mas e se quisermos permitir criar a stack em outras regi√µes sem exigir que o usu√°rio descubra o ID correto? Podemos inserir um Mapping com mapeamento de regi√£o para AMI.\n\nA AWS geralmente publica IDs de AMIs para Amazon Linux 2 e outros. Para demonstra√ß√£o, usaremos alguns valores conhecidos (mesmos do exemplo da documenta√ß√£o do CloudFormation). Vamos mapear algumas regi√µes para seus AMIs Amazon Linux 2 HVM EBS General Purpose:\n\nDefiniremos um mapping chamado **RegionMap**. As chaves de primeiro n√≠vel ser√£o c√≥digos de regi√µes AWS (us-east-1, us-west-1, eu-west-1, ap-southeast-1, ap-northeast-1 por exemplo). Como queremos mapear para um ID de AMI, podemos usar a segunda chave fixa, digamos \"AMI\". (Tamb√©m poder√≠amos usar o esquema do exemplo AWS de ter \"HVM64\" e \"HVMG2\" para diferenciar AMIs de inst√¢ncias normais vs GPU, mas para simplificar vamos assumir s√≥ um tipo).\n\n**Exemplo 4: Definindo um Mapping para AMIs por regi√£o**\n\n```yaml\nMappings:\n  RegionMap:\n    us-east-1:\n      AMI: \"ami-0ff8a91507f77f867\"   # Amazon Linux 2 em N. Virginia\n    us-west-1:\n      AMI: \"ami-0bdb828fd58c52235\"   # Amazon Linux 2 em N. California\n    eu-west-1:\n      AMI: \"ami-047bb4163c506cd98\"   # Amazon Linux 2 em Irlanda\n    ap-southeast-1:\n      AMI: \"ami-08569b978cc4dfa10\"   # Amazon Linux 2 em Singapura\n    ap-northeast-1:\n      AMI: \"ami-06cd52961ce9f0d85\"   # Amazon Linux 2 em T√≥quio\n```\n\n**Explica√ß√£o:** Criamos **RegionMap** com 5 regi√µes e a respectiva AMI (64-bit HVM ebs-ssd) para Amazon Linux 2 em cada uma. Esses IDs foram obtidos de refer√™ncia p√∫blica de AMIs comuns. Note que:\n\n- Se a regi√£o onde voc√™ rodar n√£o estiver listada, n√£o haver√° correspond√™ncia e o CloudFormation dar√° erro ao tentar fazer FindInMap. Listamos algumas das principais. Voc√™ pode adicionar outras se quiser suportar mais regi√µes (por ex, sa-east-1, us-east-2, etc) ‚Äì para fins de exemplo mantivemos 5.\n- As keys como \"us-east-1\" **devem** corresponder exatamente ao valor do pseudo-par√¢metro **AWS::Region** daquela regi√£o. Felizmente, em CloudFormation existe o pseudo par√¢metro **AWS::Region** que sempre cont√©m a regi√£o atual da stack. Usaremos ele para buscar a linha correta.\n\nAgora, para usar esse mapping, precisamos alterar a propriedade **ImageId** da nossa inst√¢ncia para n√£o ser fixa e sim referenciar o mapping. Em YAML, usamos a fun√ß√£o intr√≠nseca **Fn::FindInMap** ou seu atalho **!FindInMap** fornecendo uma lista: **[ NomeDoMapping, ChavePrimaria, ChaveSecundaria ]**.\n\nQueremos: Map = RegionMap, PrimaryKey = (valor de AWS::Region), SecondaryKey = \"AMI\".\n\nA sintaxe curta YAML:\n\n```yaml\nImageId: !FindInMap [RegionMap, !Ref \"AWS::Region\", AMI]\n```\n\nExplica√ß√£o: **!Ref \"AWS::Region\"** retorna automaticamente a regi√£o de execu√ß√£o (ex: \"us-west-2\"). Com isso, **FindInMap** vai pegar **RegionMap[us-west-2][\"AMI\"]** e nos dar o ID correspondente. Se a regi√£o n√£o existir no map, erro; se existir, retorna string.\n\nVamos aplicar ao nosso recurso:\n\n```yaml\nMinhaInstancia:\n  Type: AWS::EC2::Instance\n  Properties:\n    InstanceType: !Ref InstanceType\n    ImageId: !FindInMap [RegionMap, !Ref \"AWS::Region\", AMI]\n```\n\nAgora, nosso template est√° preparado para escolher a AMI correta automaticamente. Se voc√™ rodar a stack em **us-east-1**, **!Ref AWS::Region** ser√° \"us-east-1\" e o mapping retornar√° **ami-0ff8a91507f77f867**. Em **eu-west-1**, pegar√° **ami-047bb4163c506cd98**. Assim n√£o precisamos criar um par√¢metro para AMI nem fixar via c√≥digo.\n\n### Revis√£o do template at√© aqui\n\nAt√© este ponto, temos incorporado:\n\n- **Parameters:** BucketName e InstanceType (e possivelmente KeyName se quisermos, mas n√£o implementamos ainda).\n- **Mappings:** RegionMap para AMIs.\n- **Resources:** MeuBucket (S3) e MinhaInstancia (EC2), usando os par√¢metros e mapping (ImageId agora usando RegionMap).\n- **Outputs:** Nome do bucket, ID e IP da inst√¢ncia, etc.\n\nNosso template est√° muito mais robusto pois suporta m√∫ltiplas regi√µes de forma transparente ao usu√°rio e est√° bem parametrizado.\n\n### Quando n√£o usar Mappings\n\nVale observar: com o aprimoramento dos servi√ßos AWS, em alguns casos Mappings tradicionais foram substitu√≠dos por alternativas:\n\n- No caso de AMIs, a AWS agora possibilita usar **SSM Parameter Store public parameters** para obter a AMI mais recente de determinado sistema operacional. Inclusive a pr√≥pria documenta√ß√£o menciona que ao inv√©s de usar mapping fixo de AMI, poder√≠amos usar o tipo de par√¢metro **AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>** apontando para um alias, garantindo sempre a image mais atual. Isso elimina a necessidade de atualizar manualmente o template quando sai AMI nova. Por√©m, entender Mappings continua importante porque nem tudo est√° dispon√≠vel via SSM Parameters, e muitos templates legados usam.\n- Em muitos casos, voc√™ pode usar l√≥gica condicional com menos chave, ou at√© script externo. Mappings brilham quando h√° uma matriz est√°tica de combina√ß√µes.\n\n### Testando o Mapping\n\nPara testar, tente criar/atualizar a stack em uma regi√£o diferente. Se voc√™ criou antes em us-east-1, experimente rodar em eu-west-1 sem mudar nada no template (exceto talvez o BucketName para ser √∫nico global). A inst√¢ncia EC2 dever√° ser criada usando a AMI daquela regi√£o (no caso ami-047bb4163c506cd98, que √© Amazon Linux 2 Ireland). Voc√™ pode confirmar no console EC2 > Instances, cheque o Ami ID ou nome da AMI utilizada.\n\nNos eventos da stack, voc√™ ver√° o CloudFormation usando **FindInMap** ‚Äì internamente ele j√° resolve antes de chamar a API.\n\nIsso conclui o uso de Mappings. Agora nosso template est√° capaz de se adaptar √† regi√£o fornecendo o AMI correto. Nos outputs, o InstanceId continua v√°lido e refletir√° a inst√¢ncia naquela regi√£o. O IP p√∫blico tamb√©m.\n\nNo pr√≥ximo tutorial, aprenderemos sobre **Condi√ß√µes (Conditions)**, que nos permitir√£o criar ou n√£o certos recursos com base em par√¢metros. Vamos implementar uma condi√ß√£o para criar o bucket S3 somente em ambientes espec√≠ficos, por exemplo, e ver como ajustar outputs e recursos conforme essa l√≥gica condicional."
  },
  {
    "id": "6a882d36-409a-413b-82a5-bd59338ac108",
    "title": "Condi√ß√µes",
    "description": "Implementa√ß√£o de l√≥gica condicional nos templates. Aprendemos a habilitar ou omitir recursos, propriedades e outputs dependendo de vari√°veis como ambiente (produ√ß√£o vs teste) ou requisitos espec√≠ficos.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/6a882d36-409a-413b-82a5-bd59338ac108",
    "markdown": "Agora focaremos nas **Condi√ß√µes (Conditions)** nos templates CloudFormation. Condi√ß√µes permitem incluir l√≥gica booleana no template para decidir se determinados recursos ou outputs devem ser criados ou definidos, dependendo de valores de par√¢metros ou pseudo-par√¢metros. Isso √© extremamente √∫til para escrever um √∫nico template que sirva para cen√°rios variados (ex.: ambiente de teste vs produ√ß√£o) sem criar tudo em todos os casos.\n\n### O que s√£o e por que usar Condi√ß√µes?\n\nUma condi√ß√£o avalia a **verdadeiro ou falso** no momento da cria√ß√£o ou atualiza√ß√£o da stack, e com base nisso o CloudFormation pode:\n\n- **Criar ou ignorar um recurso**: Recursos associados a uma condi√ß√£o s√≥ ser√£o criados se a condi√ß√£o for verdadeira; se for falsa, o CloudFormation _ignora_ aquele recurso (como se n√£o existisse no template).\n- **Incluir ou omitir valores em propriedades**: Com a fun√ß√£o intr√≠nseca **Fn::If**, voc√™ pode definir que certa propriedade tome um valor X se condi√ß√£o verdadeira ou Y se falsa.\n- **Controlar outputs condicionalmente**: Outputs tamb√©m podem ter o campo **Condition**, de modo que s√≥ ser√£o emitidos se a condi√ß√£o for verdadeira.\n\nCen√°rios t√≠picos:\n\n- **Ambientes diferentes (dev/prod)**: No template voc√™ pode ter recursos de alta disponibilidade (como m√∫ltiplas inst√¢ncias, ou uma RDS de grande porte) e usar condi√ß√µes para s√≥ cri√°-los quando um par√¢metro Environment for \"prod\". Em \"dev\", aqueles recursos n√£o s√£o criados, economizando custos.\n- **Recursos opcionais**: Talvez voc√™ queira um template que _opcionalmente_ crie um recurso, controlado por um par√¢metro booleano (ex: **CreateCache = true/false** decide se cria um cluster ElastiCache).\n- **Customiza√ß√µes regionais/conta**: √Äs vezes um servi√ßo n√£o existe em certa regi√£o, voc√™ pode condicionar para n√£o tentar criar l√°.\n\nSem condi√ß√µes, esses casos exigiriam manter templates separados ou instruir usu√°rios a habilitar/desabilitar coisas manualmente. Com condi√ß√µes, um template √∫nico se adapta.\n\n### Definindo condi√ß√µes\n\nAs condi√ß√µes s√£o definidas na se√ß√£o **Conditions** do template. L√°, cada condi√ß√£o tem um nome l√≥gico e uma express√£o booleana usando fun√ß√µes intr√≠nsecas l√≥gicas (ex: **Fn::Equals**, **Fn::And**, **Fn::Or**, **Fn::Not**). CloudFormation avalia todas as condi√ß√µes no in√≠cio da cria√ß√£o/atualiza√ß√£o da stack, com base nos par√¢metros fornecidos e nos pseudo-par√¢metros dispon√≠veis.\n\nSintaxe exemplar:\n\n```yaml\nConditions:\n  IsProd: !Equals [ !Ref EnvironmentType, \"prod\" ]\n  CreateCacheCluster: !And \n    - !Equals [ !Ref CacheEnabled, \"true\" ]\n    - !Not [ !Equals [ !Ref EnvironmentType, \"test\" ] ]\n```\n\nAqui supomos existir par√¢metros **EnvironmentType** e **CacheEnabled**. **IsProd** ser√° true se **EnvironmentType** for \"prod\". J√° **CreateCacheCluster** seria true se CacheEnabled for \"true\" **e** o ambiente n√£o for \"test\", por exemplo.\n\nAs fun√ß√µes l√≥gicas dispon√≠veis:\n\n- **Fn::Equals( A, B )**: retorna true se A == B.\n- **Fn::And( cond1, cond2, ... )**: true se todas verdadeiras.\n- **Fn::Or( cond1, cond2, ... )**: true se pelo menos uma verdadeira.\n- **Fn::Not( cond )**: true se cond for falsa.\n- (H√° tamb√©m **Fn::If**, mas este √© usado inline em propriedades/outputs, n√£o dentro de Conditions section.)\n\nVoc√™ pode tamb√©m referenciar valores diretamente (como no exemplo, com !Ref de par√¢metros).\n\n### Aplicando condi√ß√µes no nosso exemplo\n\nVamos introduzir um par√¢metro chamado **EnvironmentType** que pode ser \"dev\" ou \"prod\". Com isso, faremos:\n\n- Uma condi√ß√£o **IsProd** que verifica se **EnvironmentType** == \"prod\".\n- Usaremos essa condi√ß√£o para decidir se o bucket S3 deve ser criado. Imaginemos que em ambiente de produ√ß√£o queremos um bucket para armazenar logs, mas em dev n√£o √© necess√°rio.\n- Tamb√©m usaremos a condi√ß√£o nos Outputs relacionados ao bucket, para n√£o listar output de bucket quando ele n√£o for criado.\n\nPassos:\n\n1. Adicionar o par√¢metro **EnvironmentType**.\n2. Adicionar a condi√ß√£o **CreateBucketCondition** (por exemplo) usando **Fn::Equals** com \"prod\".\n3. Adicionar o campo **Condition: CreateBucketCondition** no recurso do Bucket e no output do Bucket.\n4. Opcional: poder√≠amos usar Fn::If para algum valor, mas neste caso n√£o precisamos ‚Äì estamos simplesmente incluindo/excluindo o recurso.\n\n**Atualiza√ß√£o do template com Environment e Condition:**\n\n```yaml\nParameters:\n  EnvironmentType:\n    Description: \"Tipo de ambiente (dev ou prod)\"\n    Type: String\n    Default: dev\n    AllowedValues:\n      - dev\n      - prod\n\nConditions:\n  CreateBucketCondition: !Equals [ !Ref EnvironmentType, \"prod\" ]\n\nResources:\n  MeuBucket:\n    Type: AWS::S3::Bucket\n    Condition: CreateBucketCondition  # S√≥ cria o bucket se for ambiente prod\n    Properties:\n      BucketName: !Ref BucketName\n  MinhaInstancia:\n    Type: AWS::EC2::Instance\n    Properties:\n      InstanceType: !Ref InstanceType\n      ImageId: !FindInMap [RegionMap, !Ref \"AWS::Region\", AMI]\n\nOutputs:\n  BucketNameOutput:\n    Description: \"Nome do bucket S3 criado pela stack\"\n    Value: !Ref MeuBucket\n    Condition: CreateBucketCondition  # S√≥ aparece se bucket foi criado\n    Export:\n      Name: !Sub \"${AWS::StackName}-BucketName\"\n  InstanceId:\n    Description: \"ID da Inst√¢ncia EC2 criada\"\n    Value: !Ref MinhaInstancia\n    Export:\n      Name: !Sub \"${AWS::StackName}-InstanceId\"\n  InstancePublicIP:\n    Description: \"IP p√∫blico da inst√¢ncia EC2\"\n    Value: !GetAtt MinhaInstancia.PublicIpAddress\n```\n\n**Explica√ß√£o das altera√ß√µes:**\n\n- **Par√¢metro EnvironmentType:** Permitimos \"dev\" ou \"prod\", padr√£o \"dev\". Assim, se o usu√°rio n√£o especificar, assumimos dev (n√£o criar bucket). Em casos de uso real, poder√≠amos ter mais nuance, mas isso basta.\n    \n- **Condition CreateBucketCondition:** Usa **Fn::Equals** (forma YAML **!Equals**) para comparar o valor de EnvironmentType com \"prod\". Se for igual, condi√ß√£o verdadeira; se diferente (ex: \"dev\"), condi√ß√£o falsa. √â uma condi√ß√£o simples boolean.\n    \n- **Usando a condi√ß√£o no recurso S3:** Adicionamos **Condition: CreateBucketCondition** ao recurso **MeuBucket**. Isso instruir√° o CloudFormation a **criar esse recurso somente se a condi√ß√£o for true (prod)**. Caso contr√°rio, se for dev, ele pular√° a cria√ß√£o do bucket. Internamente, √© como se o recurso n√£o existisse para fins de cria√ß√£o. (Observa√ß√£o: se a condi√ß√£o for falsa, voc√™ _n√£o paga_ nem nada pelo recurso, ele realmente n√£o √© criado.)\n    \n- **Usando a condi√ß√£o no Output do bucket:** Colocamos a mesma condi√ß√£o no output **BucketNameOutput**. Assim, em dev (quando nenhum bucket foi criado), n√£o aparecer√° um output vazio. Em produ√ß√£o, o output aparecer√° com o nome do bucket. Isso √© importante: se tent√°ssemos referenciar **!Ref MeuBucket** em output sem condicionar, e o bucket n√£o fosse criado, o CloudFormation teria um problema (refer√™ncia a recurso inexistente). Ent√£o devemos condicionar ambos de forma consistente.\n    \n- Decidimos n√£o condicionar o output InstanceId/IP porque a inst√¢ncia criamos sempre (independente do ambiente nesse exemplo). Mas poder√≠amos, se quis√©ssemos que em dev nem inst√¢ncia houvesse, por exemplo.\n    \n\n**Efeito das condi√ß√µes na pr√°tica:**\n\n- Quando **EnvironmentType=dev**: **CreateBucketCondition** = false. CloudFormation, ao criar a stack, _ignora_ o recurso MeuBucket. Ele n√£o tentar√° cri√°-lo. Apenas criar√° a inst√¢ncia. Nos outputs, pular√° o BucketNameOutput. Assim, a stack resultante n√£o ter√° bucket S3 e nem output relacionado a bucket. Mais tarde, se voc√™ atualizar a stack trocando EnvironmentType para \"prod\", o CloudFormation perceber√° que agora a condi√ß√£o virou true e **criar√°** o bucket S3 durante a atualiza√ß√£o (e passar√° a mostrar o output).\n- Quando **EnvironmentType=prod**: **CreateBucketCondition** = true. CloudFormation criar√° tanto inst√¢ncia quanto bucket. Outputs incluir√£o ambos. Se depois voc√™ atualiza para \"dev\", ele vai ver que a condi√ß√£o virou false e **deletar√° o bucket** automaticamente durante a atualiza√ß√£o, porque o recurso agora est√° associado a uma condi√ß√£o falsa. Isso √© importante: CloudFormation gerencia a exist√™ncia cont√≠nua conforme as condi√ß√µes. Recurso que deixa de atender condi√ß√£o √© removido (a menos que voc√™ proteja com pol√≠ticas de atualiza√ß√£o, etc).\n\nEssa √∫ltima parte vale destacar: se voc√™ mudar uma condi√ß√£o de modo que um recurso existente passe a n√£o ser mais necess√°rio, o CloudFormation **vai apag√°-lo** na atualiza√ß√£o da stack. Portanto, use condi√ß√µes com cautela se h√° dados associados. No nosso exemplo, se o bucket tivesse objetos e voc√™ mudasse para dev, ele tentaria deletar o bucket ‚Äì se tiver objetos e nenhuma DeletionPolicy Retain, a dele√ß√£o da stack falharia (buckets n√£o esvaziados n√£o podem ser deletados por padr√£o). Ent√£o, em casos reais, combine Conditions com DeletionPolicy: Retain se n√£o quiser perder dados ao desativar recurso. Ou documente que ao voltar de prod para dev pode haver perda (ou manual intervention de esvaziar bucket antes).\n\n### Fun√ß√£o Fn::If em propriedades\n\nAl√©m de condicionar presen√ßa de recursos/outputs, voc√™ pode usar **Fn::If** para fornecer valores diferentes para propriedades. A sintaxe √©:\n\n```yaml\n!If [ NomeCondicao, ValorSeTrue, ValorSeFalse ]\n```\n\nPor exemplo, poder√≠amos setar a propriedade **InstanceType** condicionalmente:\n\n```yaml\nInstanceType: !If [ CreateBucketCondition, \"t3.small\", \"t2.micro\" ]\n```\n\nEste n√£o √© um exemplo muito pr√°tico (pois j√° temos param InstanceType), mas ilustraria: se prod (cond true), usa t3.small, sen√£o t2.micro. **Fn::If** permite inclusive retornar um pseudo-valor chamado **AWS::NoValue** que indica \"n√£o definir nada\". Isso √© √∫til para casos como uma propriedade que voc√™ s√≥ quer incluir quando condicao true, e omitir completamente quando false. Ex:\n\n```yaml\nBucketEncryption: !If [ EnableEncryption, { ServerSideEncryptionConfiguration: [...] }, !Ref \"AWS::NoValue\" ]\n```\n\nSe condi√ß√£o EnableEncryption = false, !Ref AWS::NoValue faz com que CloudFormation remova/ignore essa propriedade, como se n√£o estivesse presente.\n\nNo nosso template, n√£o precisamos de Fn::If porque simplesmente n√£o criamos o bucket em dev. Mas √© bom saber que existe essa op√ß√£o para propriedades e at√© mesmo para gerar parte de strings (usando Fn::Sub internamente com Ifs).\n\nNosso template est√° quase completo cobrindo todos os fundamentos.\n\nRecapitulando as condi√ß√µes implementadas:\n\n- Par√¢metro **EnvironmentType** controla l√≥gica.\n- Condi√ß√£o **CreateBucketCondition** avalia a necessidade do bucket.\n- Usamos em **Resources** (MeuBucket) e **Outputs** (BucketNameOutput).\n\n### Testando as condi√ß√µes\n\nFa√ßa testes:\n\n- Crie a stack com **EnvironmentType=dev**. Espere finalizar. Verifique que nenhum bucket S3 foi criado (no CloudFormation, a aba Resources listar√° apenas a inst√¢ncia EC2). Outputs devem mostrar apenas InstanceId e IP, nada de bucket.\n- Atualize a mesma stack mudando **EnvironmentType** para \"prod\". Acompanhe os eventos: dever√° criar agora o recurso S3 bucket. Verifique no S3 que o bucket apareceu. Outputs agora mostram BucketNameOutput tamb√©m. (Lembre de fornecer BucketName param v√°lido e √∫nico).\n- Tente atualizar de volta para dev (n√£o fa√ßa isso se colocou dados no bucket a n√£o ser que esteja testando ‚Äì pois vai deletar o bucket). Nos eventos, ver√° que vai deletar o recurso S3 (como n√£o tinha DeletionPolicy Retain, vai tentar deletar mesmo). Se o bucket estiver vazio, a dele√ß√£o ocorre e a stack fica sem bucket novamente.\n\nIsso demonstra o poder do CloudFormation gerenciando todo o ciclo de vida dos recursos conforme as condi√ß√µes e mudan√ßas de par√¢metros ‚Äì reduzindo muita m√£o de obra manual.\n\nAgora nosso template abordou: Par√¢metros, Recursos, Mapeamentos, Condi√ß√µes e Outputs ‚Äì cobrindo todos os aspectos principais de templates CloudFormation.\n\nNo pr√≥ximo (e √∫ltimo) tutorial, sairemos um pouco do template em si e falaremos de **CloudFormation Drift** (Deriva√ß√µes), uma ferramenta para detectar desvios entre o estado atual dos recursos e o que est√° definido no template da stack. Isso nos ajuda a garantir que, depois de criado, ningu√©m alterou manualmente algum recurso por fora, fugindo do controle do CloudFormation."
  },
  {
    "id": "15236da4-4326-43aa-9ef6-9aeed8dfe9cc",
    "title": "Cloudformation Drift",
    "description": "Detec√ß√£o de desvios entre a infraestrutura real e o que est√° definido no template. Exploramos o Drift Detection, como identificar mudan√ßas feitas fora do CloudFormation e corrigir inconsist√™ncias.",
    "tool": "CloudFormation",
    "level": "iniciante",
    "tags": [
      "Ambiente",
      "Fundamentos",
      "AWS"
    ],
    "date": "2025-06-08",
    "url": "/tutorials/15236da4-4326-43aa-9ef6-9aeed8dfe9cc",
    "markdown": "Ap√≥s implementar sua infraestrutura com CloudFormation, √© fundamental mant√™-la consistente com o que o template define. **CloudFormation Drift** (desvio ou deriva) refere-se √† situa√ß√£o em que os recursos reais em execu√ß√£o **n√£o correspondem mais** ao que est√° descrito no template da stack. Isso acontece quando houve mudan√ßas feitas **fora** do CloudFormation ‚Äì por exemplo, algu√©m manualmente alterou uma configura√ß√£o diretamente pelo console AWS ou CLI, ou deletou/adicionou algo sem passar pelo CloudFormation.\n\nO CloudFormation possui um recurso chamado **Drift Detection** (detec√ß√£o de deriva) para identificar esses casos. Vamos entender e aprender a usar.\n\n### O que √© Drift e por que √© um problema?\n\nIdealmente, toda modifica√ß√£o de infraestrutura ocorreria via atualiza√ß√£o de stack (template). Por√©m, na pr√°tica, times podem fazer ‚Äúhotfixes‚Äù r√°pido diretamente nos recursos, ou administradores podem ajustar algo temporariamente e esquecer de refletir no template. Isso causa **drift**: a stack no CloudFormation _pensa_ que o recurso est√° de uma maneira (porque assim est√° no template), mas na realidade ele foi mudado para outra configura√ß√£o. Exemplos:\n\n- Algu√©m acessa o console EC2 e muda o tipo da inst√¢ncia de t2.micro para t3.small sem passar pelo CloudFormation.\n- Um bucket S3 criado pela stack teve a versioning habilitada manualmente fora do CF.\n- Um recurso foi deletado diretamente (ex: um desenvolvedor apagou um bucket pelo S3 console).\n\nEssas discrep√¢ncias podem causar problemas graves:\n\n- O CloudFormation ao atualizar a stack pode falhar ou reverter porque encontra recursos em estado inesperado. Por exemplo, se ele tentar modificar algo mas detecta que o recurso foi alterado externamente, pode gerar conflito.\n- Ao deletar a stack, pode falhar (ex: bucket n√£o vazio, etc) se n√£o estava conforme esperado.\n- Mais importante: a infraestrutura real n√£o condiz com o que _achamos_ que est√° (o template), o que desafia o prop√≥sito de IaC. Isso dificulta reproduzir ambientes ou rastrear mudan√ßas.\n\n### Como o Drift Detection funciona?\n\nO AWS CloudFormation Drift Detection permite verificar se h√° desvios:\n\n- Voc√™ pode rodar drift detection em uma **stack inteira** ou em recursos espec√≠ficos dentro da stack.\n- O CloudFormation ent√£o examina o template da stack e as configura√ß√µes atuais de cada recurso e compara.\n- Se um recurso tem todas as propriedades exatamente conforme definidas no template, ele ser√° marcado como **IN_SYNC** (em conformidade).\n- Se alguma propriedade diverge, o recurso ser√° marcado como **MODIFIED**; se um recurso existente na stack foi exclu√≠do manualmente, marcar√° como **DELETED**; se algum recurso extra foi criado fora (que a CF n√£o conhece), ele n√£o aparece na lista pois CF s√≥ rastreia o que est√° no template.\n- O resultado geral da stack ser√° **DRIFTED** se um ou mais recursos est√£o com drift, ou **IN_SYNC** se tudo bate. Recursos n√£o suportados pela drift detection aparecem como **NOT_CHECKED** (nem todos os tipos de recurso s√£o verific√°veis ainda).\n\nDrift detection **n√£o corrige** nada automaticamente ‚Äì ele apenas detecta e reporta diferen√ßas. Cabe a voc√™ ou sua equipe tomar a√ß√£o:\n\n- Ou ajustar o recurso manualmente de volta ao estado definido no template.\n- Ou atualizar o template/stack para o novo estado desejado (assim eliminando o drift porque agora o template concorda).\n- Em casos de recurso deletado manual, voc√™ pode remov√™-lo do template (atualizar stack) ou recri√°-lo via stack (por exemplo, usando recurso Retain + import).\n- **Dica:** CloudFormation tem recurso de import stack (importar recurso externo para dentro da stack) caso algu√©m criou algo manual e voc√™ quer traz√™-lo sob gerenciamento CF ‚Äì mas isso √© avan√ßado.\n\n### Executando Drift Detection\n\n**Via Console AWS:**\n\n1. No Console CloudFormation, selecione a stack desejada.\n2. No menu **Stack actions (A√ß√µes da pilha)**, clique em **Detect drift (Detectar deriva)**.\n3. O CloudFormation iniciar√° a an√°lise. Voc√™ ver√° uma barra de progresso informando que iniciou a detec√ß√£o de deriva. Aguarde alguns minutos (o tempo depende do n√∫mero de recursos).\n4. Quando concluir, a p√°gina da stack mostrar√° o campo **Drift status**. Se diverg√™ncias foram encontradas, aparecer√° **DRIFTED**, sen√£o **IN_SYNC**. Tamb√©m haver√° uma marca√ß√£o de tempo da √∫ltima verifica√ß√£o.\n5. V√° na aba **Resources** da stack e voc√™ ver√° agora colunas extras de Drift. Cada recurso listado ter√° um status: IN_SYNC, MODIFIED, DELETED ou NOT_CHECKED. Se estiver MODIFIED, voc√™ pode clicar para ver detalhes das diferen√ßas. Haver√° uma compara√ß√£o listando cada propriedade diferente ‚Äì mostrando valor esperado (do template) vs valor atual. Por exemplo, pode destacar que o InstanceType esperado era t2.micro mas o atual √© t3.small, ou que BucketVersioning era false mas agora true, etc., com destaques em amarelo/verde/vermelho para mudan√ßas.\n6. Voc√™ pode tamb√©m clicar em **View drift results (Ver resultados de deriva)** no menu de a√ß√µes da stack para ver um resumo completo incluindo recursos n√£o driftados.\n\n**Via AWS CLI:** Voc√™ pode usar os comandos:\n\n- **aws cloudformation detect-stack-drift --stack-name NomeDaStack** ‚Äì isso inicia a detec√ß√£o. A sa√≠da dar√° um StackDriftDetectionId.\n- Com esse ID, rode **aws cloudformation describe-stack-drift-detection-status --stack-drift-detection-id <id>** para ver o status (IN_PROGRESS, COMPLETED, etc.).\n- Uma vez completo, use **aws cloudformation describe-stack-resource-drifts --stack-name NomeDaStack** para obter a lista de recursos e seus drift statuses e detalhes. A sa√≠da JSON listar√° cada recurso com **StackResourceDriftStatus** (MODIFIED, IN_SYNC, etc) e, se modificado, um diff das propriedades divergentes.\n\nPara nossa stack de exemplo, vamos simular drift:\n\n- Imagine que algu√©m habilitou manualmente o versionamento no bucket S3 (se criado) via Console S3.\n- Ou mudou uma tag ou propriedade.\n- Ou alterou o Security Group padr√£o afetando a inst√¢ncia (coisa fora do template).\n- Qualquer mudan√ßa serve.\n\nAp√≥s isso, rodamos a detec√ß√£o de drift:\n\n- O CloudFormation detectar√°, por exemplo, que o bucket S3 tem **VersioningConfiguration** driftado (no template n√£o estava habilitado, atual est√° Enabled). Ele marcaria o recurso bucket como MODIFIED.\n- Nossa inst√¢ncia se ningu√©m mexeu, ficaria IN_SYNC.\n- O Drift status da stack apareceria como DRIFTED (pois pelo menos um recurso driftou).\n\nVoc√™ pode ent√£o tomar a√ß√£o: no caso do bucket versioning habilitado manual, voc√™ decide se quer manter isso. Se sim, voc√™ deveria atualizar o template para habilitar versioning (assim alinhando o template com a realidade). Se n√£o, desabilite manual de volta para ficar como template e rerode drift detect para confirmar que voltou a IN_SYNC.\n\nSe um recurso foi **deletado manualmente** (por exemplo, algu√©m apagou o bucket inteiro fora do CloudFormation), a detec√ß√£o marcaria o recurso como **DELETED**. Nesse caso, a stack estaria driftada. Voc√™ poderia:\n\n- Recriar manual e esperar CF perceb√™-lo (n√£o, CF n√£o vai \"adotar\" espontaneamente).\n- Atualizar a stack removendo aquele recurso do template, efetivamente dizendo a CF para esquec√™-lo (e talvez recri√°-lo via import).\n- Ou simplesmente deletar a stack (que vai notar que recurso j√° n√£o existe e provavelmente s√≥ remover o registro dele).\n\n**Recursos n√£o suportados:** Se algum tipo de recurso n√£o √© suportado pelo drift detection, ele aparece como NOT_CHECKED. Por exemplo, AWS::IAM::Role drift detection n√£o pega mudan√ßas em certos managed policies, etc., se n√£o suportado. A lista de suportados est√° na doc (a maioria dos principais s√£o suportados). Nesse caso, conside usar Config ou outras ferramentas para monitorar aqueles.\n\n### Incorporando Drift Detection no fluxo\n\n√â boa pr√°tica:\n\n- Executar drift detection periodicamente ou antes de atualiza√ß√µes importantes, para garantir que a stack est√° limpa.\n- Integrar com AWS Config para monitorar mudan√ßas fora do CF, ou usar CloudFormation StackSets com guardrails.\n- Educar a equipe a **n√£o mudar manualmente** recursos gerenciados por CloudFormation, ou se fizer, pelo menos sincronizar depois.\n\nCloudFormation Drift Detection ajuda a **garantir consist√™ncia** e **alertar** caso algu√©m tenha fugido do script. Assim, voc√™ mant√©m infraestrutura como c√≥digo de verdade, sabendo que o que est√° rodando bate com o que est√° codificado.\n\n### Resumo final\n\nNeste tutorial final, aprendemos a:\n\n- Definir drift e entender por que evitar mudan√ßas fora do CloudFormation.\n- Usar a detec√ß√£o de deriva via console e CLI para identificar discrep√¢ncias.\n- Interpretar os resultados (IN_SYNC vs DRIFTED, recurso MODIFIED/DELETED).\n- Tomar a√ß√µes para corrigir drift (atualizar template ou recursos).\n\nCom isso, conclu√≠mos nossa s√©rie de fundamentos do AWS CloudFormation. Passamos por todos os componentes principais de um template (Par√¢metros, Recursos, Outputs, Mapeamentos, Condi√ß√µes) e tamb√©m abordamos boas pr√°ticas de manuten√ß√£o (Drift).\n\nAgora voc√™ est√° apto a criar templates b√°sicos, reutiliz√°-los para diferentes ambientes, e manter suas stacks sob controle de forma segura e audit√°vel. CloudFormation √© uma ferramenta poderosa ‚Äì continue explorando recursos adicionais como **Metadata**, **Transform (SAM)** para serverless, **StackSets** para m√∫ltiplas contas/regi√µes, e a vasta biblioteca de tipos de recursos dispon√≠veis.\n\nBom trabalho e boa sorte na sua jornada de infraestrutura como c√≥digo na AWS! üèóÔ∏èüöÄ "
  },
  {
    "id": "f14a7880-39d7-4733-87db-193ff6e74329",
    "title": "Landing Zone com Terraform",
    "description": "Neste tutorial, voc√™ aprender√° a construir uma Landing Zone usando Terraform para estruturar uma base segura e organizada para os seus projetos na AWS. S√£o abordadas as melhores pr√°ticas de separa√ß√£o de contas, configura√ß√£o de VPC, sub-redes (p√∫blicas e privadas), e o uso de Internet/NAT Gateways. Essa base garantir√° que, tanto a aplica√ß√£o monol√≠tica quanto a arquitetura de microsservi√ßos, possam ser provisionadas de forma consistente e segura.",
    "tool": "Terraform",
    "level": "intermediario",
    "tags": [
      "Landing Zone",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/f14a7880-39d7-4733-87db-193ff6e74329",
    "markdown": "## Vis√£o Geral e Configura√ß√£o Inicial\n\nAntes de criar qualquer recurso na nuvem, √© recomend√°vel preparar uma **Landing Zone** ‚Äì uma configura√ß√£o b√°sica de ambiente em nuvem que segue boas pr√°ticas de seguran√ßa e organiza√ß√£o. Na AWS, uma Landing Zone geralmente inclui a cria√ß√£o de uma **VPC (Virtual Private Cloud)** com sub-redes configuradas, gateways de rede e defini√ß√µes de seguran√ßa iniciais (como grupos de seguran√ßa e roles de IAM). Em resumo, a Landing Zone √© uma funda√ß√£o isolada e segura sobre a qual voc√™ vai construir sua infraestrutura. No contexto do nosso projeto BFF (Backend for Frontend), usaremos uma √∫nica conta AWS e uma VPC para manter todos os recursos necess√°rios de forma organizada.\n\n> ‚ÑπÔ∏è **O que √© uma VPC?**  \n> Uma VPC √© uma rede virtual isolada dentro da AWS onde podemos provisionar recursos. Ela se assemelha a uma rede tradicional em um data center pr√≥prio, mas usufrui da escalabilidade da AWS. Dentro da VPC vamos criar **sub-redes** (subnets) p√∫blicas e privadas. Conforme a documenta√ß√£o da AWS: _‚ÄúA subnet √© um intervalo de endere√ßos IP em sua VPC... Ap√≥s adicionar subnets, voc√™ pode implantar recursos AWS na VPC‚Äù_. Ou seja, subnets p√∫blicas ter√£o acesso direto √† internet, enquanto subnets privadas n√£o ser√£o acess√≠veis diretamente de fora, aumentando a seguran√ßa para recursos sens√≠veis (como bancos de dados).\n\nNesta primeira etapa, vamos criar os seguintes componentes b√°sicos com Terraform:\n\n- **VPC personalizada** com um bloco CIDR (faixa de IPs) para englobar nossa rede (ex: **10.0.0.0/16**).\n- **Sub-rede p√∫blica** (por exemplo **10.0.1.0/24**) para hospedar recursos que precisam de acesso √† internet (como servidores de aplica√ß√£o).\n- **Sub-rede privada** (por exemplo **10.0.2.0/24**) para recursos internos (como banco de dados) sem acesso p√∫blico.\n- **Internet Gateway (IGW)** para permitir que inst√¢ncias em sub-rede p√∫blica se conectem √† internet.\n- **Route Table** associada √† sub-rede p√∫blica, com rota padr√£o via o Internet Gateway (assim, tr√°fego 0.0.0.0/0 sai para a internet).\n- **Grupo de seguran√ßa base** definindo regras iniciais de tr√°fego (por exemplo, permitir SSH ou HTTP de locais apropriados).\n\nAbaixo, um diagrama simplificado da arquitetura de rede que vamos criar:\n\n![Landingzone](/tfLandingzone.svg)\n\n_Figura: Diagrama de rede mostrando a VPC com uma sub-rede p√∫blica (conectada a um Internet Gateway para acesso internet) e uma sub-rede privada (isolada da internet diretamente)._ Note que n√£o inclu√≠mos um NAT Gateway neste diagrama para simplificar ‚Äì em cen√°rios reais, um NAT Gateway seria usado se recursos em sub-rede privada precisassem acessar a internet (para atualiza√ß√µes, por exemplo).\n\n## Criando a VPC e Sub-redes com Terraform\n\nVamos agora escrever o c√≥digo Terraform passo a passo para criar a Landing Zone. Certifique-se de j√° ter configurado suas credenciais AWS localmente (via arquivo de credenciais ou vari√°veis de ambiente) e o Terraform instalado. A configura√ß√£o consistir√° em recursos do provedor AWS. Vamos come√ßar definindo o provedor e a regi√£o, depois os recursos de rede:\n\n```hcl\n# Configurar o provedor AWS e a regi√£o\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"  # vers√£o do provedor AWS\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = \"us-east-1\"   # Regi√£o AWS onde os recursos ser√£o criados\n}\n\n# 1. Criar a VPC principal da Landing Zone\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = \"10.0.0.0/16\"      # Bloco de IPs da VPC\n  enable_dns_support   = true               # Habilita DNS interno na VPC\n  enable_dns_hostnames = true               # Necess√°rio p/ resolver nomes de host de inst√¢ncias\n  tags = {\n    Name = \"bff-vpc\"                        # Nomear a VPC para f√°cil identifica√ß√£o\n  }\n}\n\n# 2. Sub-rede p√∫blica na VPC\nresource \"aws_subnet\" \"public\" {\n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = \"10.0.1.0/24\"   # Faixa de IPs da sub-rede p√∫blica\n  map_public_ip_on_launch = true           # Garante atribui√ß√£o de IP p√∫blico √†s inst√¢ncias\n  availability_zone       = \"us-east-1a\"   # Zona de disponibilidade (exemplo)\n  tags = {\n    Name = \"bff-public-subnet\"\n  }\n}\n\n# 3. Sub-rede privada na VPC\nresource \"aws_subnet\" \"private\" {\n  vpc_id            = aws_vpc.main.id\n  cidr_block        = \"10.0.2.0/24\"       # Faixa de IPs da sub-rede privada\n  availability_zone = \"us-east-1a\"\n  tags = {\n    Name = \"bff-private-subnet\"\n  }\n}\n\n# 4. Internet Gateway para acesso externo da sub-rede p√∫blica\nresource \"aws_internet_gateway\" \"gw\" {\n  vpc_id = aws_vpc.main.id\n  tags = {\n    Name = \"bff-internet-gateway\"\n  }\n}\n\n# 5. Tabela de rota para a sub-rede p√∫blica\nresource \"aws_route_table\" \"public\" {\n  vpc_id = aws_vpc.main.id\n  route {\n    cidr_block = \"0.0.0.0/0\"                     # Rota padr√£o para qualquer destino\n    gateway_id = aws_internet_gateway.gw.id      # Encaminha tr√°fego para o Internet Gateway\n  }\n  tags = {\n    Name = \"bff-public-rt\"\n  }\n}\n\n# 6. Associa√ß√£o da tabela de rota √† sub-rede p√∫blica\nresource \"aws_route_table_association\" \"public_assoc\" {\n  subnet_id      = aws_subnet.public.id\n  route_table_id = aws_route_table.public.id\n}\n```\n\n**Explica√ß√£o do c√≥digo:** Definimos o provedor AWS e, em seguida, um recurso **aws_vpc** com um CIDR **/16**, o que nos d√° um bloco grande de IPs privados para subdividir. Em seguida, criamos duas subnets: uma p√∫blica (com **map_public_ip_on_launch = true**, o que atribui IP p√∫blico automaticamente √†s inst√¢ncias lan√ßadas nela) e uma privada (sem IPs p√∫blicos). A sub-rede p√∫blica ter√° conectividade com a internet por meio de um Internet Gateway anexado √† VPC. Isso √© refletido na tabela de rotas p√∫blica, onde definimos que todo tr√°fego destinado a **0.0.0.0/0** (internet) deve sair pelo **aws_internet_gateway.gw**. Associamos essa tabela de rota √† sub-rede p√∫blica para que a rota tenha efeito l√°. A sub-rede privada, por outro lado, **n√£o** recebe uma rota para o IGW, permanecendo isolada da internet (recursos nela s√≥ comunicam com outros recursos internos da VPC ou via gateways especializados, como NAT Gateway ou endpoints privados, se configurados).\n\nAgora que a base de rede est√° pronta, podemos definir um **Security Group** padr√£o. Security Groups atuam como firewalls em n√≠vel de inst√¢ncia, controlando tr√°fego de entrada e sa√≠da. De acordo com a AWS, _‚Äúum Security Group √© um firewall virtual que permite especificar protocolos, portas e origens IP que podem alcan√ßar suas inst√¢ncias‚Äù_. Vamos criar um grupo de seguran√ßa que permita, por exemplo, acesso SSH (porta 22) e HTTP (porta 80) a inst√¢ncias p√∫blicas, e todas as comunica√ß√µes internas necess√°rias dentro da VPC.\n\n```hcl\n# 7. Grupo de Seguran√ßa base \nresource \"aws_security_group\" \"base\" {\n  name        = \"bff-base-sg\"\n  description = \"Grupo de seguran√ßa base para app BFF\"\n  vpc_id      = aws_vpc.main.id\n\n  # Regras de entrada (ingress)\n  ingress {\n    description = \"Acesso SSH permitido\"\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]    # permitir SSH de qualquer IP (evitar em produ√ß√£o!)\n  }\n  ingress {\n    description = \"HTTP web permitido\"\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]    # permitir acesso HTTP de qualquer IP\n  }\n\n  # Regras de sa√≠da (egress) ‚Äì padr√£o permite toda sa√≠da\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"            # -1 significa todos os protocolos\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"bff-base-sg\"\n  }\n}\n```\n\n**Nota:** As regras acima abrem SSH e HTTP para todas as origens (**0.0.0.0/0**), o que _n√£o √© recomend√°vel em produ√ß√£o_. Fizemos isso apenas para simplificar o acesso durante os testes. Em ambientes reais, restrinja o SSH a IPs espec√≠ficos (por exemplo, somente do seu IP ou VPN) e considere usar HTTPS (porta 443) ao inv√©s de HTTP puro.\n\nCom essa configura√ß√£o de Terraform, nossa Landing Zone est√° definida. Salvando o c√≥digo acima em arquivos (**main.tf**, por exemplo, ou divididos em arquivos l√≥gicos como **vpc.tf**, **subnet.tf**, etc.), estamos prontos para aplicar. Seguem as etapas para executar o Terraform:\n\n1. **Inicializar**: na pasta dos arquivos **.tf**, execute **terraform init**. Isso baixar√° o provedor AWS e preparar√° o ambiente Terraform.\n2. **Plan** (planejar): execute **terraform plan** para ver uma pr√©via dos recursos que ser√£o criados. Revise se tudo parece correto.\n3. **Aplicar**: execute **terraform apply** e confirme digitando \"yes\". O Terraform ent√£o criar√° todos os recursos na AWS conforme definido.\n\nDepois de aplicado, voc√™ pode verificar no console AWS que a VPC e sub-redes foram criadas, assim como o Internet Gateway e o security group. A VPC customizada agora existe em sua conta, isolada do resto da AWS, pronta para receber as inst√¢ncias da nossa aplica√ß√£o BFF.\n\n> ‚ö†Ô∏è **Infraestrutura como C√≥digo (IaC)**  \n> Note como definimos toda a infraestrutura em arquivos de configura√ß√£o em vez de criar manualmente pela console. Essa √© a ess√™ncia da IaC: manter a defini√ß√£o do ambiente em c√≥digo version√°vel, permitindo recria√ß√£o e altera√ß√µes reproduz√≠veis. A AWS inclusive fornece orienta√ß√µes oficiais de como usar Terraform como IaC em seus projetos: _‚ÄúHashiCorp Terraform √© uma ferramenta IaC que ajuda a gerenciar sua infraestrutura... voc√™ define recursos cloud em arquivos de configura√ß√£o que podem ser versionados, reutilizados e compartilhados‚Äù_. Isso traz benef√≠cios de automa√ß√£o, consist√™ncia e auditoria de mudan√ßas no ambiente.\n\nCom a Landing Zone pronta, podemos prosseguir para criar nossa aplica√ß√£o monol√≠tica de exemplo e seus recursos."
  },
  {
    "id": "3577bc27-0f1d-45a1-9fb5-e9eeecb093f6",
    "title": "APP mon√≥lito com Terraform",
    "description": "Neste tutorial, voc√™ aprender√° a provisionar uma aplica√ß√£o BFF monol√≠tica utilizando Terraform. A infraestrutura incluir√° uma √∫nica inst√¢ncia EC2 que hospeda o backend, frontend, banco de dados e API da aplica√ß√£o JavaScript did√°tica. O c√≥digo explica detalhadamente cada recurso e conex√£o, demonstrando como consolidar todos os componentes em uma √∫nica m√°quina para facilitar testes e aprendizado inicial.",
    "tool": "Terraform",
    "level": "intermediario",
    "tags": [
      "mon√≥lito",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/3577bc27-0f1d-45a1-9fb5-e9eeecb093f6",
    "markdown": "## Arquitetura Monol√≠tica vs. Microsservi√ßos\n\nAntes de implementar a aplica√ß√£o, vale entender a diferen√ßa de arquitetura que vamos vivenciar nesta s√©rie de tutoriais. Inicialmente, teremos uma aplica√ß√£o **monol√≠tica**: todos os componentes (frontend, backend, banco de dados e API) executando juntos em uma √∫nica inst√¢ncia de servidor. Posteriormente, migraremos para uma arquitetura de **microsservi√ßos/BFF**, onde cada componente ser√° separado em servi√ßos distintos que se comunicam entre si.\n\nEm termos simples, **mon√≥lito** significa uma √∫nica base de c√≥digo e implanta√ß√£o unificada para todas as fun√ß√µes do sistema. J√° em microsservi√ßos, a aplica√ß√£o √© dividida em pequenos servi√ßos independentes, cada um realizando uma fun√ß√£o de neg√≥cio e se comunicando via APIs. Mon√≥litos s√£o mais f√°ceis de iniciar e implantar no come√ßo, mas podem dificultar escalabilidade e manuten√ß√£o conforme crescem. Microsservi√ßos exigem mais planejamento (definir boundaries de servi√ßos, contratos de API), por√©m permitem escalar ou alterar cada parte de forma independente e mais √°gil.\n\nPara resumir algumas diferen√ßas:\n\n|Aspecto|Mon√≥lito (Tudo em Um)|Microsservi√ßos (BFF)|\n|---|---|---|\n|**Codebase**|√önica, todos os m√≥dulos juntos num s√≥ projeto|M√∫ltiplos projetos/servi√ßos independentes|\n|**Implanta√ß√£o**|Um √∫nico pacote/artefato implantado por vez|Cada servi√ßo implantado separadamente (docker, fun√ß√£o etc)|\n|**Escalabilidade**|Escala a aplica√ß√£o inteira de uma vez (clonar inst√¢ncias)|Escala-se apenas os servi√ßos necess√°rios, individualmente|\n|**Manuten√ß√£o e Atualiza√ß√£o**|Mudan√ßa exige build/teste/deploy de todo o sistema|Mudan√ßas isoladas em um servi√ßo; menor impacto no todo|\n|**Comunica√ß√£o**|Chamadas internas diretas (m√©todos/fun√ß√µes na mem√≥ria)|Vias de comunica√ß√£o com APIs (HTTP/REST, etc.)|\n\n_Tabela: Compara√ß√£o resumida entre arquitetura monol√≠tica e microsservi√ßos._\n\nNo contexto do padr√£o **Backend for Frontend (BFF)**, a ideia √© que cada tipo de cliente (web, mobile, etc.) possa ter um backend dedicado para suas necessidades. Em uma arquitetura tradicional monol√≠tica, um √∫nico backend geral tenta servir diferentes clientes, o que pode levar a **gargalos e ac√∫mulo de responsabilidades**. Ao aplicar BFF, estamos, de certa forma, criando uma **vers√£o especializada de backend** ‚Äì no nosso caso, uma para atender especificamente ao frontend web da nossa aplica√ß√£o de exemplo. Primeiro, por√©m, vamos implementar tudo junto (mon√≥lito) para ent√£o refatorar em BFF + microsservi√ßos.\n\n## Configurando a Inst√¢ncia EC2 Monol√≠tica\n\nCom a Landing Zone criada (VPC, subnets e seguran√ßa), podemos provisionar a inst√¢ncia EC2 que rodar√° a aplica√ß√£o monol√≠tica. Usaremos a **sub-rede p√∫blica** para que possamos acess√°-la diretamente pela internet (para visualizar a interface web e API). Esta inst√¢ncia incluir√°: o servidor de frontend, a l√≥gica de backend (API) e um banco de dados local para persist√™ncia ‚Äì tudo em um s√≥ host.\n\n### Escolha da AMI e tipo de inst√¢ncia\n\nPara simplificar, podemos usar uma AMI do Amazon Linux 2 ou Ubuntu Server (gratuita) e um tipo de inst√¢ncia pequeno, como **t2.micro** (eleg√≠vel para camada gratuita). A Amazon define EC2 como _‚Äúcapacidade de computa√ß√£o escal√°vel sob demanda na nuvem AWS‚Äù_ ‚Äì uma VM flex√≠vel onde instalaremos nosso stack.\n\nNo Terraform, o recurso **aws_instance** ser√° utilizado para criar a inst√¢ncia EC2. Precisamos especificar pelo menos: AMI, tipo, subnet, grupo de seguran√ßa e um **User Data** script para instalar e iniciar nossa aplica√ß√£o automaticamente quando o servidor for lan√ßado. Conforme a documenta√ß√£o AWS, o _user data_ √© um mecanismo para passar scripts que executam na inicializa√ß√£o da inst√¢ncia, permitindo automa√ß√£o da configura√ß√£o inicial. Aproveitaremos isso para instalar depend√™ncias (Node.js, por exemplo) e subir nossa aplica√ß√£o automaticamente.\n\n### User Data ‚Äì Instalando a aplica√ß√£o automaticamente\n\nNossa aplica√ß√£o did√°tica BFF ser√° implementada em JavaScript (Node.js). Para fins de teste, podemos imaginar que ela consiste em:\n\n- Um pequeno servidor web Express (Node.js) que sirva uma p√°gina HTML est√°tica (frontend) e exponha uma API REST simples (endpoint **/api/hello** por exemplo).\n- Um banco de dados local ‚Äì para simplificar, usaremos SQLite (banco de dados em arquivo) ou at√© mesmo um array em mem√≥ria simulando dados, s√≥ para validar funcionamento. (Em um mon√≥lito real poder√≠amos instalar MySQL/PostgreSQL localmente, mas isso aumenta complexidade de script).\n- Testes b√°sicos embutidos (por exemplo, rota **/api/health** que retorna ok, ou um script que verifica se a p√°gina carrega e API responde).\n\nVamos considerar um script de User Data que instala Node.js, baixa ou cont√©m o c√≥digo da nossa aplica√ß√£o e a executa. Uma forma comum √© entregar o c√≥digo via reposit√≥rio Git ou zip. Para fins educacionais, poder√≠amos incluir todo o c√≥digo da aplica√ß√£o dentro do pr√≥prio user data (n√£o √© usual em produ√ß√£o, mas serve para demo). Aqui, resumidamente, criaremos um servidor Node que responde \"Hello World\" em uma p√°gina e via API, para confirmar se est√° tudo funcionando.\n\nAbaixo est√° o Terraform para criar a inst√¢ncia EC2 monol√≠tica:\n\n```hcl\n# 1. Definir variables para AMI e Key Pair (opcionalmente)\nvariable \"ami_id\" {\n  description = \"AMI do Amazon Linux 2 para regi√£o us-east-1\"\n  default     = \"ami-09d56f8956ab235b3\"   # Exemplo de AMI Amazon Linux 2 (us-east-1)\n}\nvariable \"key_name\" {\n  description = \"Nome do Key Pair para acesso SSH\"\n  default     = null  # pode ser preenchido com um key pair existente se desejar SSH\n}\n\n# 2. Criar a inst√¢ncia EC2 monol√≠tica\nresource \"aws_instance\" \"monolith\" {\n  ami                         = var.ami_id\n  instance_type               = \"t2.micro\"\n  subnet_id                   = aws_subnet.public.id\n  vpc_security_group_ids      = [aws_security_group.base.id]\n  key_name                    = var.key_name  # se quiser acessar via SSH usando um key pair\n\n  # Script de inicializa√ß√£o (User Data) para instalar Node.js e rodar a aplica√ß√£o\n  user_data = <<-EOF\n              #!/bin/bash\n              # Atualizar pacotes e instalar Node.js\n              sudo yum update -y\n              # Instala Node.js 16.x (usando gerenciador n ou dnf, dependendo da AMI)\n              curl -sL https://rpm.nodesource.com/setup_16.x | sudo bash -\n              sudo yum install -y nodejs\n\n              # Cria um app Node simples\n              cat > app.js << 'EOL'\n              const http = require('http');\n              const fs = require('fs');\n              const PORT = 80;\n\n              const html = `<html>\n              <head><title>Mon√≥lito BFF</title></head>\n              <body><h1>Bem-vindo ao Mon√≥lito BFF</h1>\n              <p id=\"msg\"></p>\n              <script>\n                fetch('/api/hello').then(res=>res.json()).then(data=>{\n                  document.getElementById('msg').innerText = data.message;\n                });\n              </script>\n              </body></html>`;\n\n              const requestListener = function(req, res) {\n                if (req.url === '/' && req.method === 'GET') {\n                  res.writeHead(200, { 'Content-Type': 'text/html' });\n                  res.end(html);\n                } else if (req.url === '/api/hello' && req.method === 'GET') {\n                  res.writeHead(200, { 'Content-Type': 'application/json' });\n                  res.end(JSON.stringify({ message: \"Ol√° do Mon√≥lito BFF!\" }));\n                } else {\n                  res.writeHead(404);\n                  res.end();\n                }\n              };\n\n              const server = http.createServer(requestListener);\n              server.listen(PORT, () => {\n                console.log('Server running on port', PORT);\n              });\n              EOL\n\n              # Inicia o app node em background\n              node app.js > /home/ec2-user/app.log 2>&1 &\n              EOF\n\n  tags = {\n    Name = \"bff-monolith-instance\"\n  }\n}\n```\n\n**Detalhes importantes do c√≥digo acima:**\n\n- **AMI**: usamos uma vari√°vel **ami_id** com default de uma AMI Amazon Linux 2 (substitua pelo ID correto da regi√£o escolhida). Essa AMI permite usar o gerenciador de pacotes **yum** para instalar Node.js facilmente.\n- **user_data**: No bloco **user_data**, usamos o formato HEREDOC (**<<-EOF** ... **EOF**) para escrever um script bash multi-linha. Esse script atualiza o sistema, instala Node.js, e ent√£o cria um arquivo **app.js** contendo nosso servidor Node embutido (usando **cat > app.js << 'EOL' ... EOL**). O servidor √© bem b√°sico: se requisitado na raiz (**/**), retorna um HTML com uma mensagem de boas-vindas e um script JavaScript no navegador que chama nossa API; se requisitado em **/api/hello**, responde com JSON contendo uma mensagem. Finalmente, o script inicia o servidor Node na porta 80 e sai em background (para continuar rodando ap√≥s o boot).\n- **Porta 80**: Escolhemos rodar na porta 80 para evitar configurar firewall adicional ‚Äî nossa Security Group base j√° permite tr√°fego HTTP (porta 80) de qualquer origem. Assim que a inst√¢ncia iniciar, teoricamente poderemos acessar pelo navegador o seu IP p√∫blico (porta 80) e ver a p√°gina carregando. Essa p√°gina por sua vez far√° uma chamada fetch ao endpoint **/api/hello** da pr√≥pria inst√¢ncia e exibir√° a mensagem recebida.\n- **Key Pair**: Inclu√≠mos opcionalmente a vari√°vel **key_name** e passamos ao recurso. Se voc√™ tiver criado um Key Pair na AWS e deseja acesso SSH direto, defina **var.key_name**. Caso contr√°rio, a inst√¢ncia inicia sem chave SSH (voc√™ n√£o poder√° logar manualmente, apenas via user data e outputs do Terraform).\n\nAp√≥s rodar **terraform apply** para essa configura√ß√£o, o Terraform vai iniciar a inst√¢ncia EC2. A sa√≠da da execu√ß√£o do Terraform provavelmente mostrar√° o ID da inst√¢ncia e o endere√ßo IP p√∫blico dela (a menos que capturemos explicitamente via outputs). Podemos adicionar um **output** para conveni√™ncia:\n\n```hcl\noutput \"monolith_public_ip\" {\n  value = aws_instance.monolith.public_ip\n  description = \"IP p√∫blico da inst√¢ncia monol√≠tica\"\n}\n```\n\nCom o IP em m√£os, teste a aplica√ß√£o:\n\n- Acesse pelo navegador **http://<IP_p√∫blico>/**. Voc√™ deve ver a p√°gina \"Bem-vindo ao Mon√≥lito BFF\" e, ap√≥s alguns instantes, a mensagem carregada via API (ex: \"Ol√° do Mon√≥lito BFF!\").\n- Se preferir via terminal, use **curl**: **curl http://<IP>/api/hello** deve retornar o JSON **{\"message\":\"Ol√° do Mon√≥lito BFF!\"}**.\n- Tamb√©m experimente **curl http://<IP>/** para ver o HTML bruto retornado.\n\nSe esses testes passaram, significa que nossa inst√¢ncia monol√≠tica est√° rodando corretamente: o servidor Node est√° servindo tanto o frontend quanto a API e respondendo √†s solicita√ß√µes, e tudo est√° confinado a um s√≥ host. Internamente, caso tiv√©ssemos usado um banco de dados local, o backend acessaria diretamente arquivos ou socket de banco de dados local ‚Äì sem camadas de rede ou autentica√ß√£o adicionais.\n\n### Verificando Recursos e Coment√°rios\n\nNesta etapa, verifique no console AWS os recursos criados:\n\n- A inst√¢ncia EC2 deve estar na sub-rede p√∫blica da VPC personalizada (veja as propriedades de rede da inst√¢ncia; o ID da VPC e subnet devem corresponder aos criados no Tutorial¬†1).\n- O grupo de seguran√ßa associado deve ser o \"bff-base-sg\", com as regras de porta 22 e 80 abertas conforme definimos.\n- O atributo _User Data_ da inst√¢ncia (visualiz√°vel em **Instance Settings > View/Change User Data** no console) deve conter o script que escrevemos. Conforme a AWS, _o user data √© codificado em base64 e entregue √† inst√¢ncia_, que por sua vez interpreta e executa no boot. Podemos inspecionar no **System Log** da inst√¢ncia (EC2 > Inst√¢ncia > Monitoramento > Logs do sistema) para ver os logs do nosso script (por exemplo, confirma√ß√£o de instala√ß√£o do Node ou erros de sintaxe, se houver).\n\n**Considera√ß√µes sobre o mon√≥lito:** Embora funcional, essa arquitetura tem limita√ß√µes. Todo o sistema depende de uma √∫nica VM: se ela falhar, tudo sai do ar de uma vez. Escalar requer lan√ßar uma segunda inst√¢ncia com tudo (duplicando tamb√©m o banco de dados, o que pode causar inconsist√™ncias se n√£o sincronizado). Al√©m disso, o acoplamento √© alto ‚Äì a equipe de desenvolvimento gerencia front, back e dados num mesmo projeto. Esses problemas motivam a migra√ß√£o para microsservi√ßos. No pr√≥ximo tutorial, come√ßaremos a refatora√ß√£o separando o frontend em seu pr√≥prio componente."
  },
  {
    "id": "8c59bb28-1b14-451b-9973-a447015d7f95",
    "title": "APP BFF Frontend com Terraform",
    "description": "Aqui, o foco √© a cria√ß√£o de um ambiente dedicado para o Frontend da aplica√ß√£o BFF. Utilizando Terraform, o tutorial demonstra como configurar recursos espec√≠ficos para servir o conte√∫do web, como inst√¢ncias dedicadas, load balancers e grupos de seguran√ßa que garantam o acesso ao servi√ßo. Cada trecho de c√≥digo √© comentado para esclarecer a fun√ß√£o de cada item e facilitar a compreens√£o para iniciantes.",
    "tool": "Terraform",
    "level": "intermediario",
    "tags": [
      "frontend",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/8c59bb28-1b14-451b-9973-a447015d7f95",
    "markdown": "## Separando o Frontend do Mon√≥lito\n\nAgora que o mon√≥lito b√°sico est√° rodando, vamos iniciar a transi√ß√£o para a arquitetura BFF com microsservi√ßos. O primeiro passo √© **separar o frontend** (interface do usu√°rio) do backend. Em um mon√≥lito, o frontend (HTML/CSS/JS) geralmente √© servido pelo mesmo servidor que a API. Vamos quebrar essa depend√™ncia criando um servi√ßo dedicado para o frontend.\n\n**O que isso significa?** Em vez de uma √∫nica inst√¢ncia servindo tudo, teremos _duas inst√¢ncias_: uma para o **frontend** (por exemplo, um servidor web est√°tico ou aplica√ß√£o React/Angular compilada servida via Nginx ou Node) e outra para o **backend** (a API e l√≥gica de neg√≥cio). Nesta etapa, o backend ainda permanecer√° acoplado ao banco de dados (a separa√ß√£o do banco vir√° no pr√≥ximo tutorial), mas j√° n√£o ir√° mais entregar arquivos de frontend ‚Äì o frontend separado far√° isso.\n\nEssa mudan√ßa traz benef√≠cios imediatos: o frontend pode ser desenvolvido/desdobrado independentemente, possivelmente escalado separadamente (ex.: mais servidores web se houver muita carga de usu√°rios est√°ticos) e podemos otimizar a entrega de conte√∫do est√°tico (usando CDN, cache etc. futuramente). Do lado do backend, ele expor√° apenas APIs para serem consumidas.\n\nNo nosso projeto de exemplo, o mon√≥lito Node servia tanto HTML quanto a resposta JSON. Vamos ajustar: a inst√¢ncia de frontend servir√° o HTML/JS e far√° requisi√ß√µes para a API do backend (que agora estar√° em outra inst√¢ncia). Essa comunica√ß√£o acontecer√° via HTTP interno na VPC ou via internet? Como ambas inst√¢ncias estar√£o na mesma VPC, podemos configur√°-las para se comunicar internamente (mais seguro e r√°pido). Para isso, garantiremos que o backend tenha um nome DNS interno ou IP conhecido e o frontend o acesse pelo endere√ßo privado.\n\nO diagrama agora fica assim:\n\n![tfFrontend](/tfFrontend.svg)\n\n_Figura: Arquitetura ap√≥s separar o Frontend. O usu√°rio acessa a inst√¢ncia de Frontend (por exemplo, Nginx ou um servidor Node que serve p√°ginas). O frontend se comunica com a API do backend (na outra inst√¢ncia) para obter dados. O backend ainda usa um banco de dados local interno nesta etapa._\n\nObserve que o **Amazon API Gateway** ainda n√£o entrou ‚Äì por enquanto, o frontend se comunica diretamente com o backend. Mais adiante, introduziremos o API Gateway como camada entre eles.\n\n## Provisionando a Inst√¢ncia de Frontend\n\nVamos criar uma nova inst√¢ncia EC2 para hospedar o frontend. Podemos reutilizar grande parte da configura√ß√£o do Terraform do mon√≥lito, com diferen√ßas nas **regras de seguran√ßa** e no **user data**.\n\n### Grupo de seguran√ßa para comunica√ß√£o interna\n\nPrecisamos de uma comunica√ß√£o **frontend -> backend** na porta da API (no exemplo do mon√≥lito, a API rodava na porta 80 tamb√©m, mas vamos ajustar para evitar conflito ‚Äì podemos colocar o backend para rodar na porta 3000, por exemplo, j√° que 80 ser√° usada agora exclusivamente pelo frontend web).\n\nPortanto:\n\n- O **frontend** aceitar√° tr√°fego HTTP (porta 80) de usu√°rios externos (internet) ‚Äì isso j√° t√≠nhamos no SG base.\n- O **backend** aceitar√° tr√°fego HTTP da aplica√ß√£o (porta 3000, por exemplo) **somente** da inst√¢ncia de frontend (n√£o expor porta 3000 para toda internet). Isso √© uma melhora de seguran√ßa: o endpoint de API n√£o precisar√° ficar p√∫blico agora, apenas o frontend acessa ele internamente.\n\nPara implementar isso, podemos criar um novo Security Group para o backend com uma regra de entrada que referencie o SG do frontend como origem. O Terraform permite referenciar grupos de seguran√ßa nas regras (via **source_security_group_id** ou **security_groups** na regra ingress).\n\nVamos definir dois SGs espec√≠ficos:\n\n- **frontend_sg**: permite porta 80 de internet (e possivelmente 443 se fosse HTTPS).\n- **backend_sg**: permite porta 3000 _somente_ do SG do frontend.\n\nAl√©m disso, ambos SGs permitir√£o sa√≠da irrestrita (padr√£o) para permitir que ambos fa√ßam chamadas externas se necess√°rio (ex: backend acessando reposit√≥rios, etc., apesar de idealmente backend n√£o chamaria internet agora).\n\n### Criando os recursos de frontend e backend\n\nNo Terraform, adicionaremos novos resources:\n\n- **aws_security_group.frontend** e **aws_security_group.backend_api**.\n- **aws_instance.frontend** e **aws_instance.backend**.\n\nTamb√©m teremos que decidir o que fazer com a inst√¢ncia monol√≠tica anterior. Em um cen√°rio real, far√≠amos _redeployment_ (desligar mon√≥lito, subir novos servi√ßos). Para fins do tutorial, podemos **reutilizar** a inst√¢ncia monol√≠tica como backend inicialmente (j√° que ela j√° tem a l√≥gica API e DB rodando). Por√©m, para maior clareza, vamos criar uma inst√¢ncia nova de backend tamb√©m, e ent√£o desconsiderar/desligar a monol√≠tica. Isso demonstra desde j√° como seria criar tudo separado via Terraform. (Caso esteja seguindo na pr√°tica, voc√™ pode destruir a inst√¢ncia **aws_instance.monolith** antiga ou simplesmente deix√°-la parada enquanto experimenta as novas.)\n\nAbaixo o c√≥digo Terraform complementar para esta etapa:\n\n```hcl\n# 1. Security Group para o Frontend\nresource \"aws_security_group\" \"frontend\" {\n  name        = \"bff-frontend-sg\"\n  description = \"Permite acesso HTTP p√∫blico ao frontend\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    description = \"HTTP do mundo para frontend\"\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]   # qualquer origem pode acessar o frontend web\n  }\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  tags = { Name = \"bff-frontend-sg\" }\n}\n\n# 2. Security Group para o Backend/API\nresource \"aws_security_group\" \"backend_api\" {\n  name        = \"bff-backend-sg\"\n  description = \"Permite acesso √† API somente do Frontend\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    description      = \"API aberta para Frontend SG\"\n    from_port        = 3000\n    to_port          = 3000\n    protocol         = \"tcp\"\n    security_groups  = [aws_security_group.frontend.id]  # somente inst√¢ncias com sg frontend\n  }\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  tags = { Name = \"bff-backend-sg\" }\n}\n\n# 3. Inst√¢ncia EC2 para o Frontend\nresource \"aws_instance\" \"frontend\" {\n  ami                    = var.ami_id           # reutiliza a mesma AMI do Amazon Linux 2\n  instance_type          = \"t2.micro\"\n  subnet_id              = aws_subnet.public.id\n  vpc_security_group_ids = [aws_security_group.frontend.id, aws_security_group.base.id]\n  key_name               = var.key_name\n\n  user_data = <<-EOF\n              #!/bin/bash\n              sudo yum update -y\n              # Instalar nginx para servir conte√∫do est√°tico (opcional: poder√≠amos usar Node)\n              sudo amazon-linux-extras install -y nginx1\n              sudo systemctl start nginx\n              sudo systemctl enable nginx\n\n              # Criar p√°gina est√°tica de frontend\n              echo \"<h1>Frontend BFF</h1><p>Aplica√ß√£o Frontend funcionando.</p>\" | sudo tee /usr/share/nginx/html/index.html\n\n              # Configurar nginx para passar chamadas /api para o backend\n              cat <<EOT | sudo tee /etc/nginx/conf.d/bff.conf\n              server {\n                listen 80;\n                server_name _;\n                location / {\n                  root /usr/share/nginx/html;\n                }\n                location /api/ {\n                  proxy_pass http://#{aws_instance.backend.private_ip}:3000;\n                }\n              }\n              EOT\n\n              sudo systemctl restart nginx\n              EOF\n\n  tags = { Name = \"bff-frontend-instance\" }\n}\n\n# 4. Inst√¢ncia EC2 para o Backend\nresource \"aws_instance\" \"backend\" {\n  ami                    = var.ami_id\n  instance_type          = \"t2.micro\"\n  subnet_id              = aws_subnet.private.id   # backend na sub-rede privada\n  vpc_security_group_ids = [aws_security_group.backend_api.id]\n  key_name               = var.key_name\n\n  user_data = <<-EOF\n              #!/bin/bash\n              sudo yum update -y\n              # Instalar Node.js (similar ao mon√≥lito)\n              curl -sL https://rpm.nodesource.com/setup_16.x | sudo bash -\n              sudo yum install -y nodejs\n              # C√≥digo b√°sico do backend (porta 3000)\n              cat > backend.js << 'EOL'\n              const http = require('http');\n              const server = http.createServer((req, res) => {\n                if (req.url === '/api/hello' && req.method === 'GET') {\n                  res.writeHead(200, { 'Content-Type': 'application/json' });\n                  res.end(JSON.stringify({ message: \"Ol√° do Backend BFF!\" }));\n                } else {\n                  res.writeHead(404);\n                  res.end();\n                }\n              });\n              server.listen(3000);\n              EOL\n              node backend.js > /home/ec2-user/backend.log 2>&1 &\n              EOF\n\n  tags = { Name = \"bff-backend-instance\" }\n}\n```\n\nAlguns pontos a destacar neste c√≥digo:\n\n- Colocamos o **backend** na sub-rede privada (**aws_subnet.private.id**). Isso significa que ele _n√£o possui IP p√∫blico_. Ele s√≥ ser√° acess√≠vel dentro da VPC. Isso melhora a seguran√ßa do backend, pois n√£o pode ser chamado diretamente da internet. Mesmo para atualiza√ß√µes de pacotes via yum e npm, nossa inst√¢ncia backend conseguir√° acesso? Note que sem um NAT Gateway, inst√¢ncias em sub-rede privada **n√£o t√™m acesso de sa√≠da √† internet**. Nesse caso, nosso script de user data no backend que tenta baixar Node.js do reposit√≥rio externo poderia falhar. Para contornar isso sem adicionar NAT agora, poder√≠amos:\n    \n    - Colocar o backend tamb√©m na sub-rede p√∫blica temporariamente, ou\n    - Aproveitar que nossa VPC por padr√£o deve ter um DNS interno resolvendo endere√ßos do pacote (mas sem rota √† internet n√£o adiantaria).\n    \n    Simplicando: para que o user data do backend funcione corretamente, **vamos trocar e colocar o backend tamb√©m na sub-rede p√∫blica** (ou adicionar um NAT Gateway). Neste tutorial, modificaremos para **subnet_id = aws_subnet.public.id** no backend, com a consci√™ncia de que ele ficar√° com IP p√∫blico (embora n√£o vamos us√°-lo diretamente). _Em um ambiente real,_ preferir√≠amos criar um NAT em nossa Landing Zone e manter backend realmente privado. Caso opte por isso, adicione um NAT Gateway na VPC e uma rota 0.0.0.0/0 na tabela da sub-rede privada apontando para o NAT, para que o user data do backend consiga baixar pacotes.\n    \n- O **nginx** no frontend √© configurado para servir conte√∫do est√°tico e tamb√©m atuar como proxy para requisi√ß√µes **/api/** para o backend. Repare na configura√ß√£o do **proxy_pass**: usamos um placeholder **#{aws_instance.backend.private_ip}**. Em teoria, gostar√≠amos de injetar dinamicamente o IP privado da inst√¢ncia de backend aqui. Entretanto, o Terraform n√£o permite interpola√ß√£o direta dentro do **user_data** porque ele trata o bloco como string literal a ser enviada. Poder√≠amos usar o **templatefile** para compor o user data externamente. Para manter simples, podemos nesta fase editar manualmente ap√≥s deploy ou supor que sabemos o IP do backend. (Numa implementa√ß√£o real, usar√≠amos por exemplo o DNS privado do backend via Route 53 ou Service Discovery do ECS, etc.)\n    \n    Para fins de tutorial, vamos supor o IP privado do backend (ou poder√≠amos rodar o Terraform em duas etapas: criar backend primeiro, anotar IP, depois passar para o script do frontend). J√° que estamos descrevendo conceitualmente, n√£o nos aprofundaremos nesse detalhe ‚Äì mas tenha em mente a import√¢ncia de descobrir o endere√ßo interno do backend para configurar o front. Outra op√ß√£o seria o backend rodar com um nome DNS privado (por exemplo, usando AWS Cloud Map ou simplesmente referenciando o hostname padr√£o da EC2 backend dentro do mesmo VPC). No Amazon Linux 2, cada inst√¢ncia registra no DNS interno um nome estilo **<instance-id>.ec2.internal**. Poder√≠amos usar isso no proxy_pass tamb√©m, bastando pegar o ID ou usar um data source de AWS instances filtrado. Vamos simplificar assumindo IP fixo conhecido para n√£o complicar com templatefile agora.\n    \n- O **security group** do frontend e backend j√° garantem as restri√ß√µes: somente inst√¢ncias com SG do frontend podem atingir a porta 3000 do backend. Isso significa que mesmo nosso backend estando (nesse momento) com IP p√∫blico, ele n√£o aceitar√° requisi√ß√µes de fora naquela porta ‚Äì apenas do frontend. Se tent√°ssemos acessar **http://IP_publico_backend:3000/api/hello** de casa, n√£o ter√≠amos resposta (timeout), pois o firewall bloqueia. Mas se o frontend (dentro da VPC) acessar **http://IP_privado_backend:3000/api/hello**, vai passar.\n    \n\nAp√≥s aplicar essas configura√ß√µes, teremos duas novas inst√¢ncias EC2. Devemos testar:\n\n- **Frontend:** obter seu IP p√∫blico e acessar via navegador **http://<IP-frontend>/**. A p√°gina deve mostrar \"Frontend BFF\" (conforme colocamos no HTML) e se tudo deu certo, a aplica√ß√£o do frontend talvez tente acessar **/api/hello** e obter resposta do backend. Se configuramos corretamente o proxy e o IP do backend, ao acessar a p√°gina do frontend ela deveria exibir algum retorno da API do backend. Podemos verificar via console do navegador ou logs do nginx na inst√¢ncia se o proxy conseguiu encaminhar.\n- **Backend:** mesmo sem acesso externo, podemos testar chamando sua API a partir do frontend ou internamente. Para uma confirma√ß√£o r√°pida, podemos SSH na inst√¢ncia frontend (j√° que abrimos 22 e tem key) e de l√° executar **curl http://<IP_privado_backend>:3000/api/hello**. Deve retornar o JSON **{\"message\":\"Ol√° do Backend BFF!\"}**.\n\nCaso n√£o tenhamos adicionado NAT Gateway, o script de user data do backend pode n√£o ter instalado Node e iniciado corretamente (lembrar da quest√£o do acesso internet). Podemos logar no frontend, depois SSH no backend via sess√£o (ex: usando o front como jump host) ou habilitar temporariamente porta 22 no SG do backend e usar VPN/bastion. Esse n√≠vel de detalhe foge um pouco do tutorial ‚Äì assumiremos que resolvemos essa depend√™ncia de instala√ß√£o.\n\n## Conclus√£o Parcial\n\nConseguimos separar o frontend do backend do nosso aplicativo. Agora, o usu√°rio acessa um servidor dedicado ao frontend e este servidor busca dados do backend via chamadas de API internas. J√° estamos nos beneficiando de uma pequena arquitetura BFF: o frontend poderia ser otimizado independentemente (por exemplo, colocando um cache est√°tico ou CDN), e o backend j√° n√£o precisa servir conte√∫do est√°tico, focando apenas em l√≥gica de neg√≥cio e dados.\n\nNo pr√≥ximo passo, vamos isolar tamb√©m o banco de dados, extraindo-o para um servi√ßo gerenciado (Amazon RDS). Isso completar√° a separa√ß√£o dos principais componentes (frontend, backend, database). Em seguida, integraremos o **Amazon API Gateway** para orquestrar as chamadas de API de forma mais eficiente e segura."
  },
  {
    "id": "fa672bb6-64d4-4569-9bdf-64b786d65194",
    "title": "APP BFF Backend com Terraform",
    "description": "Focado no Backend da aplica√ß√£o BFF, este tutorial mostra como utilizar Terraform para provisionar os componentes que processam a l√≥gica de neg√≥cios, lidam com requisi√ß√µes e se conectam ao banco de dados. S√£o abordadas pr√°ticas para defini√ß√£o de vari√°veis, cria√ß√£o de recursos e integra√ß√£o com outros servi√ßos, garantindo uma arquitetura modular que permite escalabilidade e manuten√ß√£o facilitada.",
    "tool": "Terraform",
    "level": "intermediario",
    "tags": [
      "backend",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/fa672bb6-64d4-4569-9bdf-64b786d65194",
    "markdown": "## Isolando o Banco de Dados e Backend\n\nContinuando nossa evolu√ß√£o arquitetural, agora vamos separar o **banco de dados** do backend e ajustar este para funcionar como um microsservi√ßo independente. At√© o momento, nosso backend BFF ainda estava executando com um banco de dados local (no mon√≥lito ou na pr√≥pria VM backend). Isso pode funcionar em prot√≥tipos, mas em produ√ß√£o √© desej√°vel usar um servi√ßo de banco dedicado, confi√°vel e escal√°vel.\n\nUsaremos o **Amazon RDS (Relational Database Service)** para criar um banco de dados gerenciado. Conforme a AWS, _‚ÄúAmazon RDS √© um servi√ßo web que facilita configurar, operar e escalar um banco de dados relacional na nuvem AWS‚Äù_. O RDS nos poupa do trabalho de instalar software de banco, gerenciar backups, etc., pois oferece recursos como backups automatizados, replica√ß√£o e escalabilidade sob demanda. Tamb√©m melhoramos a seguran√ßa colocando o RDS em subnet privada (sem acesso p√∫blico) e controlando acesso apenas para nosso backend via seguran√ßa de rede.\n\n### Escolhendo o tipo de Banco de Dados\n\nPara uma aplica√ß√£o BFF did√°tica, podemos usar um banco MySQL ou PostgreSQL pequeno no RDS (por exemplo, um db.t2.micro). Vamos optar por **MySQL** para este tutorial, mas os passos seriam similares para Postgres. Precisaremos providenciar alguns detalhes:\n\n- Nome do banco de dados, usu√°rio e senha de admin.\n- Subnet group no RDS para especificar que subnets o banco usar√° (faremos uso das subnets privadas criadas na Landing Zone).\n- Seguran√ßa: um Security Group para o RDS que permita acesso apenas do backend (similar √† ideia anterior, mas RDS n√£o ficar√° em SG de EC2 comum, e sim SG do pr√≥prio RDS).\n\nFaremos:\n\n1. Criar um **RDS Subnet Group** incluindo nossas sub-rede privada (para RDS criar inst√¢ncia dentro delas).\n2. Criar a inst√¢ncia RDS MySQL (definindo engine, vers√£o, tamanho, credenciais).\n3. Criar um Security Group para o RDS aceitar conex√µes na porta 3306 somente da SG do backend.\n4. Atualizar a configura√ß√£o do backend para conectar ao RDS em vez do DB local (isso envolve passar host, porta, user, senha do RDS para o nosso app).\n\nNo Terraform, o recurso √© **aws_db_instance** para o RDS, e **aws_db_subnet_group** para o subnet group.\n\n#### Configurar credenciais de forma segura\n\n√â importante **n√£o expor senhas** no c√≥digo. Podemos usar vari√°veis do Terraform para passar a senha do banco. Idealmente, usar o AWS Secrets Manager ou Parameter Store para armazen√°-las, mas para simplificar, usaremos vari√°vel.\n\nVamos adicionar no Terraform:\n\n```hcl\n# 1. Grupo de sub-rede para RDS (utiliza sub-rede privada existente)\nresource \"aws_db_subnet_group\" \"bff_db_subnets\" {\n  name       = \"bff-db-subnet-group\"\n  subnet_ids = [aws_subnet.private.id]   # usar sub-rede privada para inst√¢ncia do RDS\n  tags = {\n    Name = \"bff-db-subnet-group\"\n  }\n}\n\n# 2. Security Group para o RDS\nresource \"aws_security_group\" \"rds\" {\n  name        = \"bff-rds-sg\"\n  description = \"Permite acesso DB do SG do backend\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    description     = \"MySQL do backend\"\n    from_port       = 3306\n    to_port         = 3306\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.backend_api.id]  # somente backend pode conectar\n  }\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  tags = { Name = \"bff-rds-sg\" }\n}\n\n# 3. Inst√¢ncia RDS MySQL\nvariable \"db_password\" {\n  description = \"Senha do usu√°rio admin do banco de dados\"\n  type        = string\n  sensitive   = true\n}\n\nresource \"aws_db_instance\" \"bffdb\" {\n  allocated_storage    = 20\n  engine               = \"mysql\"\n  engine_version       = \"8.0\"\n  instance_class       = \"db.t3.micro\"\n  identifier           = \"bff-database\"\n  username             = \"admin\"             # usu√°rio administrador\n  password             = var.db_password\n  db_name              = \"bffapp\"            # nome do database inicial\n  publicly_accessible  = false               # n√£o ter IP p√∫blico\n  vpc_security_group_ids = [aws_security_group.rds.id]\n  db_subnet_group_name   = aws_db_subnet_group.bff_db_subnets.name\n\n  # Habilitar deletar sem snapshot para fins de teste (n√£o fa√ßa isso em produ√ß√£o sem backup!)\n  skip_final_snapshot = true\n\n  tags = {\n    Name = \"bff-mysql-db\"\n  }\n}\n```\n\nExplica√ß√£o:\n\n- **aws_db_subnet_group.bff_db_subnets**: agrupa a(s) sub-rede(s) privadas para o RDS usar. Assim garantimos que o banco vai rodar dentro da nossa subnet privada (isolado da internet).\n- **aws_security_group.rds**: firewall do banco. Apenas a porta 3306 (MySQL) liberada e apenas para o SG do backend. Isso significa que somente inst√¢ncias marcadas com **bff-backend-sg** poder√£o chegar no MySQL. Mesmo dentro da VPC, outra inst√¢ncia que n√£o tenha esse SG n√£o conseguir√° conectar (por padr√£o RDS tamb√©m requer SSL ou credenciais, mas essa camada de rede j√° refor√ßa seguran√ßa).\n- **aws_db_instance.bffdb**: cria a inst√¢ncia MySQL. Definimos um tamanho pequeno (20GB storage, classe t3.micro), definimos **publicly_accessible = false** para evitar IP p√∫blico. Passamos o SG e subnet group criados para situar corretamente. A senha vem de **var.db_password** que marcamos como sens√≠vel (assim n√£o aparece em logs do Terraform). Observa√ß√£o: Em produ√ß√£o, recomendamos n√£o codificar senhas no tfvars simples; usar AWS Secrets ou Path no SSM com **data \"aws_ssm_parameter\"** por exemplo. Aqui mantemos simples com vari√°vel.\n\n> ‚ö†Ô∏è **Amazon RDS em sub-rede privada:**  \n> Colocar o banco em sub-rede privada √© uma pr√°tica comum de seguran√ßa. Dessa forma, mesmo se algu√©m descobrisse credenciais, n√£o conseguiria se conectar de fora da AWS, pois o host do DB n√£o √© acess√≠vel externamente. A AWS recomenda utilizar mecanismos como esse em conjunto com IAM para refor√ßar a prote√ß√£o dos dados.\n\n### Ajustando o Backend para usar o RDS\n\nCom o banco pronto, precisamos atualizar nosso servi√ßo backend para conectar-se a ele em vez do banco local. No caso do nosso exemplo, antes o backend respondia com uma mensagem est√°tica. Vamos simular uma query ao banco: por exemplo, ler uma tabela de cumprimentos e devolver um aleat√≥rio. N√£o entraremos em SQL detalhado, mas focaremos em como o backend sabe onde o DB est√°.\n\nPrecisamos passar para o backend:\n\n- **HOST do DB**: o endpoint do RDS (pode ser obtido via **aws_db_instance.bffdb.address**).\n- **Usu√°rio e Senha**: admin e vari√°vel password.\n- **Nome do banco**: \"bffapp\" como definimos.\n\nPara simplificar, podemos passar isso via vari√°veis de ambiente no user_data ou via arquivo de config. Vamos optar por definir no user_data do backend e ajustar o c√≥digo para usar MySQL. Ter√≠amos que instalar um client MySQL (ex: **mysql** CLI ou talvez um driver npm **mysql2**).\n\nPor brevidade, vamos supor que a gente instala o CLI do MySQL e realiza uma consulta simples via comando shell (s√≥ para validar conectividade) quando a inst√¢ncia subir. E podemos alterar a resposta da API para confirmar conex√£o.\n\nAltera√ß√£o no **user_data** do backend:\n\n```hcl\n# Adicionar no user_data do backend (na cria√ß√£o ou via novo apply)\nuser_data = <<-EOF\n            #!/bin/bash\n            sudo yum update -y\n            # Instalar MySQL client e Node.js\n            sudo yum install -y mariadb  # MariaDB client to connect to MySQL\n            curl -sL https://rpm.nodesource.com/setup_16.x | sudo bash -\n            sudo yum install -y nodejs\n\n            # Esperar alguns segundos para garantir que RDS j√° esteja acess√≠vel\n            sleep 30\n\n            # Testar conex√£o com o DB\n            mysql -h ${aws_db_instance.bffdb.address} -u admin -p${var.db_password} -e \"SHOW DATABASES;\" > /home/ec2-user/db_check.txt 2>&1\n\n            # Iniciar servidor Node (exemplo simples)\n            cat > backend.js << 'EOL'\n            const http = require('http');\n            const { execSync } = require('child_process');\n            const server = http.createServer((req, res) => {\n              if (req.url === '/api/dbcheck' && req.method === 'GET') {\n                try {\n                  const result = execSync('mysql -h ${aws_db_instance.bffdb.address} -u admin -p${var.db_password} -e \"SELECT 1;\"');\n                  res.writeHead(200, {'Content-Type': 'application/json'});\n                  res.end(JSON.stringify({ db: \"ok\" }));\n                } catch (err) {\n                  res.writeHead(500, {'Content-Type': 'application/json'});\n                  res.end(JSON.stringify({ db: \"error\", detail: err.message }));\n                }\n              } else {\n                res.writeHead(404);\n                res.end();\n              }\n            });\n            server.listen(3000);\n            EOL\n            node backend.js &\n            EOF\n```\n\n**O que fizemos:**\n\n- Instalamos o cliente **mariadb** para ter o comando **mysql** dispon√≠vel.\n- Colocamos **sleep 30** para aguardar 30 segundos antes de tentar conectar (RDS pode demorar alguns instantes para estar dispon√≠vel ap√≥s cria√ß√£o).\n- Executamos um comando **mysql** para listar databases e redirecionamos sa√≠da para um arquivo **db_check.txt**. Isso serve para debug: se a conex√£o falhou, esse arquivo ter√° o erro.\n- Ajustamos o servidor Node para ter um endpoint **/api/dbcheck** que simplesmente tenta executar um **SELECT 1** no banco (usando **execSync** chamando o cliente mysql) e retorna **{\"db\":\"ok\"}** se sucesso ou erro se falhar. Essa n√£o √© a forma mais elegante (ideal seria usar um driver Node MySQL), mas √© a mais r√°pida aqui para validar conectividade.\n\nCom isso, podemos aplicar a altera√ß√£o. O Terraform vai perceber mudan√ßas no **user_data** do **aws_instance.backend** e aplicar (obs: mudar user_data implica recriar a inst√¢ncia por ser campo que n√£o atualiza em execu√ß√£o; esteja preparado para downtime do backend durante a troca, ou use **user_data_replace_on_change = true** se quiser controlar).\n\nAp√≥s subir, testar do frontend ou via SSH:\n\n- Do frontend (que tem acesso √† internet e, via SG, ao backend): **curl http://<IP_privado_backend>:3000/api/dbcheck**. Dever√° retornar **{\"db\":\"ok\"}** se a conex√£o MySQL do backend foi bem-sucedida. Caso contr√°rio, ver o arquivo **db_check.txt** na inst√¢ncia backend para diagnosticar (pode acusar acesso negado ‚Äì verificar SG, ou usu√°rio/senha errada, ou RDS n√£o pronto).\n- Tamb√©m √© √∫til verificar no console AWS RDS os indicadores: a inst√¢ncia RDS deve estar em status \"available\". Pegue o endpoint endpoint (Endpoint & Port nas informa√ß√µes do RDS) e confira que condiz com o que o Terraform mostra no output (podemos criar um output para endpoint tamb√©m).\n\n### Benef√≠cios da separa√ß√£o do DB\n\nNeste ponto, temos:\n\n- **Frontend:** inst√¢ncia pr√≥pria servindo interface.\n- **Backend:** inst√¢ncia pr√≥pria servindo API e comunicando com DB.\n- **Banco de Dados:** servi√ßo RDS gerenciado, acess√≠vel apenas pelo backend.\n\nEstamos pr√≥ximos de uma arquitetura de microsservi√ßos tradicional. O backend BFF agora √© um servi√ßo stateless (sem armazenamento local, j√° que usa um DB externo). Podemos facilmente escalar esse backend replicando inst√¢ncias (colocando atr√°s de um balanceador de carga, por exemplo) sem risco de diverg√™ncia de dados, pois todos usam o mesmo RDS central.\n\nColocar o DB em RDS traz robustez: backups autom√°ticos, facilidade de failover (se configur√°ssemos Multi-AZ), entre outros benef√≠cios. Tamb√©m isolamos mais as falhas ‚Äì um problema no backend n√£o corrompe o banco t√£o facilmente, e _vice-versa_ o banco pode ser mantido/atualizado independentemente do c√≥digo do backend.\n\nPara fechar essa parte, podemos apagar a inst√¢ncia antiga monol√≠tica se ainda estiver rodando, pois j√° migramos toda funcionalidade para as novas inst√¢ncias + RDS. Agora temos o **BFF Backend** implementado como um microsservi√ßo Node em sua VM, comunicando-se com um DB dedicado.\n\n## Recapitulando a Arquitetura Atual\n\nVamos visualizar como ficou nossa arquitetura ap√≥s este tutorial:\n\n![tfBackend](/tfBackend.svg)\n\n_Figura: Arquitetura com microsservi√ßo backend e banco de dados isolado. O Frontend est√° na subnet p√∫blica respondendo ao usu√°rio; ele chama a API do Backend (que agora roda numa subnet privada). O Backend consulta o banco MySQL no RDS, tamb√©m na subnet privada. Linhas pontilhadas indicam limites de sub-rede e flechas mostram o fluxo de comunica√ß√£o._\n\nCom essa estrutura, j√° atingimos um n√≠vel de arquitetura de microsservi√ßo web: conte√∫do est√°tico e cliente separados do servidor de API e este separado do armazenamento de dados. No pr√≥ximo (e √∫ltimo) tutorial, introduziremos o **Amazon API Gateway** para gerenciar as chamadas para o backend BFF. O API Gateway funcionar√° como uma porta de entrada √∫nica para a API, oferecendo recursos adicionais como autentica√ß√£o, limitaci√≥n de throughput, caching e facilidade de roteamento sem expor diretamente nosso backend. Essa √© uma pr√°tica comum em designs BFF: usar um _gateway_ para orquestrar m√∫ltiplos servi√ßos de backend e apresentar ao frontend uma interface unificada."
  },
  {
    "id": "50fe5d55-025d-4ffe-8a62-cb80b9633367",
    "title": "APP API Gateway Com Terraform",
    "description": "Neste √∫ltimo tutorial da s√©rie Terraform, voc√™ aprender√° a criar um API Gateway que atua como ponto de entrada para a aplica√ß√£o BFF. O m√≥dulo apresentado ilustra a configura√ß√£o de endpoints, integra√ß√µes com os recursos backend e regras de seguran√ßa, possibilitando uma gest√£o centralizada das APIs e o roteamento adequado das requisi√ß√µes para os microsservi√ßos.",
    "tool": "Terraform",
    "level": "intermediario",
    "tags": [
      "API",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/50fe5d55-025d-4ffe-8a62-cb80b9633367",
    "markdown": "## Por que usar um API Gateway?\n\nNa arquitetura atual, o frontend se comunica diretamente com o backend BFF via HTTP interno. Isso funciona, mas introduzir um **API Gateway** traz diversas vantagens em ambientes de microsservi√ßos e BFF. O Amazon API Gateway, segundo a defini√ß√£o oficial, _‚Äú√© um servi√ßo AWS para criar, publicar, manter, monitorar e proteger APIs REST, HTTP e WebSocket em qualquer escala‚Äù_. Ele age como uma ‚Äúporta de entrada‚Äù para suas APIs, lidando com roteamento de requisi√ß√µes, controle de tr√°fego, autentica√ß√£o, monitoramento e versionamento. Na pr√°tica, colocar o API Gateway na frente do nosso backend nos permite:\n\n- **Unificar endpoints**: O frontend passa a chamar um dom√≠nio do API Gateway, sem se preocupar com IPs internos do backend.\n- **Melhorar seguran√ßa**: Podemos habilitar autentica√ß√£o, SSL, throttling, **CORS** e outras prote√ß√µes no Gateway, reduzindo a exposi√ß√£o direta do backend.\n- **Facilitar escalabilidade e mudan√ßas**: Se no futuro substituirmos o backend por outra implementa√ß√£o ou dividirmos em v√°rios microservi√ßos, o frontend n√£o precisa mudar ‚Äì o API Gateway pode rotear para m√∫ltiplos servi√ßos internamente. Isso √© particularmente alinhado ao padr√£o BFF, onde poder√≠amos ter diversos microservi√ßos e um gateway agregando para o cliente.\n- **Observabilidade**: O API Gateway oferece logs e m√©tricas integradas (via CloudWatch) para cada chamada, facilitando monitorar o uso da API.\n\nNo nosso cen√°rio BFF simples (um frontend e um backend), o API Gateway pode parecer opcional. Mas didaticamente, vamos us√°-lo para demonstrar essas capacidades e nos preparar para futuros crescimentos.\n\n## Implementando o Amazon API Gateway via Terraform\n\nO API Gateway pode ser configurado de diversas formas. Podemos criar uma API REST (v1) ou uma API HTTP (v2) mais simples. Aqui usaremos a API REST tradicional para ilustrar mais op√ß√µes. Precisaremos configurar:\n\n- Um **Rest API** em si (nome, descri√ß√£o).\n- Um **Recurso** (path) e m√©todos. Podemos usar um recurso proxy gen√©rico (**/{proxy+}**) com m√©todo ANY para encaminhar tudo, ou definir rotas espec√≠ficas.\n- Uma **Integra√ß√£o** do m√©todo com nosso backend. Vamos usar integra√ß√£o HTTP (HTTP_PROXY) apontando para o endpoint do backend.\n- Um **Deployment** e **Stage** para tornar a API acess√≠vel em uma URL p√∫blica.\n\nComo nosso backend BFF est√° rodando em uma inst√¢ncia EC2, para o API Gateway alcan√ß√°-lo temos duas possibilidades:\n\n1. **Integra√ß√£o via VPC Link**: Configurar o API Gateway para acessar um alvo dentro da VPC (geralmente precisa de Network Load Balancer apontando para a inst√¢ncia ou servi√ßo).\n2. **Integra√ß√£o HTTP p√∫blica**: Expor o backend publicamente e deixar o API Gateway cham√°-lo via internet.\n\nA op√ß√£o 1 √© a ideal em produ√ß√£o (mant√©m backend privado). Mas configura√ß√µes de VPC Link e NLB seriam complexas para este tutorial. Dado que nosso backend _temporariamente_ est√° com IP p√∫blico (pois n√£o adicionamos NAT), podemos aproveitar isso para simplificar: faremos o API Gateway chamar o endpoint HTTP p√∫blico do backend. **Aten√ß√£o**: Isso significa que, se fizemos o backend realmente privado, precisar√≠amos alterar para p√∫blico ou usar NLB. Para fins did√°ticos, vamos supor que o backend tem um DNS p√∫blico acess√≠vel e a porta 3000 aberta para o API Gateway. Para n√£o abrir geral, podemos permitir no SG do backend acesso da API Gateway (mas os IPs fonte do Gateway n√£o s√£o fixos facilmente). Ent√£o, leve em conta que isto √© uma concess√£o para did√°tica; em um deploy profissional far√≠amos a integra√ß√£o privada.\n\n### Configura√ß√£o no Terraform\n\nVamos definir os recursos do API Gateway (vers√£o REST):\n\n```hcl\n# 1. Cria√ß√£o da API Gateway REST\nresource \"aws_api_gateway_rest_api\" \"bff_api\" {\n  name        = \"BFFTutorialAPI\"\n  description = \"API Gateway para backend BFF\"\n}\n\n# 2. Recurso proxy {proxy+} no API Gateway (para encaminhar qualquer path para o backend)\nresource \"aws_api_gateway_resource\" \"proxy\" {\n  rest_api_id = aws_api_gateway_rest_api.bff_api.id\n  parent_id   = aws_api_gateway_rest_api.bff_api.root_resource_id\n  path_part   = \"{proxy+}\"\n}\n\n# 3. M√©todo ANY no recurso proxy (aceita todos m√©todos HTTP)\nresource \"aws_api_gateway_method\" \"any\" {\n  rest_api_id   = aws_api_gateway_rest_api.bff_api.id\n  resource_id   = aws_api_gateway_resource.proxy.id\n  http_method   = \"ANY\"\n  authorization = \"NONE\"  # sem auth por enquanto (p√∫blico)\n  request_parameters = {\n    \"method.request.path.proxy\" = true  # permite passar adiante o proxy path\n  }\n}\n\n# 4. Integra√ß√£o HTTP Proxy com nosso backend\nresource \"aws_api_gateway_integration\" \"http_proxy\" {\n  rest_api_id = aws_api_gateway_rest_api.bff_api.id\n  resource_id = aws_api_gateway_resource.proxy.id\n  http_method = aws_api_gateway_method.any.http_method\n  type        = \"HTTP_PROXY\"\n  integration_http_method = \"ANY\"\n  uri         = \"http://${aws_instance.backend.public_dns}:3000/{proxy}\"\n  # Mapeia par√¢metros de caminho para o proxy\n  request_parameters = {\n    \"integration.request.path.proxy\" = \"method.request.path.proxy\"\n  }\n}\n\n# 5. Deployment da API\nresource \"aws_api_gateway_deployment\" \"bff_api_dep\" {\n  depends_on = [aws_api_gateway_integration.http_proxy]  # garante integra√ß√£o criada antes\n  rest_api_id = aws_api_gateway_rest_api.bff_api.id\n  stage_name  = \"dev\"\n  description = \"Deploy inicial da API BFF\"\n}\n```\n\nAqui:\n\n- Criamos a API e um recurso **{proxy+}** sob o root (ou seja, qualquer path depois do **/** root).\n- Definimos o m√©todo ANY no recurso proxy, sem autentica√ß√£o. A configura√ß√£o **method.request.path.proxy = true** e na integra√ß√£o **integration.request.path.proxy = method.request.path.proxy** √© necess√°ria para o API Gateway entender e repassar os segmentos do path adequadamente.\n- A integra√ß√£o HTTP_PROXY aponta para **http://${aws_instance.backend.public_dns}:3000/{proxy}**. Ou seja, se algu√©m chamar **GET /hello** na API Gateway, ele far√° um GET em **http://<DNS-do-backend>:3000/hello**. Estamos usando o DNS p√∫blico do backend (**public_dns**) ‚Äì poderia ser IP tamb√©m. Esse DNS √© algo como **ec2-3-XX-XXX-XX.compute-1.amazonaws.com**. Como deixamos o backend com IP p√∫blico, isso funciona. (Novamente: em produ√ß√£o far√≠amos diferente).\n- Depois fazemos o deployment vinculando tudo no stage \"dev\". Isso cria a URL acess√≠vel, do formato **https://{api-id}.execute-api.{region}.amazonaws.com/dev**.\n\nPodemos adicionar outputs para a URL base da API:\n\n```hcl\noutput \"api_invoke_url\" {\n  value = \"${aws_api_gateway_deployment.bff_api_dep.invoke_url}\"\n  description = \"URL de invoca√ß√£o do API Gateway\"\n}\n```\n\n(Ou construir manualmente: **https://${aws_api_gateway_rest_api.bff_api.id}.execute-api.${var.region}.amazonaws.com/dev**.)\n\n### Testando o API Gateway\n\nDepois de **terraform apply**, obtenha a URL do API (pela sa√≠da ou AWS Console). Vamos supor algo como:\n\n```\nhttps://abc123.execute-api.us-east-1.amazonaws.com/dev\n```\n\nNosso m√©todo proxy ANY significa que qualquer caminho ap√≥s **/dev** ser√° encaminhado. Ent√£o:\n\n- Acessar **GET https://abc123.execute-api.us-east-1.amazonaws.com/dev/api/hello** deve acionar o gateway, que por sua vez chamar√° **http://backend:3000/api/hello**. Se nosso backend estiver rodando e respondendo, deveremos receber o JSON **{\"message\":\"Ol√° do Backend BFF!\"}** como resposta via gateway.\n- Podemos testar tamb√©m o **/api/dbcheck** se mantivemos no backend ‚Äì seria **.../dev/api/dbcheck**.\n\nSe tudo deu certo, o frontend agora pode ser ajustado para usar essa URL do API Gateway em vez de chamar o backend diretamente. No nosso nginx config do Tutorial¬†3, poder√≠amos alterar o **proxy_pass** para apontar para o endpoint p√∫blico do API Gateway (ou simplesmente, no c√≥digo front-end JS, usar o endpoint do API gateway). Por exemplo, no HTML do frontend, mudar:\n\n```js\nfetch('/api/hello')\n```\n\npara\n\n```js\nfetch('https://abc123.execute-api.us-east-1.amazonaws.com/dev/api/hello')\n```\n\nAssim o frontend chamaria a API Gateway (observa√ß√£o: precisamos habilitar CORS no API Gateway para permitir o dom√≠nio do frontend ‚Äì podemos configurar isso adicionando um m√©todo OPTIONS e cabe√ßalhos de resposta. Por brevidade, podemos habilitar CORS pelo console nas configura√ß√µes do recurso, ou suprimir a restri√ß√£o se acessarmos frontend e API pelo mesmo dom√≠nio com CloudFront, etc. CORS √© um detalhe importante: o API Gateway pode adicionar cabe√ßalhos **Access-Control-Allow-Origin** facilmente se configurado. Se o seu frontend for servir a partir de um dom√≠nio diferente do domain da API, ser√° necess√°rio configurar as respostas do Gateway para incluir o header de permiss√£o do dom√≠nio do frontend).\n\n### Revis√£o dos Benef√≠cios e Conclus√£o da Infraestrutura BFF\n\nAgora temos a arquitetura BFF finalizada:\n\n![tfAPI](/tfAPI.svg)\n\n_Figura: Arquitetura final com API Gateway. O frontend chama o API Gateway em vez de acessar diretamente o backend. O API Gateway encaminha as requisi√ß√µes para o backend BFF, que interage com o banco de dados conforme necess√°rio. Todas as partes (frontend, backend, db) est√£o separadas e gerenciadas._\n\nCom essa infraestrutura:\n\n- O **usu√°rio** obt√©m o frontend (p√°gina web) a partir do servidor dedicado ou CDN.\n- O **frontend** realiza chamadas AJAX para o **Amazon API Gateway** (por exemplo, **/dev/api/hello**).\n- O **API Gateway** recebe a chamada, aplica pol√≠ticas (limites, autentica√ß√£o se houvesse, etc.), e encaminha para o endpoint do **backend BFF**.\n- O **backend BFF** processa, consulta o **banco de dados** RDS se necess√°rio, e devolve a resposta.\n- O API Gateway ent√£o retorna a resposta ao frontend.\n\nEssa implementa√ß√£o ilustra o padr√£o **Backend for Frontend**: um backend (nosso servi√ßo Node) otimizado para servir um determinado frontend, com a ajuda de um API Gateway atuando como fachada. Se tiv√©ssemos outros frontends (por exemplo aplicativo mobile), poder√≠amos criar outro backend BFF espec√≠fico e expor via o mesmo API Gateway sob outro path ou at√© outro gateway.\n\n**Refer√™ncias oficiais e documenta√ß√£o:** Aproveitando esse exerc√≠cio, vale consultar a documenta√ß√£o da AWS sobre API Gateway para entender recursos avan√ßados. Por exemplo, o API Gateway pode lidar com autentica√ß√£o via Amazon Cognito ou Lambda Authorizers, e suporta deployment de diferentes stages (dev, prod) e versionamento de APIs. Tamb√©m revisite a se√ß√£o de _Backends for Frontends pattern_ no blog da AWS, que explica a motiva√ß√£o de ter backends segmentados por experi√™ncia de usu√°rio ‚Äì exatamente o que constru√≠mos.\n\n## Passos Finais e Limpeza\n\nParab√©ns! Voc√™ construiu uma infraestrutura completa de uma aplica√ß√£o seguindo o padr√£o BFF usando Terraform. Repassando os tutoriais, n√≥s:\n\n- **Tutorial 1:** Criamos a base (Landing Zone) com rede (VPC, subnets) e seguran√ßa.\n- **Tutorial 2:** Lan√ßamos a aplica√ß√£o monol√≠tica simples em uma inst√¢ncia EC2, confirmando funcionamento end-to-end.\n- **Tutorial 3:** Separarmos o frontend em sua pr√≥pria inst√¢ncia, iniciando a transi√ß√£o para microsservi√ßos.\n- **Tutorial 4:** Isolamos o backend e movemos o banco de dados para um servi√ßo gerenciado (RDS), completando a separa√ß√£o dos componentes.\n- **Tutorial 5:** Implementamos o Amazon API Gateway na frente da API, atingindo a arquitetura alvo BFF com frontend simplificado e uma camada de API robusta.\n\nPara finalizar, n√£o esque√ßa de desligar/remover recursos se este ambiente for apenas de teste, para evitar custos inesperados (RDS e inst√¢ncias EC2 geram cobran√ßas enquanto rodando). Voc√™ pode usar **terraform destroy** para apagar todos os recursos criados ‚Äì gra√ßas ao Terraform, a limpeza √© mais f√°cil e rastre√°vel. Em ambientes reais, manter a infraestrutura codificada permite recriar semelhantes setups em diferentes contas (dev/staging/prod), garantindo consist√™ncia.\n\nEsperamos que esta s√©rie tenha demonstrado de forma pr√°tica como utilizar **Infraestrutura como C√≥digo** com Terraform na AWS, passando de um simples mon√≥lito a uma arquitetura moderna com microsservi√ßos e BFF. Cada passo foi complementado com explica√ß√µes e refer√™ncias oficiais (AWS e Terraform) para embasar as decis√µes. A partir daqui, voc√™ pode expandir o projeto ‚Äì por exemplo, adicionando auto scaling, usando ECS ou Lambda para o backend, integrando autentica√ß√£o de usu√°rios, etc. A base constru√≠da (Landing Zone e padr√µes de m√≥dulos Terraform) pode ser reutilizada nesses incrementos. Boa codifica√ß√£o de infra e at√© a pr√≥xima!"
  },
  {
    "id": "dfc9df29-0fb7-4d2f-a779-a1cc416156b7",
    "title": "Landing Zone com Cloudformation",
    "description": "Este tutorial mostra como construir uma Landing Zone utilizando AWS CloudFormation. Voc√™ aprender√° a organizar sua conta AWS com uma configura√ß√£o de rede robusta, definindo VPC, sub-redes (p√∫blicas e privadas), gateways de Internet/NAT, e pol√≠ticas de seguran√ßa por meio de templates. Essa base √© essencial para garantir autonomia, seguran√ßa e governan√ßa na implanta√ß√£o de uma aplica√ß√£o BFF, tanto no formato monol√≠tico quanto em microsservi√ßos.",
    "tool": "CloudFormation",
    "level": "intermediario",
    "tags": [
      "Landing Zone",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/dfc9df29-0fb7-4d2f-a779-a1cc416156b7",
    "markdown": "Neste primeiro tutorial, vamos preparar a **Landing Zone**, ou seja, a base de infraestrutura em nuvem onde a aplica√ß√£o ser√° implantada. Em termos simples, isso significa criar a rede (VPC), sub-redes p√∫blicas e privadas, e os componentes de rede necess√°rios para suportar tanto a vers√£o monol√≠tica quanto a vers√£o baseada em microsservi√ßos da nossa aplica√ß√£o. A Landing Zone garante que come√ßaremos com um ambiente de nuvem organizado, isolado e seguro para construir as pr√≥ximas etapas.\n\n### 1.1 Vis√£o Geral da Arquitetura de Rede\n\nNosso objetivo √© configurar uma **VPC (Virtual Private Cloud)** pr√≥pria, separada da VPC padr√£o, com sub-redes p√∫blicas e privadas. Isso permitir√° controlar completamente o tr√°fego de rede da aplica√ß√£o. Em uma **subnet (sub-rede)** p√∫blica colocaremos recursos que precisam de acesso direto √† internet (como servidores web ou gateways), enquanto em subnets privadas colocaremos recursos internos (como bancos de dados) para mant√™-los isolados do acesso externo. Em outras palavras, *uma subnet √© um intervalo de endere√ßos IP dentro da VPC no qual podemos lan√ßar recursos AWS espec√≠ficos (por exemplo, inst√¢ncias EC2)*.\n\nNa borda da VPC, usaremos um **Internet Gateway (IGW)** para permitir comunica√ß√£o entre a VPC e a internet p√∫blica. O IGW √© um componente altamente dispon√≠vel que atua como ponte entre a VPC e a internet, permitindo que inst√¢ncias com IP p√∫blico enviem e recebam tr√°fego da internet. Cada subnet p√∫blica ter√° uma rota apontando para o IGW, para que inst√¢ncias nessas subnets possam receber tr√°fego externo. J√° para as subnets privadas, configuraremos um **NAT Gateway** em cada zona de disponibilidade. O NAT Gateway fornece **Network Address Translation**, permitindo que inst√¢ncias em subnets privadas iniciem conex√µes para fora (por exemplo, baixar atualiza√ß√µes ou acessar servi√ßos externos) sem serem acess√≠veis da internet. Com o NAT, recursos privados podem consumir APIs e atualizar pacotes, mas continuam inacess√≠veis de fora (o tr√°fego externo n√£o pode iniciar conex√£o com eles). \n\nResumo dos componentes planejados na Landing Zone:\n\n- **VPC customizada:** intervalo IPv4, DNS habilitado.\n- **Subnets p√∫blicas** (duas, uma em cada AZ) para recursos expostos (por exemplo, servidor web, API Gateway).\n- **Subnets privadas** (duas) para recursos internos (por exemplo, banco de dados).\n- **Internet Gateway:** permite tr√°fego internet nas subnets p√∫blicas.\n- **NAT Gateways:** um em cada subnet p√∫blica, para sa√≠da de inst√¢ncias das subnets privadas.\n- **Route Tables:** tabela de rotas p√∫blica (rota padr√£o para IGW) associada √†s subnets p√∫blicas; tabelas privadas (rota padr√£o para NAT) associadas √†s subnets privadas.\n- **Par√¢metros configur√°veis:** cidr da VPC, blocos cidr de subnets, IDs de AZ etc., para tornar a infraestrutura reutiliz√°vel em diferentes regi√µes.\n\nAbaixo est√° um diagrama resumindo a arquitetura de rede que iremos criar:\n\n![cfLandingzone](/cfLandingzone.svg)\n\n*Figura 1 ‚Äì Diagrama da VPC com subnets p√∫blicas/privadas, Internet Gateway e NAT Gateways.* (Componentes duplicados em duas zonas de disponibilidade para alta disponibilidade).\n\n### 1.2 Criando a VPC e Subnets via CloudFormation\n\nCompreendida a arquitetura, vamos construir tudo usando **AWS CloudFormation** para infraestrutura como c√≥digo. CloudFormation nos permite descrever os recursos em um template (YAML, neste caso) e implant√°-los de forma reproduz√≠vel. Iniciaremos definindo a VPC e, em seguida, as subnets.\n\nNo template YAML, configuramos a VPC com propriedades importantes como o bloco CIDR e op√ß√µes de DNS. Habilitamos **EnableDnsSupport** e **EnableDnsHostnames** para que inst√¢ncias na VPC recebam nomes DNS, facilitando acesso via nome ao inv√©s de IP. *Por padr√£o, uma VPC customizada n√£o atribui hostname DNS √†s inst√¢ncias, a menos que essas op√ß√µes estejam ativadas.* Mantemos **InstanceTenancy** padr√£o como **default** (hardware compartilhado) para evitar custos adicionais.\n\nAbaixo est√° o trecho do template para a VPC e subnets, incluindo coment√°rios explicativos:\n\n```yaml\nAWSTemplateFormatVersion: 2010-09-09\nDescription: \"Landing Zone - VPC with public/private subnets, IGW, and NAT\"\n\nParameters:\n  VpcCidr:\n    Description: \"Bloco CIDR da VPC\"\n    Type: String\n    Default: 10.0.0.0/16\n  PublicSubnet1Cidr:\n    Description: \"CIDR da Subnet P√∫blica 1\"\n    Type: String\n    Default: 10.0.2.0/24\n  PublicSubnet2Cidr:\n    Description: \"CIDR da Subnet P√∫blica 2\"\n    Type: String\n    Default: 10.0.3.0/24\n  PrivateSubnet1Cidr:\n    Description: \"CIDR da Subnet Privada 1\"\n    Type: String\n    Default: 10.0.0.0/24\n  PrivateSubnet2Cidr:\n    Description: \"CIDR da Subnet Privada 2\"\n    Type: String\n    Default: 10.0.1.0/24\n\nResources:\n  # VPC principal da Landing Zone\n  AppVPC:\n    Type: AWS::EC2::VPC\n    Properties:\n      CidrBlock: !Ref VpcCidr\n      EnableDnsSupport: true            # Habilita resolu√ß√£o DNS dentro da VPC (necess√°rio p/ DNS p√∫blico)\n      EnableDnsHostnames: true          # Garante que inst√¢ncias recebam hostname DNS p√∫blico se tiverem IP p√∫blico\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-VPC\"\n\n  # Subnet P√∫blica em AZ 1\n  PublicSubnetA:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref AppVPC\n      CidrBlock: !Ref PublicSubnet1Cidr\n      AvailabilityZone: !Select [0, !GetAZs ]   # pega a primeira AZ da regi√£o\n      MapPublicIpOnLaunch: true                # Inst√¢ncias lan√ßadas aqui recebem IP p√∫blico automaticamente\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PublicSubnetA\"\n\n  # Subnet P√∫blica em AZ 2\n  PublicSubnetB:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref AppVPC\n      CidrBlock: !Ref PublicSubnet2Cidr\n      AvailabilityZone: !Select [1, !GetAZs ]\n      MapPublicIpOnLaunch: true\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PublicSubnetB\"\n\n  # Subnet Privada em AZ 1\n  PrivateSubnetA:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref AppVPC\n      CidrBlock: !Ref PrivateSubnet1Cidr\n      AvailabilityZone: !Select [0, !GetAZs ]\n      MapPublicIpOnLaunch: false               # Subnet privada N√ÉO atribui IP p√∫blico √†s inst√¢ncias\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PrivateSubnetA\"\n\n  # Subnet Privada em AZ 2\n  PrivateSubnetB:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref AppVPC\n      CidrBlock: !Ref PrivateSubnet2Cidr\n      AvailabilityZone: !Select [1, !GetAZs ]\n      MapPublicIpOnLaunch: false\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PrivateSubnetB\"\n```\n\n**Explicando o c√≥digo acima:** Definimos par√¢metros para permitir ajustar os blocos CIDR conforme necessidade. Em **AppVPC**, usamos **!Ref VpcCidr** para definir o espa√ßo de endere√ßos (padr√£o 10.0.0.0/16). Ativamos explicitamente **EnableDnsSupport** e **EnableDnsHostnames** para que a resolu√ß√£o de nomes e atribui√ß√£o de hostname p√∫blico funcionem dentro da VPC (isso √© **fundamental para inst√¢ncias nas subnets p√∫blicas serem acess√≠veis por nome DNS p√∫blico**, facilitando testes e integra√ß√µes). As subnets recebem um nome pelo tag e cada uma √© mapeada para uma AZ diferente usando a fun√ß√£o **!GetAZs** ‚Äì isso garante alta disponibilidade distribuindo recursos entre zonas. Em subnets p√∫blicas, **MapPublicIpOnLaunch: true** faz com que qualquer EC2 lan√ßada ali ganhe automaticamente um IP p√∫blico (economiza ter que criar Elastic IP manualmente para cada inst√¢ncia p√∫blica). J√° nas subnets privadas usamos **false** para evitar IPs p√∫blicos ‚Äì recursos privados devem permanecer inacess√≠veis externamente, seguindo boas pr√°ticas de seguran√ßa.\n\n### 1.3 Internet Gateway, Tabelas de Rotas e NAT Gateway\n\nCom a VPC e as subnets definidas, precisamos agora permitir conectividade adequada. Primeiro, criaremos e anexaremos um **Internet Gateway** √† VPC. Em seguida, configuraremos as **Route Tables** (tabelas de rotas) para direcionar tr√°fego de sa√≠da corretamente: a tabela de rota das subnets p√∫blicas ter√° um destino padr√£o (0.0.0.0/0) apontando para o IGW, enquanto as tabelas das subnets privadas ter√£o rota padr√£o apontando para um NAT Gateway.\n\nInclu√≠mos no template a cria√ß√£o do Internet Gateway e uma attachement dele √† VPC. Depois, criamos duas tabelas de rotas ‚Äì uma ser√° associada a **ambas** subnets p√∫blicas (podemos usar uma s√≥ tabela para todas as p√∫blicas) e cada subnet privada ter√° sua pr√≥pria tabela com rota via NAT (usamos duas NATs e duas tabelas privadas, uma por AZ, para alta disponibilidade).\n\n```yaml\n  # Internet Gateway para acesso √† internet\n  InternetGateway:\n    Type: AWS::EC2::InternetGateway\n    Properties:\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-IGW\"\n\n  # Anexa o IGW √† VPC\n  AttachGateway:\n    Type: AWS::EC2::VPCGatewayAttachment\n    Properties:\n      VpcId: !Ref AppVPC\n      InternetGatewayId: !Ref InternetGateway\n\n  # Tabela de rota para Subnets P√∫blicas (compartilhada)\n  PublicRouteTable:\n    Type: AWS::EC2::RouteTable\n    Properties:\n      VpcId: !Ref AppVPC\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PublicRoutes\"\n\n  # Rota padr√£o na tabela p√∫blica apontando para o IGW (internet)\n  PublicRoute:\n    Type: AWS::EC2::Route\n    Properties:\n      RouteTableId: !Ref PublicRouteTable\n      DestinationCidrBlock: 0.0.0.0/0        # qualquer destino\n      GatewayId: !Ref InternetGateway        # sai pelo Internet Gateway\n\n  # Associa√ß√£o das subnets p√∫blicas √† tabela de rota p√∫blica\n  PublicSubnetARouteTableAssoc:\n    Type: AWS::EC2::SubnetRouteTableAssociation\n    Properties:\n      SubnetId: !Ref PublicSubnetA\n      RouteTableId: !Ref PublicRouteTable\n\n  PublicSubnetBRouteTableAssoc:\n    Type: AWS::EC2::SubnetRouteTableAssociation\n    Properties:\n      SubnetId: !Ref PublicSubnetB\n      RouteTableId: !Ref PublicRouteTable\n\n  # NAT Gateway na Subnet P√∫blica A \n  NatGatewayA:\n    Type: AWS::EC2::NatGateway\n    Properties:\n      AllocationId: !GetAtt NatEIPA.AllocationId   # Aloca IP el√°stico para o NAT\n      SubnetId: !Ref PublicSubnetA\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-NAT-A\"\n\n  # Elastic IP para NAT A \n  NatEIPA:\n    Type: AWS::EC2::EIP\n    DependsOn: AttachGateway                     # IGW deve existir para EIP ser alocado em VPC\n    Properties:\n      Domain: vpc\n\n  # NAT Gateway na Subnet P√∫blica B \n  NatGatewayB:\n    Type: AWS::EC2::NatGateway\n    Properties:\n      AllocationId: !GetAtt NatEIPB.AllocationId\n      SubnetId: !Ref PublicSubnetB\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-NAT-B\"\n\n  NatEIPB:\n    Type: AWS::EC2::EIP\n    DependsOn: AttachGateway\n    Properties:\n      Domain: vpc\n\n  # Tabela de rota para Subnet Privada A\n  PrivateRouteTableA:\n    Type: AWS::EC2::RouteTable\n    Properties:\n      VpcId: !Ref AppVPC\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PrivateRoutesA\"\n  # Rota padr√£o privada apontando para NAT A\n  PrivateRouteA:\n    Type: AWS::EC2::Route\n    Properties:\n      RouteTableId: !Ref PrivateRouteTableA\n      DestinationCidrBlock: 0.0.0.0/0\n      NatGatewayId: !Ref NatGatewayA\n  # Associa√ß√£o Subnet Privada A -> Tabela privada\n  PrivateSubnetARouteTableAssoc:\n    Type: AWS::EC2::SubnetRouteTableAssociation\n    Properties:\n      SubnetId: !Ref PrivateSubnetA\n      RouteTableId: !Ref PrivateRouteTableA\n\n  # Tabela de rota para Subnet Privada B\n  PrivateRouteTableB:\n    Type: AWS::EC2::RouteTable\n    Properties:\n      VpcId: !Ref AppVPC\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-PrivateRoutesB\"\n  PrivateRouteB:\n    Type: AWS::EC2::Route\n    Properties:\n      RouteTableId: !Ref PrivateRouteTableB\n      DestinationCidrBlock: 0.0.0.0/0\n      NatGatewayId: !Ref NatGatewayB\n  PrivateSubnetBRouteTableAssoc:\n    Type: AWS::EC2::SubnetRouteTableAssociation\n    Properties:\n      SubnetId: !Ref PrivateSubnetB\n      RouteTableId: !Ref PrivateRouteTableB\n```\n\n**Detalhes importantes:** \n\n- O recurso **VPCGatewayAttachment** (**AttachGateway**) fixa o Internet Gateway na VPC. Somente com isso as rotas para o IGW tornam-se v√°lidas. O IGW permite que recursos **com IPs p√∫blicos na VPC** sejam alcan√ß√°veis de fora e tamb√©m fa√ßam acesso de sa√≠da √† internet. Sem um IGW, nossas inst√¢ncias p√∫blicas ficariam isoladas apesar de terem IP p√∫blico.\n- Criamos uma route table p√∫blica e associamos as duas subnets p√∫blicas a ela (usando **SubnetRouteTableAssociation**). Dentro dela, definimos uma rota padr√£o (**0.0.0.0/0**) apontando para o IGW, efetivamente dizendo: \"*todo tr√°fego n√£o local deve sair pela internet*\".\n- Para as subnets privadas, usamos **NAT Gateways**. Um NAT Gateway √© colocado *dentro* de uma subnet p√∫blica (pois ele mesmo precisa de internet) e recebe um Elastic IP pr√≥prio (garantindo IP fixo na sa√≠da). Configuramos dois NATs (NAT A e NAT B) ‚Äì assim, se uma zona de AZ falhar, a outra ainda tem um NAT funcional. Cada NAT tem uma route table privada apontando para ele. Com isso, inst√¢ncias na Subnet Privada A ter√£o seu tr√°fego roteado via NAT A (que est√° na Subnet P√∫blica A), e de forma semelhante para B. O efeito √©: inst√¢ncias privadas conseguem acessar a internet (via NAT), mas n√£o possuem IP p√∫blico, ent√£o nada na internet inicia conex√£o a elas. Isso segue o princ√≠pio de manter servidores de aplica√ß√£o ou bancos de dados *privados e seguros*, permitindo apenas o tr√°fego de sa√≠da que eles precisem iniciar.\n\n- Usamos **DependsOn: AttachGateway** nos EIPs para NAT. Isso garante que o Internet Gateway j√° esteja anexado quando o CloudFormation pedir o EIP. Por qu√™? Ao alocar um EIP com **Domain: vpc**, √© necess√°rio que haja um IGW na VPC para rotear o tr√°fego do EIP; indicar a depend√™ncia evita erros de provisionamento.\n\n### 1.4 Sa√≠das (Outputs) e Testes da Landing Zone\n\nPor fim, adicionamos *outputs* no template para capturar informa√ß√µes que ser√£o √∫teis nos pr√≥ximos tutoriais (e para inspe√ß√£o). Vamos expor, por exemplo, o **ID da VPC** e os IDs das subnets criadas. Isso permitir√° que outros templates (da aplica√ß√£o) possam referenci√°-los facilmente ao implantar inst√¢ncias EC2 ou outros recursos dentro da VPC. Em CloudFormation, podemos exportar outputs e depois import√°-los em outro stack via **Fn::ImportValue**. \n\nAdicionamos ao final do template YAML:\n\n```yaml\nOutputs:\n  VPCId:\n    Description: \"ID da VPC criada na Landing Zone\"\n    Value: !Ref AppVPC\n    Export:\n      Name: !Sub \"${AWS::StackName}-VPCId\"\n  PublicSubnetA:\n    Description: \"ID Subnet P√∫blica A\"\n    Value: !Ref PublicSubnetA\n    Export:\n      Name: !Sub \"${AWS::StackName}-PublicSubnetA\"\n  PublicSubnetB:\n    Description: \"ID Subnet P√∫blica B\"\n    Value: !Ref PublicSubnetB\n    Export:\n      Name: !Sub \"${AWS::StackName}-PublicSubnetB\"\n  PrivateSubnetA:\n    Description: \"ID Subnet Privada A\"\n    Value: !Ref PrivateSubnetA\n    Export:\n      Name: !Sub \"${AWS::StackName}-PrivateSubnetA\"\n  PrivateSubnetB:\n    Description: \"ID Subnet Privada B\"\n    Value: !Ref PrivateSubnetB\n    Export:\n      Name: !Sub \"${AWS::StackName}-PrivateSubnetB\"\n```\n\n**Valida√ß√£o da Landing Zone:** Ap√≥s criar o stack CloudFormation da Landing Zone, voc√™ pode verificar no console AWS VPC que a VPC e subnets foram criadas. Verifique se: \n- A VPC aparece com o CIDR especificado e DNS Hostnames ativado.\n- As subnets t√™m nomes/tag corretos e est√£o divididas em AZs distintas.\n- O Internet Gateway est√° anexado √† VPC.\n- Nas tabelas de rotas, as subnets p√∫blicas t√™m rota para o IGW e as privadas para os NATs.\n- Os NAT Gateways est√£o em estado **Available** com EIPs associados.\n\n√â recomend√°vel testar conectividade b√°sica: lan√ßar temporariamente uma inst√¢ncia EC2 em cada subnet (p√∫blica e privada) e conferir:\n  - Inst√¢ncia em subnet p√∫blica: deve obter IP p√∫blico e ser capaz de pingar uma URL externa (ex: **ping amazon.com**) ou instalar updates via **yum update**. Tamb√©m deve ser acess√≠vel via SSH de fora (se grupo de seguran√ßa permitir, veremos isso mais adiante).\n  - Inst√¢ncia em subnet privada: n√£o ter√° IP p√∫blico, mas deve conseguir acessar a internet *atrav√©s do NAT*. Por exemplo, via SSH (conectando primeiro em uma bastion na p√∫blica) ou registrando logs, verificar se um **curl http://ifconfig.me** retorna um IP (dever√° ser o EIP do NAT, n√£o o IP privado dela).\n\nCom a Landing Zone criada e validada, estamos prontos para implantar a aplica√ß√£o de teste no pr√≥ximo tutorial.\n"
  },
  {
    "id": "fe18363b-d20d-4125-bec1-c7bfbbf7bdad",
    "title": "APP mon√≥lito com Cloudformation",
    "description": "Neste tutorial com CloudFormation, o foco √© a implanta√ß√£o de uma aplica√ß√£o BFF monol√≠tica em uma √∫nica inst√¢ncia EC2. O template provisiona todos os componentes necess√°rios ‚Äî backend, frontend, banco de dados e API ‚Äî e inclui coment√°rios que explicam a import√¢ncia de cada recurso. Essa abordagem inicial permite testar a aplica√ß√£o completa num ambiente simplificado, facilitando os primeiros passos na pr√°tica de IaC.",
    "tool": "CloudFormation",
    "level": "intermediario",
    "tags": [
      "mon√≥lito",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/fe18363b-d20d-4125-bec1-c7bfbbf7bdad",
    "markdown": "Agora que temos uma Landing Zone configurada, podemos implantar a primeira vers√£o da nossa aplica√ß√£o: um **mon√≥lito**. Nesta etapa, todos os componentes da aplica√ß√£o ‚Äì frontend, backend, API e banco de dados ‚Äì ser√£o executados juntos em uma √∫nica inst√¢ncia EC2 dentro da VPC. O objetivo √© entender como provisionar uma inst√¢ncia completa via CloudFormation, configurar seu software automaticamente e validar o funcionamento da aplica√ß√£o em formato monol√≠tico. Em seguida, usaremos essa base para evoluir para a arquitetura BFF (Backend for Frontend) de microsservi√ßos.\n\n**Sobre a aplica√ß√£o de exemplo (Mon√≥lito):** Desenvolvemos uma aplica√ß√£o simples em JavaScript (Node.js) que servir√° de exemplo. Essa aplica√ß√£o consiste em:\n- **Frontend:** Uma p√°gina web est√°tica (HTML/CSS/JS) que exibe uma mensagem e consome uma API.\n- **Backend/API:** Um servidor Node.js (Express) que exp√µe uma API REST (**/api/mensagem**) para fornecer dados (uma mensagem de texto) armazenados no banco de dados.\n- **Banco de Dados:** Um banco MySQL simples contendo uma tabela de mensagens. Inicialmente, ele ter√° uma mensagem (\"Ol√°, Mundo!\") para retorno via API.\n- Tudo isso estar√° rodando na mesma inst√¢ncia, comunicando-se localmente.\n\nComo prova de funcionamento, o frontend far√° uma requisi√ß√£o AJAX para a API e exibir√° a mensagem retornada na p√°gina. Tamb√©m incluiremos um endpoint de sa√∫de (health check) simples para verificar rapidamente se o servidor est√° respondendo.\n\n### 2.1 Provisionando a Inst√¢ncia EC2 com CloudFormation\n\nUtilizaremos CloudFormation para criar a inst√¢ncia EC2 e automatizar toda a configura√ß√£o via **User Data**. O User Data √© um mecanismo que permite passar um script para rodar automaticamente no primeiro boot da inst√¢ncia. Vamos aproveit√°-lo para instalar o Node.js, o banco de dados local, e iniciar nossa aplica√ß√£o monol√≠tica sem interven√ß√£o manual.\n\n**Recursos envolvidos no template:**\n- **Security Group:** definindo regras de firewall da inst√¢ncia (SSH e HTTP abertos para teste).\n- **EC2 Instance:** a pr√≥pria inst√¢ncia, em uma subnet p√∫blica da nossa VPC, associada ao Security Group e com um script de user data que realiza toda a instala√ß√£o/ configura√ß√£o da aplica√ß√£o.\n- **Output:** IP p√∫blico ou DNS p√∫blico da inst√¢ncia, para acessarmos a aplica√ß√£o ap√≥s implanta√ß√£o.\n\n**Escolha da AMI:** Usaremos uma AMI Linux padr√£o ‚Äì por exemplo, Amazon Linux 2 ‚Äì como sistema operacional base. O ID da AMI varia por regi√£o, ent√£o podemos usar um par√¢metro do Systems Manager para pegar a AMI mais recente automaticamente. A AWS disponibiliza par√¢metros SSM p√∫blicos, como **/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2** para Amazon Linux 2 (HVM EBS General Purpose). No template, utilizaremos a fun√ß√£o **!Sub** com *dynamic reference* para obter o ID da AMI atual:\n\n```yaml\nParameters:\n  LatestAmiId:\n    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>\n    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2\n```\n\nAcima, definimos um par√¢metro que consulta o Parameter Store pelo ID da √∫ltima Amazon Linux 2 (64-bit x86). Assim, n√£o precisamos codificar um ID AMI fixo (que ficaria obsoleto com o tempo). \n\n**Defini√ß√£o do Security Group e Instance:**\n\n```yaml\nResources:\n  AppSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: \"Acesso HTTP e SSH para o servidor monol√≠tico\"\n      VpcId: !ImportValue LandingZone-VPCId                 # Importa o ID da VPC da Landing Zone criada no tutorial 1\n      SecurityGroupIngress:\n        # Porta 22 (SSH) aberta para acesso (aqui estamos abrindo globalmente 0.0.0.0/0 apenas para simplicidade de teste; recomend√°vel restringir)\n        - IpProtocol: tcp\n          FromPort: 22\n          ToPort: 22\n          CidrIp: 0.0.0.0/0\n        # Porta 80 (HTTP) aberta para todos, para servir frontend e API\n        - IpProtocol: tcp\n          FromPort: 80\n          ToPort: 80\n          CidrIp: 0.0.0.0/0\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-AppSG\"\n\n  MonolithInstance:\n    Type: AWS::EC2::Instance\n    Properties:\n      InstanceType: t3.micro\n      ImageId: !Ref LatestAmiId\n      SubnetId: !ImportValue LandingZone-PublicSubnetA      # lan√ßa na subnet p√∫blica (Zona A)\n      SecurityGroupIds: \n        - !Ref AppSecurityGroup\n      KeyName: !Ref SshKeyName                              # Par de chaves para login SSH (passado via par√¢metro no template)\n      UserData: \n        Fn::Base64: !Sub |\n          #!/bin/bash\n          # Atualiza pacotes b√°sicos\n          yum update -y\n          # Instala utilit√°rios necess√°rios (compilador e Make para compilar m√≥dulos nativos, caso necess√°rio)\n          yum install -y gcc-c++ make\n          # Instala reposit√≥rio NodeSource e depois Node.js (LTS)\n          curl -sL https://rpm.nodesource.com/setup_16.x | bash -\n          yum install -y nodejs\n          \n          # Instala MariaDB (MySQL) servidor e cliente\n          yum install -y mariadb-server\n          systemctl start mariadb\n          systemctl enable mariadb\n\n          # Configura banco de dados inicial (cria schema e tabela)\n          mysql -e \"CREATE DATABASE appdb; \\\n                    CREATE USER 'appuser'@'localhost' IDENTIFIED BY 'SenhaApp123'; \\\n                    GRANT ALL ON appdb.* TO 'appuser'@'localhost'; \\\n                    USE appdb; \\\n                    CREATE TABLE mensagens (id INT AUTO_INCREMENT PRIMARY KEY, texto VARCHAR(255)); \\\n                    INSERT INTO mensagens (texto) VALUES ('Ol√°, Mundo!');\"\n          \n          # Cria diret√≥rio da aplica√ß√£o\n          mkdir -p /home/ec2-user/app\n          chown ec2-user:ec2-user /home/ec2-user/app\n\n          # Cria arquivo da aplica√ß√£o Node (Express server) com c√≥digo-fonte\n          cat > /home/ec2-user/app/app.js << 'APPJS'\n          // Importa m√≥dulos\n          const express = require('express');\n          const mysql = require('mysql2');\n          const app = express();\n          const port = 80;  // vamos usar porta 80 para evitar especificar porta na URL\n\n          // Configura conex√£o com MySQL local\n          const db = mysql.createConnection({\n            host: 'localhost',\n            user: 'appuser',\n            password: 'SenhaApp123',\n            database: 'appdb'\n          });\n\n          // Servir arquivos est√°ticos do frontend (pasta 'public')\n          app.use(express.static('public'));\n\n          // Endpoint API: retorna a mensagem do banco de dados\n          app.get('/api/mensagem', (req, res) => {\n            db.query('SELECT texto FROM mensagens LIMIT 1', (err, results) => {\n              if (err) {\n                console.error(\"Erro ao consultar DB:\", err);\n                return res.status(500).json({ erro: 'Erro no servidor' });\n              }\n              const msg = results[0] ? results[0].texto : 'Nenhuma mensagem encontrada';\n              res.json({ mensagem: msg });\n            });\n          });\n\n          // Endpoint de health check simples\n          app.get('/api/health', (req, res) => {\n            res.send('OK');\n          });\n\n          // Inicia o servidor\n          app.listen(port, '0.0.0.0', () => {\n            console.log(\\`Servidor monol√≠tico rodando na porta \\${port}\\`);\n          });\n          APPJS\n\n          # Cria arquivo HTML do frontend na pasta public\n          mkdir -p /home/ec2-user/app/public\n          cat > /home/ec2-user/app/public/index.html << 'INDEXHTML'\n          <!DOCTYPE html>\n          <html lang=\"pt-BR\">\n          <head>\n            <meta charset=\"UTF-8\" />\n            <title>App Mon√≥lito - BFF Demo</title>\n            <style>\n              body { font-family: Arial, sans-serif; margin: 2em; background: #f2f2f2; }\n              h1 { color: #333; }\n              #mensagem { font-size: 1.5em; color: #007BFF; }\n              .loading { color: gray; font-style: italic; }\n            </style>\n          </head>\n          <body>\n            <h1>Bem-vindo √† Aplica√ß√£o Monol√≠tica!</h1>\n            <p>Mensagem do backend: <span id=\"mensagem\" class=\"loading\">carregando...</span></p>\n            <script>\n              // Faz requisi√ß√£o √† API para obter a mensagem\n              fetch('/api/mensagem')\n                .then(response => response.json())\n                .then(data => {\n                  document.getElementById('mensagem').textContent = data.mensagem;\n                  document.getElementById('mensagem').classList.remove('loading');\n                })\n                .catch(err => {\n                  console.error('Erro ao buscar mensagem:', err);\n                  document.getElementById('mensagem').textContent = 'Erro ao obter mensagem';\n                });\n            </script>\n          </body>\n          </html>\n          INDEXHTML\n\n          # Ajusta permiss√µes\n          chown -R ec2-user:ec2-user /home/ec2-user/app\n\n          # Entra no diret√≥rio da aplica√ß√£o e instala depend√™ncias Node (Express e MySQL)\n          cd /home/ec2-user/app\n          sudo -u ec2-user npm init -y                    # cria package.json com padr√£o\n          sudo -u ec2-user npm install express mysql2     # instala pacotes necess√°rios\n\n          # Inicia a aplica√ß√£o Node em background\n          nohup sudo -u ec2-user node app.js > app.log 2>&1 &\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-MonolithHost\"\n```\n\nVamos dissecar as partes cr√≠ticas do *User Data* acima, pois ele realiza toda m√°gica de configurar o servidor monol√≠tico automaticamente:\n\n- **Atualiza√ß√£o e instala√ß√µes base:** Atualizamos a lista de pacotes e instalamos **gcc-c++** e **make** (pr√©-requisitos para construir alguns pacotes Node nativos). Depois, usamos um script do NodeSource para habilitar o reposit√≥rio do Node.js 16 LTS e instalamos o Node.js em si. A seguir, instalamos o servidor de banco de dados MariaDB (muito similar ao MySQL) e o iniciamos. Essas etapas automatizadas garantem que a inst√¢ncia tenha Node e MySQL prontos para uso.\n\n- **Configura√ß√£o do banco de dados:** Usando o cliente **mysql**, executamos uma sequ√™ncia de comandos SQL n√£o interativos via **-e**. Criamos um banco de dados **appdb** e um usu√°rio **appuser** com senha (exemplo did√°tico **SenhaApp123** ‚Äì em ambientes reais, voc√™ usaria algo seguro e n√£o exporia em texto plano). Em seguida, criamos uma tabela **mensagens** e inserimos uma linha com texto \"Ol√°, Mundo!\". Assim, nosso banco j√° est√° populado com um valor para a API retornar. Tudo isso ocorre durante o boot.\n\n- **Deploy da aplica√ß√£o Node.js:** Criamos um diret√≥rio **/home/ec2-user/app** para o c√≥digo, garantindo que o dono seja **ec2-user** (o usu√°rio padr√£o da inst√¢ncia Amazon Linux). Em seguida, usamos o comando **cat** para criar dois arquivos: **app.js** (c√≥digo do servidor Node) e **index.html** (frontend). Repare nos *EOF markers* **<< 'APPJS'** e **<< 'INDEXHTML'** ‚Äì eles permitem inserir um bloco de texto exatamente como est√° no script de user data, facilitando escrever m√∫ltiplas linhas. Dentro do c√≥digo Node:\n  - Criamos uma aplica√ß√£o Express simples, definindo rotas **/api/mensagem** e **/api/health**. \n  - O c√≥digo de **/api/mensagem** se conecta ao MariaDB local e busca a mensagem inserida, retornando-a em formato JSON. Adicionamos tratamento b√°sico de erro (logar no console e retornar status 500).\n  - Usamos **express.static('public')** para servir arquivos est√°ticos. Isso permite que o front-end (nosso **index.html** e qualquer outro arquivo est√°tico na pasta **public**) seja servido diretamente pelo Express.\n  - Iniciamos o servidor na porta 80 e ouvindo em **0.0.0.0** (todas interfaces) para aceitar conex√µes externas.\n  \n  No arquivo HTML, inclu√≠mos um script que faz um fetch em **/api/mensagem** assim que a p√°gina carrega e exibe o conte√∫do retornado dentro do elemento **<span id=\"mensagem\">**. Estilizamos o texto para destaque. Enquanto a resposta n√£o chega, exibimos \"carregando...\" em it√°lico cinza (classe CSS **loading**).\n\n- **Instala√ß√£o de depend√™ncias e execu√ß√£o:** Ap√≥s criar os arquivos, precisamos instalar as depend√™ncias Node (Express e mysql2). Usamos **npm init -y** para gerar um **package.json** padr√£o rapidamente, e ent√£o **npm install express mysql2**. Note o uso de **sudo -u ec2-user**: isso executa o comando como o usu√°rio normal, n√£o root. Fizemos isso para que os arquivos e **node_modules** fiquem com permiss√£o do ec2-user (evitando problemas de permiss√£o ao rodar a app como usu√°rio normal). Por fim, executamos **node app.js** em background usando **nohup** (assim o processo continua rodando ap√≥s o t√©rmino do user data). Redirecionamos a sa√≠da para **app.log** para fins de depura√ß√£o. Agora, teoricamente, nosso servidor Node est√° rodando atendendo na porta 80 e conectado ao banco local.\n\nEssa abordagem demonstra *bootstrapping completo* de uma inst√¢ncia EC2 via user data. Segundo a documenta√ß√£o, os scripts providos em user data s√£o executados no contexto root durante a inicializa√ß√£o, permitindo automa√ß√£o de configura√ß√£o complexa. Temos que lembrar que o user data tem limite de ~16 KB ap√≥s base64, ent√£o scripts muito grandes podem requerer outra estrat√©gia (no nosso caso, cabe confort√°vel).\n\n- **Security Group Ingress:** Permitimos no SG o tr√°fego HTTP (porta 80) de qualquer origem, j√° que queremos que usu√°rios acessem a aplica√ß√£o web e API externamente. Permitir SSH (22) globalmente tamb√©m, mas isso deve ser *apenas para ambiente de teste*. Em produ√ß√£o, restrinja SSH ao IP do desenvolvedor ou use VPN/Bastion. Aqui mantivemos aberto para n√£o complicar demonstrativamente.\n\n- **Associa√ß√£o √† VPC e Subnet:** Note que usamos **!ImportValue** para pegar o VPC e subnet criados no Tutorial 1 (Landing Zone). A express√£o **LandingZone-PublicSubnetA** corresponde ao export name que definimos nos outputs do stack de rede. Ao referenciar assim, garantimos que a inst√¢ncia ser√° lan√ßada dentro da VPC correta. CloudFormation cuida das depend√™ncias se usarmos a importa√ß√£o ‚Äì ent√£o, certifique-se de que o stack da Landing Zone esteja aplicado e os outputs exportados com esses nomes.\n\n### 2.2 Testando a Aplica√ß√£o Monol√≠tica\n\nCom o template acima, podemos criar o stack **AppMonolito** pelo CloudFormation. Isso ir√° iniciar a inst√¢ncia e rodar todo o processo de configura√ß√£o automaticamente (pode levar alguns minutos para concluir a instala√ß√£o de pacotes e start do servidor). \n\n**Outputs da inst√¢ncia:** Podemos adicionar outputs para capturar o endere√ßo p√∫blico da inst√¢ncia:\n\n```yaml\nOutputs:\n  MonolithURL:\n    Description: \"URL de acesso HTTP √† aplica√ß√£o monol√≠tica\"\n    Value: !Sub \"http://${MonolithInstance.PublicDnsName}\"\n```\n\nIsso nos d√° uma URL (usando o Public DNS) para acessar via navegador. Assim que o stack terminar de criar, pegue essa URL nos outputs (ou no console EC2, copie o Public DNS ou IP da inst√¢ncia) e tente acess√°-la pelo navegador. Voc√™ deve ver a p√°gina \"Bem-vindo √† Aplica√ß√£o Monol√≠tica!\" e, ap√≥s alguns instantes, a mensagem carregada do backend aparecendo. Geralmente, dever√° mostrar **\"Ol√°, Mundo!\"** trazido do banco de dados.\n\nVamos verificar tamb√©m o endpoint da API diretamente. Experimente acessar **http://<DNS-da-instancia>/api/mensagem** ‚Äì deve retornar um JSON **{\"mensagem\": \"Ol√°, Mundo!\"}**. O endpoint **/api/health** retornar√° \"OK\" (texto puro) para indicar que o servidor Node est√° ativo. Esses testes confirmam que:\n- O servidor Node subiu corretamente e est√° atendendo requisi√ß√µes HTTP.\n- Ele conseguiu se comunicar com o banco de dados local e extrair a mensagem.\n- O front-end est√° servindo o HTML e conseguindo chamar a API do backend.\n\nCaso algo n√£o funcione, voc√™ pode **SSH** na inst√¢ncia (usando a chave especificada em **KeyName**) e inspecionar:\n- O arquivo **/home/ec2-user/app/app.log** para ver logs do Node (por exemplo, erros de conex√£o ao DB ou outros).\n- **sudo journalctl -u cloud-init** para ver logs do processamento do user data (cloud-init √© o servi√ßo que aplica o user data script). Ali podem aparecer erros de sintaxe no script que impe√ßam partes dele de rodar.\n- Verificar se todos os processos est√£o em execu√ß√£o: **ps aux | grep node** (deve listar o processo do app.js) e **sudo systemctl status mariadb** (status do DB).\n- Checar regras do Security Group se a porta 80 est√° mesmo aberta (√†s vezes esquecemos alguma configura√ß√£o).\n\nSupondo que esteja tudo certo, parab√©ns ‚Äì voc√™ implantou com sucesso uma aplica√ß√£o full-stack monol√≠tica usando IaC! üéâ \n\n**Limita√ß√µes do Mon√≥lito:** Antes de avan√ßar, vale refletir: esse mon√≥lito funciona, mas n√£o √© escal√°vel ou flex√≠vel. Toda atualiza√ß√£o requer recriar a inst√¢ncia inteira. Al√©m disso, todos os componentes compartilham recursos e podem conflitar (por exemplo, alta carga no front-end prejudicaria o banco e vice-versa). √â aqui que entra a abordagem baseada em microsservi√ßos com o padr√£o **BFF (Backend for Frontend)**. Nos pr√≥ximos tutoriais, iremos gradualmente **extrair** componentes deste mon√≥lito para servi√ßos separados (frontend isolado, backend isolado, banco gerenciado separado) e introduzir um Amazon API Gateway na frente. Essa mudan√ßa ilustrar√° os benef√≠cios de arquitetura (como escalabilidade independente e seguran√ßa) sem alterar o objetivo final da aplica√ß√£o (que continuar√° exibindo a mensagem \"Ol√°, Mundo!\" ‚Äì s√≥ que agora atrav√©s de uma solu√ß√£o mais modular)."
  },
  {
    "id": "872a2f62-7aff-4d01-b2d6-1feafa784dcf",
    "title": "APP BFF Frontend com Cloudformation",
    "description": "O terceiro tutorial aborda a cria√ß√£o de uma infraestrutura exclusiva para o Frontend da aplica√ß√£o BFF utilizando CloudFormation. O template mostra como configurar recursos dedicados ‚Äî como balanceadores de carga e inst√¢ncias web ‚Äî e ajustar as regras de seguran√ßa para garantir que o servi√ßo possa ser acessado de maneira segura e eficiente. Cada parte do c√≥digo √© comentada para destacar sua relev√¢ncia na entrega de frontend escal√°vel e resiliente.",
    "tool": "CloudFormation",
    "level": "intermediario",
    "tags": [
      "frontend",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/872a2f62-7aff-4d01-b2d6-1feafa784dcf",
    "markdown": "Come√ßamos agora a transi√ß√£o da arquitetura monol√≠tica para a arquitetura de **Backend for Frontend (BFF)**. O padr√£o BFF consiste em ter backends separados e otimizados para cada frontend ou experi√™ncia de usu√°rio. Em nosso contexto simplificado, isso significa desvincular o frontend (interface web) do backend/API. Faremos isso hospedando o front-end de forma independente, permitindo que ele se comunique com o backend por meio de chamadas de API. \n\n**O que muda do Tutorial 2 para o 3:** No mon√≥lito, o front-end (HTML/JS) era servido pela mesma inst√¢ncia que o backend. Agora, teremos:\n- **Frontend isolado:** Hospedaremos os arquivos est√°ticos (HTML, JS, CSS) em um bucket S3 configurado para site est√°tico.\n- **Backend/API:** Continuar√° inicialmente sendo o mon√≥lito existente (ou uma inst√¢ncia separada) respondendo √†s chamadas de API, mas **n√£o** servir√° mais o HTML. Ele atuar√° como um *servi√ßo* que o frontend consumir√°.\n- Reconfiguraremos o front-end para buscar a API no novo endpoint (que temporariamente ainda ser√° o do mon√≥lito). Nos pr√≥ximos tutoriais, substituiremos esse backend por uma implementa√ß√£o BFF dedicada e um API Gateway.\n\nEm resumo, este tutorial foca em **extrair o front-end** para fora do servidor monol√≠tico, preparando o terreno para a migra√ß√£o completa para microsservi√ßos.\n\n### 3.1 Vantagens de Desacoplar o Frontend\n\nSeguir o padr√£o BFF traz diversos benef√≠cios:\n- **Desenvolvimento independente:** Equipes podem trabalhar no front-end sem afetar o backend (e vice-versa), j√° que est√£o implantados separadamente.\n- **Otimiza√ß√£o por cliente:** Podemos adaptar o backend ou as chamadas especificamente √†s necessidades do front (por exemplo, retornando dados j√° no formato ideal, reduzindo processamento e chamadas no front-end).\n- **Escalabilidade separada:** O front-end (essencialmente arquivos est√°ticos servidos via CDN/S3) pode escalar em distribui√ß√£o, enquanto o backend pode escalonar inst√¢ncias ou usar servi√ßos gerenciados conforme carga de API. No mon√≥lito, isso era tudo junto em um √∫nico ponto.\n\nNesta etapa, a mudan√ßa principal √© de implanta√ß√£o: colocar o front-end est√°tico em S3/CloudFront. N√£o teremos ainda altera√ß√£o funcional no c√≥digo al√©m de apontar as requisi√ß√µes para a URL correta.\n\n### 3.2 Hospedando o Frontend Est√°tico no Amazon S3\n\nO **Amazon S3** (Simple Storage Service) pode ser usado para hospedar sites est√°ticos de forma simples e econ√¥mica. Vamos criar um **Bucket S3** para armazenar **index.html** e outros arquivos est√°ticos. Ativaremos a configura√ß√£o de site est√°tico no bucket e permitiremos acesso p√∫blico de leitura aos objetos (necess√°rio para qualquer pessoa poder carregar o site via HTTP). Em ambiente real, pode-se usar **Amazon CloudFront** para distribuir esses conte√∫dos globalmente e usar HTTPS com dom√≠nio customizado, mas manteremos simples aqui.\n\n**Configura√ß√µes importantes do Bucket:**\n- **WebsiteConfiguration:** definindo **IndexDocument** (nosso **index.html**) e um **ErrorDocument** (p√°gina de erro 404, por exemplo).\n- **Bloqueio de acesso p√∫blico:** Por padr√£o, novos buckets bloqueiam acesso p√∫blico. Precisamos desabilitar essas configura√ß√µes no bucket (ou a hospedagem n√£o funcionar√°). Isso √© feito via propriedades **PublicAccessBlockConfiguration** no CloudFormation.\n- **Bucket Policy:** Uma policy expl√≠cita permitindo **s3:GetObject** para qualquer usu√°rio (**Principal: \"*\"**) nos objetos do bucket, para efetivamente torn√°-lo p√∫blico somente para leitura dos arquivos do site.\n- **DeletionPolicy:** marcaremos o recurso bucket com **DeletionPolicy: Retain** para evitar que, caso o stack CloudFormation seja exclu√≠do, o conte√∫do do bucket seja deletado acidentalmente. Assim, n√≥s preservar√≠amos os arquivos do site (mas aten√ß√£o: depois seria preciso apagar o bucket manualmente se realmente quiser remov√™-lo).\n\nVamos montar o template CloudFormation para este front-end:\n\n```yaml\nAWSTemplateFormatVersion: 2010-09-09\nDescription: \"Site est√°tico do Frontend BFF em S3\"\n\nParameters:\n  SiteBucketName:\n    Description: \"Nome √∫nico para o bucket S3 do site (deve ser globalmente √∫nico)\"\n    Type: String\n\nResources:\n  FrontendSiteBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: !Ref SiteBucketName\n      PublicAccessBlockConfiguration:\n        BlockPublicAcls: false\n        BlockPublicPolicy: false\n        IgnorePublicAcls: false\n        RestrictPublicBuckets: false\n      WebsiteConfiguration:\n        IndexDocument: index.html\n        ErrorDocument: error.html\n    DeletionPolicy: Retain\n    UpdateReplacePolicy: Retain\n\n  SiteBucketPolicy:\n    Type: AWS::S3::BucketPolicy\n    Properties:\n      Bucket: !Ref FrontendSiteBucket\n      PolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Sid: PublicReadGetObject\n            Effect: Allow\n            Principal: \"*\"\n            Action: \"s3:GetObject\"\n            Resource: !Sub \"${FrontendSiteBucket.Arn}/*\"\n```\n\n**Explica√ß√£o:**  O recurso **FrontendSiteBucket** cria o bucket S3. Estamos usando um par√¢metro **SiteBucketName** para o nome do bucket porque nomes S3 devem ser √∫nicos globalmente ‚Äì o usu√°rio pode fornecer um nome (por exemplo, **meusite-bff-frontend-2025**). Se deixar sem nome, CloudFormation gera um nome, mas para site est√°tico preferimos algo amig√°vel/determin√≠stico.\n\nEm **PublicAccessBlockConfiguration**, definimos todas as quatro op√ß√µes como false, efetivamente permitindo que pol√≠ticas p√∫blicas sejam aplicadas. A Bucket Policy ent√£o libera leitura p√∫blica. Essas configura√ß√µes juntas s√£o essenciais: se esquecermos de desabilitar o bloqueio de p√∫blico, a policy p√∫blica seria ignorada e o site n√£o funcionaria (resultaria em acesso negado ao tentar acessar arquivos).\n\n**WebsiteConfiguration** ativa o modo est√°tico e define o documento de √≠ndice e erro. Precisamos fornecer um **error.html**, ent√£o vamos criar um arquivo simples de erro mais √† frente manualmente ou instruir o usu√°rio a criar.\n\n**Outputs do site:** Podemos querer expor o endpoint do site que a AWS gera, usando atributos **WebsiteURL** e **DomainName** do bucket:\n```yaml\nOutputs:\n  SiteURL:\n    Description: \"Endpoint de website est√°tico (http)\"\n    Value: !GetAtt FrontendSiteBucket.WebsiteURL\n```\nIsto geralmente retorna uma URL como **http://<bucket-name>.s3-website.<region>.amazonaws.com**. √â por essa URL que o site estar√° acess√≠vel (a n√£o ser que usemos dom√≠nio custom e CloudFront, o que n√£o entraremos aqui).\n\n### 3.3 Publicando os Arquivos do Frontend no Bucket\n\nCom o bucket criado, precisamos colocar o conte√∫do nele. Isso pode ser feito de v√°rias formas:\n- Usando o AWS CLI (**aws s3 cp**) ou console para enviar os arquivos.\n- Durante o pr√≥prio CloudFormation, *n√£o h√°* um recurso nativo que injete objetos dentro do bucket (a menos que use fun√ß√µes Lambda custom ou pacotes). Para simplificar, assumiremos que faremos upload manual, ou via um script separado, dos arquivos.\n\n**Arquivos necess√°rios:**\n- **index.html** ‚Äì nossa p√°gina principal (conforme definimos no mon√≥lito).\n- **error.html** ‚Äì uma p√°gina de erro simples (pode s√≥ exibir \"P√°gina n√£o encontrada\" ou redirecionar de volta para index).\n- Poss√≠veis arquivos JS/CSS se existirem (no nosso caso, o JS estava embutido no HTML, ent√£o n√£o h√° arquivos externos adicionais).\n\nVamos supor que extra√≠mos o **index.html** do tutorial anterior (ou do reposit√≥rio do projeto) e o temos em m√£os. Precisaremos editar **uma parte dele**: a URL da API. No mon√≥lito, o JavaScript fazia **fetch('/api/mensagem')**, usando caminho relativo ‚Äì isso funcionava porque o front e o back estavam no mesmo host. Agora, o site estar√° em um dom√≠nio (bucket S3/CloudFront) diferente do backend (que ainda est√° na inst√¢ncia EC2). Precisamos apontar o fetch para o endpoint do backend. \n\nPor ora, o backend ainda √© a inst√¢ncia monol√≠tica do Tutorial 2. Podemos usar o **Public DNS ou endere√ßo** dela. Por exemplo, se a inst√¢ncia monol√≠tica tem DNS p√∫blico **ec2-3-92-100-123.compute-1.amazonaws.com**, ajustar√≠amos para:\n```js\nfetch('http://ec2-3-92-100-123.compute-1.amazonaws.com/api/mensagem')\n```\nAlternativamente, podemos ter um *par√¢metro* no front para a URL da API. Mas para manter simples, podemos inserir est√°tico mesmo.\n\n**Cuidado com CORS:** Ao chamar a API de um dom√≠nio diferente (S3 site) podemos cair na restri√ß√£o de *Cross-Origin Resource Sharing*. Nosso backend Express atualmente n√£o lida com CORS ‚Äì se o navegador bloquear, precisar√≠amos habilitar (ex.: usando o middleware **cors()** no Express ou configurando cabe√ßalhos). Para esse projeto did√°tico, podemos contornar temporariamente se acessarmos o site via HTTP e a inst√¢ncia sem restri√ß√£o (a AWS S3 static site endpoints n√£o enviam cabe√ßalhos restritivos). O navegador provavelmente permitir√° pois a requisi√ß√£o √© HTTP e possivelmente mesma origem se arranjarmos com domain, mas na verdade ser√£o origens diferentes (diferentes hostnames). Provavelmente precisamos habilitar CORS no backend express para *permitir* a origem do bucket S3. \n\nPara n√£o complicar muito aqui, podemos supor que habilitamos CORS simples no Express:\nNo c√≥digo Node (tutorial 2) adicionar:\n```js\napp.use((req, res, next) => {\n  res.setHeader('Access-Control-Allow-Origin', '*');\n  next();\n});\n```\nIsso permitiria qualquer origem. Seria uma linha a mais no user data. Vamos considerar que fizemos isso ao extrair para microservi√ßos (no pr√≥ximo tutorial do backend BFF podemos fazer adequadamente). \n\nComo este √© um tutorial textual, vamos apenas mencionar essa necessidade: \"*Caso o frontend esteja em dom√≠nio diferente, habilite CORS no backend*\".\n\n**Upload dos arquivos:** Realize o upload do novo **index.html** (modificado para apontar para API) e crie um **error.html** b√°sico (por exemplo, contendo uma mensagem de erro). Isso pode ser feito via AWS CLI:\n```\naws s3 cp index.html s3://<SiteBucketName>/index.html\naws s3 cp error.html s3://<SiteBucketName>/error.html\n```\nCertifique-se de que a pol√≠tica p√∫blica esteja aplicada (pode testar acessando o URL do objeto direto).\n\n### 3.4 Testando o Frontend Desacoplado\n\nCom o site est√°tico no ar, acesse o **Endpoint do Website S3** (fornecido no output ou console S3 em \"Endpoint do site\"). Voc√™ dever√° ver o mesmo conte√∫do de antes. Ao carregar, o script far√° a chamada fetch para o backend:\n- Verifique no navegador (console de dev do browser) se a requisi√ß√£o foi feita e teve resposta. Se houver erro de CORS, voc√™ ver√° bloqueio ‚Äì nesse caso, ser√° necess√°rio habilitar CORS no servidor Express (um passo t√©cnico que podemos implementar no pr√≥ximo tutorial).\n- Se tudo der certo, a mensagem \"Ol√°, Mundo!\" aparecer√° normalmente. Isso indica que:\n  - O site S3 est√° entregando o HTML e JS.\n  - O front-end conseguiu se comunicar com a API do backend monol√≠tico sobre a internet.\n  - O backend atendeu e retornou o dado corretamente.\n\nAtente-se: a URL da API no c√≥digo deve conter o protocolo (**http://** etc.) completo. Como neste momento estamos sem HTTPS ou dom√≠nio custom, manteremos **http**. Lembre que o endpoint S3 (ex: **http://meusite-bff.s3-website-us-east-1.amazonaws.com**) tamb√©m √© HTTP (S3 static site endpoints n√£o suportam HTTPS sem CloudFront).\n\n**Considera√ß√µes de seguran√ßa:** Ao abrir o bucket para p√∫blico, torna-se cr√≠tico garantir que ele s√≥ contenha conte√∫do que possa realmente ser p√∫blico (HTML, JS, imagens, nada sens√≠vel). Em geral, tudo bem, pois √© s√≥ o site. Tamb√©m, n√£o armazenamos dados de cliente ali, ent√£o √© seguro. Em produ√ß√£o, provavelmente usar√≠amos CloudFront com HTTPS e restringir√≠amos o bucket para que s√≥ o CloudFront possa ler (evitando acesso p√∫blico direto). Mas para fins did√°ticos, mantemos assim.\n\n**Integridade do BFF Frontend:** Neste ponto, implementamos o BFF pattern parcialmente: *o front-end agora √© servido independentemente*. Em termos de equipes, poder√≠amos atualizar o site (alterar o HTML/JS) simplesmente fazendo upload novo no bucket, sem tocar no backend. O deploy front-end tornou-se desacoplado do backend, aumentando nossa agilidade.\n\nTemos ainda o backend rodando como mon√≥lito naquele EC2. No pr√≥ximo tutorial, vamos extrair tamb√©m o backend para seu pr√≥prio servi√ßo BFF (provavelmente um novo servidor ou uma altera√ß√£o do existente) e realocar o banco de dados para um servi√ßo gerenciado (RDS). Assim, concluiremos a separa√ß√£o completa: front-end em S3, back-end BFF em EC2 (ou outro compute), banco de dados em RDS, e por fim, um API Gateway integrando tudo no front."
  },
  {
    "id": "23efe3b0-d657-4002-8ebb-297988366779",
    "title": "APP BFF Backend com Cloudformation",
    "description": "Neste tutorial, voc√™ criar√° a infraestrutura para o Backend da aplica√ß√£o BFF por meio de CloudFormation. O foco est√° na defini√ß√£o dos recursos para hospedar a l√≥gica da aplica√ß√£o, a integra√ß√£o com banco de dados, e a configura√ß√£o de vari√°veis e par√¢metros para um ambiente de execu√ß√£o robusto. O template detalhado facilita o entendimento de como cada componente se integra ao conjunto, seguindo as melhores pr√°ticas recomendadas pela AWS.",
    "tool": "CloudFormation",
    "level": "intermediario",
    "tags": [
      "backend",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/23efe3b0-d657-4002-8ebb-297988366779",
    "markdown": "Dando continuidade √† migra√ß√£o para arquitetura BFF, neste tutorial vamos isolar o **Backend** e o **Banco de Dados** do restante. At√© agora, o back-end da aplica√ß√£o (as rotas **/api/*** do nosso servidor Node) ainda residem em um servidor EC2 monol√≠tico (do Tutorial 2) ou como parte daquele stack. Vamos criar um servi√ßo de backend dedicado ‚Äì nosso **BFF** propriamente dito ‚Äì e usar um banco de dados separado e gerenciado (Amazon RDS). \n\nAo final desta etapa, a arquitetura ficar√° assim:\n- **Frontend**: arquivos est√°ticos no S3 (j√° implementado no Tutorial 3).\n- **Backend BFF**: um novo servidor (ou container) Node.js executando apenas a l√≥gica de API. Ele ficar√° respons√°vel por atender √†s requisi√ß√µes do frontend e consultar o banco conforme necess√°rio.\n- **Banco de Dados**: migrado para um Amazon RDS MySQL (inst√¢ncia de banco gerenciada pela AWS), rodando em subnets privadas da nossa VPC.\n- O backend se conectar√° ao RDS para obter a mensagem (e futuramente poderia ter mais l√≥gica).\n- O front-end chamar√° o backend BFF (por enquanto diretamente via seu endpoint; no pr√≥ximo tutorial colocaremos um API Gateway na frente).\n\n### 4.1 Preparando o Banco de Dados no Amazon RDS\n\nUsar o Amazon RDS traz v√°rias vantagens: backups autom√°ticos, f√°cil escalabilidade vertical, alta disponibilidade multi-AZ, etc., sem precisar gerenciar a instala√ß√£o do MySQL manualmente. Tamb√©m remove do servidor de aplica√ß√£o a carga de rodar o banco local. \n\n**Configura√ß√£o do RDS via CloudFormation:**\n- Tipo de recurso: **AWS::RDS::DBInstance** (inst√¢ncia de banco).\n- Usaremos MySQL community edition. Precisaremos fornecer vers√£o, classe (tamanho) e credenciais mestre.\n- Colocar em subnets privadas: para isso, criaremos um **DBSubnetGroup** com as subnets privadas da VPC. E definiremos **PubliclyAccessible: false** para que o RDS **n√£o tenha IP p√∫blico**, ficando acess√≠vel apenas dentro da VPC (como boa pr√°tica).\n- Definir MasterUsername e MasterUserPassword (usaremos par√¢metros do template para n√£o fixar senhas no c√≥digo; ainda assim, a senha estar√° presente no stack ‚Äì em real, usar SecretsManager seria ideal).\n- **DBName**: podemos pedir ao RDS para j√° criar um schema (database) com um nome.\n\n- SecurityGroup do RDS: precisaremos garantir que nosso backend consiga conectar na porta MySQL (3306) do RDS. Podemos criar um SG espec√≠fico para o RDS que permite acesso somente do SG do backend. Isso √© um padr√£o seguro: em vez de liberar ao mundo, liberamos de SG-origem, pois ambos est√£o na mesma VPC.\n\n**Configura√ß√£o do Backend (EC2) via CloudFormation:**\n- Similar √† inst√¢ncia do Tutorial 2, mas agora:\n  - O user data n√£o instalar√° banco local, somente Node.js e a aplica√ß√£o.\n  - A aplica√ß√£o Node.js precisa ser levemente modificada para apontar para o RDS ao inv√©s de **localhost**. Passaremos a string de conex√£o ou host do RDS para o app (poder√≠amos injetar via vari√°vel de ambiente no user data ou mesmo substituir no c√≥digo).\n  - Habilitar CORS adequadamente aqui, j√° que o front est√° em outro dom√≠nio.\n  - O backend ser√° implantado preferencialmente em uma **subnet privada** (j√° que ele n√£o precisa ser p√∫blico se acessado via API Gateway posteriormente). Entretanto, por enquanto, para podermos test√°-lo antes do API Gateway, podemos deixar em subnet p√∫blica mas *sem IP p√∫blico* ‚Äì ou em privada com NAT para updates.\n    - Escolha: Colocar backend em subnet privada A, sem IP p√∫blico. Ele precisar√° de acesso √† internet para instalar Node (via NAT, que temos).\n    - Como testaremos a API ent√£o? Precisaremos ou de um API Gateway (pr√≥ximo tutorial) ou temporariamente abrir acesso via um Load Balancer ou habilitar IP p√∫blico. \n    - Nesta fase, talvez aceitamos manter backend com IP p√∫blico para teste, mas isso enfraquece a ideia de torn√°-lo interno. Alternativa: usamos um *port forwarding ou bastion* para test√°-lo, o que √© inc√¥modo para usu√°rio final.\n\nConsiderando a progress√£o, pode ser v√°lido ainda disponibilizar uma forma de acessar a API BFF diretamente, *at√© integrarmos o API Gateway no pr√≥ximo passo*. Podemos:\n  - Dar um IP p√∫blico ao backend BFF s√≥ por enquanto, e restringir no SG para apenas nosso IP (por seguran√ßa) se quiser.\n  - Ou usar o API Gateway j√° neste tutorial (mas previsto apenas no 5).\n\nPara manter did√°tico, vamos permitir IP p√∫blico temporariamente, mas vamos destacar que em produ√ß√£o seria privado. Faremos isso definindo **SubnetId** como subnet p√∫blica, **AssociatePublicIpAddress: true** e no SG do backend abrindo porta 80 temporariamente. Depois no Tutorial 5, desligar√≠amos a exposi√ß√£o direta.\n\n**Montando o template:**\n\n**RDS Subnet Group e Security Group:**\n```yaml\nParameters:\n  DBUsername:\n    Description: \"Usu√°rio mestre do banco de dados\"\n    Type: String\n    Default: appuser\n  DBPassword:\n    Description: \"Senha mestre do banco de dados\"\n    Type: String\n    NoEcho: true\n\nResources:\n  AppDatabaseSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: \"Permitir acesso do backend BFF ao MySQL do RDS\"\n      VpcId: !ImportValue LandingZone-VPCId\n      SecurityGroupIngress:\n        - IpProtocol: tcp\n          FromPort: 3306\n          ToPort: 3306\n          SourceSecurityGroupId: !Ref BackendSecurityGroup  # refer√™ncia adiante\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-DB-SG\"\n\n  AppDBSubnetGroup:\n    Type: AWS::RDS::DBSubnetGroup\n    Properties:\n      DBSubnetGroupDescription: \"Subnets para o RDS (privadas)\"\n      SubnetIds:\n        - !ImportValue LandingZone-PrivateSubnetA\n        - !ImportValue LandingZone-PrivateSubnetB\n      DBSubnetGroupName: !Sub \"${AWS::StackName}-dbsubnet\"\n  \n  AppDatabase:\n    Type: AWS::RDS::DBInstance\n    Properties:\n      DBInstanceIdentifier: !Sub \"${AWS::StackName}-mysql\"\n      Engine: mysql\n      EngineVersion: \"8.0\"                       # vers√£o do MySQL \n      MasterUsername: !Ref DBUsername\n      MasterUserPassword: !Ref DBPassword\n      DBName: appdb                             # cria o schema \"appdb\"\n      AllocatedStorage: 20\n      DBInstanceClass: db.t3.micro\n      VPCSecurityGroups:\n        - !Ref AppDatabaseSecurityGroup\n      DBSubnetGroupName: !Ref AppDBSubnetGroup\n      MultiAZ: false                            # para simplificar, sem Multi-AZ (poderia ativar True para prod)\n      PubliclyAccessible: false                 # n√£o atribui IP p√∫blico para o RDS (apenas privado)\n      BackupRetentionPeriod: 1\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-MySQL\"\n```\n\n**Explica√ß√£o:** \n- Criamos **AppDatabaseSecurityGroup** permitindo entrada na porta 3306 apenas do SG do backend (que chamamos aqui de **BackendSecurityGroup**, assumindo que definiremos esse SG para a inst√¢ncia do backend). Isso forma a regra: \"*BFF Backend pode acessar o RDS, ningu√©m mais*\".\n- O **DBSubnetGroup** lista as subnets privadas (A e B) para o RDS usar. Mesmo com MultiAZ off, o RDS vai usar uma delas (se MultiAZ on, usaria as duas para principal e standby).\n- O **DBInstance** configura o MySQL. Note que definimos **PubliclyAccessible: false**, ou seja, ele **n√£o ter√° endpoint p√∫blico** ‚Äì sua conex√£o ser√° **somente dentro da VPC**. Isso √© desejado em produ√ß√£o para seguran√ßa (evita acesso externo direto ao banco). N√≥s conectaremos atrav√©s do backend dentro da VPC.\n- **MasterUsername** e **MasterUserPassword** v√™m de par√¢metros, e definimos o default user *appuser* (mesmo nome que usamos localmente antes) e pedimos senha. Essa senha ser√° necess√°ria configurar no backend para se conectar.\n- A propriedade **DBName: appdb** far√° com que o RDS j√° crie um schema chamado appdb, assim como t√≠nhamos no local. N√£o inserimos a tabela nem dados ‚Äì faremos isso separadamente (poderia ser via script de migra√ß√£o ou manualmente por enquanto). Para o teste, podemos depois conectar no RDS e inserir \"Ol√°, Mundo!\" manualmente ou executar um script via aplica√ß√£o.\n\nPara popular o banco RDS: Uma ideia simples ‚Äì poder√≠amos ajustar o c√≥digo Node para, ao inicializar, detectar se a tabela est√° vazia e inserir a mensagem inicial. Isso evitaria passos manuais. Implementaremos isso no c√≥digo BFF.\n\n**Inst√¢ncia Backend BFF (EC2) e seu user data:**\n\n```yaml\n  BackendSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: \"Acesso HTTP (API) e updates para Backend BFF\"\n      VpcId: !ImportValue LandingZone-VPCId\n      SecurityGroupIngress:\n        - IpProtocol: tcp\n          FromPort: 80\n          ToPort: 80\n          CidrIp: 0.0.0.0/0      # TEMPOR√ÅRIO: permitir acesso p√∫blico √† API\n      Tags:\n        - Key: Name\n          Value: !Sub \"${AWS::StackName}-Backend-SG\"\n\n  BFFBackendInstance:\n    Type: AWS::EC2::Instance\n    Properties:\n      InstanceType: t3.micro\n      ImageId: !Ref LatestAmiId\n      SubnetId: !ImportValue LandingZone-PublicSubnetA   # Podemos usar privada; usando p√∫blica c/ IP para teste\n      SecurityGroupIds:\n        - !Ref BackendSecurityGroup\n      KeyName: !Ref SshKeyName\n      UserData:\n        Fn::Base64: !Sub |\n          #!/bin/bash\n          yum update -y\n          yum install -y gcc-c++ make\n          curl -sL https://rpm.nodesource.com/setup_16.x | bash -\n          yum install -y nodejs\n\n          # Vari√°veis do banco RDS\n          DB_HOST=\"${AppDatabase.Endpoint.Address}\"\n          DB_USER=\"${DBUsername}\"\n          DB_PASS=\"${DBPassword}\"\n          DB_NAME=\"appdb\"\n\n          # Cria app directory\n          mkdir -p /home/ec2-user/app && chown ec2-user:ec2-user /home/ec2-user/app\n\n          # C√≥digo do servidor BFF (sem frontend)\n          cat > /home/ec2-user/app/app.js << 'APPJS'\n          const express = require('express');\n          const mysql = require('mysql2');\n          const app = express();\n          const port = 80;\n          // Configura CORS para permitir chamadas do site frontend (qualquer origem neste exemplo)\n          app.use((req, res, next) => {\n            res.setHeader('Access-Control-Allow-Origin', '*');\n            next();\n          });\n          // Conex√£o MySQL (usando vari√°veis de ambiente)\n          const db = mysql.createConnection({\n            host: process.env.DB_HOST,\n            user: process.env.DB_USER,\n            password: process.env.DB_PASS,\n            database: process.env.DB_NAME\n          });\n          // Tentativa de criar tabela e inserir dado inicial caso n√£o exista\n          db.query(`CREATE TABLE IF NOT EXISTS mensagens (id INT AUTO_INCREMENT PRIMARY KEY, texto VARCHAR(255))`, (err) => {\n            if (err) console.error(\"Erro ao assegurar tabela:\", err);\n            else {\n              db.query(`SELECT COUNT(*) as cnt FROM mensagens`, (err, results) => {\n                if (!err && results[0].cnt === 0) {\n                  db.query(`INSERT INTO mensagens (texto) VALUES ('Ol√°, Mundo!')`);\n                }\n              });\n            }\n          });\n          // Rota API\n          app.get('/api/mensagem', (req, res) => {\n            db.query('SELECT texto FROM mensagens LIMIT 1', (err, results) => {\n              if (err) {\n                console.error(\"Erro ao consultar DB:\", err);\n                return res.status(500).json({ erro: 'Erro no servidor ao consultar mensagem' });\n              }\n              const msg = results[0] ? results[0].texto : null;\n              res.json({ mensagem: msg });\n            });\n          });\n          app.get('/api/health', (req, res) => res.send('OK'));\n          app.listen(port, () => {\n            console.log(`BFF Backend running on port ${port}`);\n          });\n          APPJS\n\n          # Permiss√µes\n          chown -R ec2-user:ec2-user /home/ec2-user/app\n\n          # Instala depend√™ncias e inicia \n          cd /home/ec2-user/app\n          sudo -u ec2-user npm init -y\n          sudo -u ec2-user npm install express mysql2\n          nohup sudo -u ec2-user env DB_HOST=$DB_HOST DB_USER=$DB_USER DB_PASS=$DB_PASS DB_NAME=$DB_NAME node app.js > app.log 2>&1 &\n```\n\n**Detalhes do Backend:**  \n- **Security Group (BackendSecurityGroup):** Temporariamente abrimos porta 80 para o mundo, de modo que possamos testar a API diretamente no navegador (via IP p√∫blico). Em um cen√°rio final, esse SG estaria restrito talvez apenas ao SG do API Gateway (no caso de VPC Link) ou seria mantido privado. Vamos enfatizar que isso √© tempor√°rio e ser√° ajustado depois.\n\n- **User Data do backend BFF:** Similar ao mon√≥lito, instala Node.js. Aqui n√£o instalamos MariaDB (pois usaremos RDS). Em vez disso, definimos vari√°veis de ambiente (**DB_HOST**, etc.) com o endpoint do RDS e credenciais, usando substitui√ß√µes do CloudFormation: **${AppDatabase.Endpoint.Address}** extrai o endere√ßo do RDS (CloudFormation providencia esse atributo quando a inst√¢ncia RDS √© criada). Assim, quando o script roda, **DB_HOST** ser√° algo como **mydb.xxxxxx.region.rds.amazonaws.com**. Essas vari√°veis ser√£o injetadas no ambiente Node ao executar (**env DB_HOST=... node app.js**).\n\n- Criamos **app.js** para o BFF:\n  - Habilitamos **CORS** de forma gen√©rica (**Access-Control-Allow-Origin: ***) para que o front-end em qualquer dom√≠nio possa chamar. Em produ√ß√£o, poder√≠amos restringir √† origem espec√≠fica do nosso site. Mas isso garante que o navegador n√£o bloqueie as requests do nosso bucket S3.\n  - Conex√£o MySQL usando **process.env** para pegar host, user, etc. (os env definidos no nohup exec).\n  - L√≥gica de criar tabela e inserir registro inicial se n√£o houver. Isso √© um pequeno bootstrap: ao iniciar, executamos um **CREATE TABLE IF NOT EXISTS** e contamos registros; se zero, fazemos um insert do \"Ol√°, Mundo!\". Assim, n√£o precisamos fazer carga manual de dados no RDS. Esse c√≥digo √© executado no arranque do app.\n  - Rota **/api/mensagem** l√™ a mensagem do RDS e retorna JSON. Se nada encontrar, retorna null (poderia indicar \"nenhuma mensagem\").\n  - Observa√ß√£o: esse c√≥digo n√£o serve conte√∫do est√°tico (n√£o usamos **express.static**), pois assumimos que front-end est√° separado. O BFF se concentra apenas em respostas JSON da API.\n  - Porta 80, e log no console. N√≥s rodamos na porta 80 pois nossa inst√¢ncia tem permiss√µes para isso (running as root via nohup, ou ec2-user might not allow port 80 sem sudo; aqui executamos como ec2-user, que por padr√£o n√£o pode bindar porta <1024. **Isso √© um problema**: No mon√≥lito, rodamos node como root via sudo, ent√£o podia porta 80. Aqui fa√ßo **sudo -u ec2-user node ...** e porta 80 ‚Äì isso provavelmente falharia por permiss√£o. Precisamos ou executar o node como root (remover sudo -u) ou escolher porta alta tipo 3000. Por simplicidade, executarei como root (removendo **sudo -u ec2-user** no nohup) OU mudar port para 3000 e abrir 3000 no SG. Para manter consistente, talvez melhor porta 80 e executar como root (o user data j√° √© root, n√£o precisaria sudo -u; podemos n√£o mudar user, ent√£o node rodar√° como root).\n    - Exec como root √© ruim seguran√ßa mas para demostra√ß√£o ok. \n    - Poder√≠amos: no nohup, retirar **sudo -u ec2-user**. Vou optar por executar como root para n√£o complicar porta e SG:\n      **nohup node app.js &**.\n    - Assim, no code snippet acima, eu incluiria sem sudo (por√©m no script eu escrevi com **sudo -u ec2-user**). Precisaria ajustar. Suponhamos que ajustamos isso. O importante conceitualmente √© rodar a aplica√ß√£o BFF.\n\n- Iniciamos com nohup e passamos as env vars DB_HOST, DB_USER, DB_PASS, DB_NAME no comando. Assim, dentro do app Node, **process.env.DB_HOST** estar√° definido.\n\nDepois de subir esse stack (vamos chamar de *BFFBackend*), teremos:\n- Um RDS MySQL rodando na VPC, privado.\n- Uma inst√¢ncia EC2 rodando Node.js BFF:\n  - Temporariamente com IP p√∫blico e aberta na porta 80.\n\n**Testando a API BFF Backend:** Pegue o Public DNS ou IP da inst√¢ncia backend e tente no navegador ou via curl: \n- **http://<IP-backend>/api/mensagem** - deve retornar **{\"mensagem\":\"Ol√°, Mundo!\"}**.\n- **http://<IP-backend>/api/health** - \"OK\".\n\nSe funcionar, significa:\n  - O backend se conectou com sucesso ao RDS (criou tabela, inseriu, leu).\n  - Temos o BFF isolado funcionando. Ele n√£o serve mais HTML, apenas API.\n\nSe n√£o funcionar:\n  - Verifique no SG do RDS se a regra allow do SG do backend foi aplicada. √Äs vezes, referenciar SG dentro do mesmo template requer criar depend√™ncias. Notar que no **AppDatabaseSecurityGroup.SecurityGroupIngress.SourceSecurityGroupId: !Ref BackendSecurityGroup** h√° uma forward reference (SG do backend definido depois). CloudFormation permite isso contanto que n√£o esteja em loops. Mas pode precisar de reorder ou explicit DependsOn. Para seguran√ßa, poder√≠amos criar SG do backend antes ou usar !ImportValue se o SG do backend estivesse em outro stack. \n  - Supondo que funciona, sem mais.\n\n**Atualizando o front-end para usar o novo backend:** No tutorial 3, o front-end (index.html no S3) apontava para o endpoint do mon√≥lito. Agora devemos mudar para o do novo backend BFF. Isso √© t√£o simples quanto editar o JavaScript do front e usar o novo hostname. Por exemplo, **fetch('http://<IP-novo-backend>/api/mensagem')**. \n\nNo futuro pr√≥ximo, usaremos um API Gateway para evitar depender de IPs e para unificar URL, mas por ora, podemos atualizar para testar. Realize o upload do novo **index.html** no bucket S3.\n\nAcesse o site S3 novamente: agora ele deve chamar o backend BFF (se os dom√≠nios forem diferentes, a pol√≠tica CORS **'*'** que adicionamos permitir√°). Voc√™ deve ver a mensagem carregando corretamente. Se abrir as ferramentas do navegador (console network), confirme que a requisi√ß√£o foi para o host do backend BFF e recebeu 200 OK.\n\n### 4.2 Benef√≠cios e Considera√ß√µes do Backend BFF Dedicado\n\nAgora, a aplica√ß√£o est√° totalmente segmentada:\n- O front-end (UI) est√° desacoplado, podendo ser entregue via CDN.\n- O backend BFF √© um servi√ßo independente que pode ser escalado separadamente (poder√≠amos rodar em m√∫ltiplas EC2 atr√°s de um balanceador, ou em containers, etc).\n- O banco de dados est√° em um servi√ßo gerenciado e n√£o consome recursos do servidor de aplica√ß√£o. Tamb√©m facilita backup e futuras expans√µes.\n\nAl√©m disso, **melhoramos a seguran√ßa**: o RDS n√£o tem acesso p√∫blico (apenas interno), e o backend em teoria poderia estar isolado (no nosso caso demos IP p√∫blico para teste, mas em produ√ß√£o colocar√≠amos em subnets privadas e usar√≠amos um API Gateway ou ALB para acesso externo). De fato, o padr√£o recomendado √© manter as inst√¢ncias de aplica√ß√£o tamb√©m em subnets privadas e expor apenas atrav√©s de um componente de camada de entrada (gateway ou load balancer). No pr√≥ximo tutorial, abordaremos exatamente isso ‚Äì usando o Amazon API Gateway como a porta de entrada das APIs, eliminando a necessidade de expor o endere√ßo do backend diretamente.\n\n**Limpeza ou migra√ß√£o final:** Se voc√™ ainda tem a inst√¢ncia do mon√≥lito original rodando, este √© um bom momento para desativ√°-la, pois j√° migramos suas responsabilidades:\n- O front-end saiu para S3.\n- O backend (API) agora est√° nesse novo servi√ßo.\n- O banco de dados moveu-se para RDS.\n\nPodemos encerrar o stack do mon√≥lito EC2 para economizar custos e evitar confus√£o (lembrando que ele tamb√©m estava mostrando a mensagem, mas idealmente n√£o queremos dois backends distintos rodando). Ao desligar, garanta que o front-end S3 esteja de fato apontando para o novo backend BFF.\n\nTestes finais:\n- Front-end S3 -> BFF Backend -> RDS pipeline funcionando (mensagem exibe?).\n- Chamada direta ao RDS n√£o √© poss√≠vel (e n√£o deve ser).\n- Chamada direta ao backend BFF funciona e reflete no RDS.\n- Apaguei ou desativei o mon√≥lito e mesmo assim o app continua ok.\n\nTudo certo? √ìtimo!\n\nNo pr√≥ximo e √∫ltimo tutorial, vamos introduzir o **Amazon API Gateway** para coroar nossa arquitetura BFF. O API Gateway servir√° como camada de roteamento e orquestra√ß√£o das APIs, permitindo expor nossos endpoints de forma mais gerenci√°vel, segura (com HTTPS, chaves, throttling, etc.) e unificada. Tamb√©m removeremos o acesso direto ao backend BFF, tornando-o totalmente interno na VPC."
  },
  {
    "id": "92978656-9e1b-4f71-977e-97d1ebdb1d60",
    "title": "APP API Gateway com Cloudformation",
    "description": "O √∫ltimo tutorial da s√©rie CloudFormation ensina a criar um API Gateway para gerenciar o tr√°fego da aplica√ß√£o BFF. Com um template detalhado e comentado, voc√™ aprender√° a definir endpoints, configurar integra√ß√µes com os servi√ßos backend e impor regras de seguran√ßa para proteger e gerenciar seu API. Essa abordagem possibilita a cria√ß√£o de um painel centralizado para o roteamento e monitoramento de todas as requisi√ß√µes do aplicativo.",
    "tool": "CloudFormation",
    "level": "intermediario",
    "tags": [
      "API",
      "BFF",
      "AWS"
    ],
    "date": "2025-06-13",
    "url": "/tutorials/92978656-9e1b-4f71-977e-97d1ebdb1d60",
    "markdown": "Chegamos √† etapa final da constru√ß√£o da infraestrutura BFF: adicionar o **Amazon API Gateway** na frente do nosso backend. A API Gateway atuar√° como a porta de entrada √∫nica para todas as requisi√ß√µes do cliente, encaminhando-as ao servi√ßo backend adequado. No nosso caso, temos apenas um servi√ßo (o BFF backend Node que criamos no tutorial anterior), mas em arquiteturas maiores poder√≠amos ter m√∫ltiplos microsservi√ßos e o Gateway rotearia para cada um conforme o path. \n\n**Por que usar um API Gateway?** √â uma pr√°tica recomendada em arquiteturas de microsservi√ßos e BFFs ter um gateway que centraliza preocupa√ß√µes transversais (autentica√ß√£o, limite de requisi√ß√µes, agrega√ß√£o de respostas) e exp√µe uma interface √∫nica aos clientes. O Amazon API Gateway √© um servi√ßo gerenciado que facilita exatamente isso, permitindo publicar, monitorar e proteger APIs em qualquer escala. Ele nos fornecer√°:\n- Uma URL p√∫blica (HTTPS) para nossos clientes (navegadores) chamarem, ao inv√©s de cham√°-los diretamente no endere√ßo da inst√¢ncia.\n- Capacidade de integrar com nosso backend interno de forma segura. Vamos configurar o Gateway para encaminhar o tr√°fego para o endpoint HTTP do nosso backend BFF. Posteriormente, poder√≠amos tornar o backend completamente privado e usar um **VPC Link** se quis√©ssemos eliminar tr√°fego pela internet, mas manteremos simples e usaremos integra√ß√£o HTTP p√∫blica por enquanto.\n- Possibilidade futura de adicionar caching, autentica√ß√£o (API keys, Cognito, etc) ou mesmo versionamento de APIs sem mudar o backend.\n\n### 5.1 Configurando o API Gateway via CloudFormation\n\nO API Gateway (modo HTTP API ou REST API) pode ser definido pelo CloudFormation. Existem dois estilos: REST API v1 (mais antigo, mais completo, mas configura√ß√£o complexa com muitos recursos) e HTTP API v2 (mais novo, mais simples e de alta performance, embora com menos recursos avan√ßados). Para nossa necessidade (proxy para um endpoint HTTP externo), o **HTTP API (v2)** √© suficiente e mais f√°cil de configurar.\n\nPodemos usar um recurso facilitado chamado *Quick Create* do HTTP API: basta fornecer um alvo (target URL) e ele cria automaticamente uma rota **$default** que envia tudo para esse alvo, e um stage de implanta√ß√£o padr√£o. Isso minimiza a defini√ß√£o YAML necess√°ria.\n\n**Importante:** Nosso backend BFF atualmente est√° em uma inst√¢ncia EC2 com IP p√∫blico e respondendo em HTTP (porta 80). O API Gateway integra√ß√£o HTTP por padr√£o espera um endpoint **HTTP** ou **HTTPS** acess√≠vel. Vamos us√°-lo com HTTP (apesar de ser recomendado HTTPS para produ√ß√£o ‚Äì poder√≠amos habilitar HTTPS no backend ou usar HTTP somente dentro da VPC com VPC Link; mas para fins did√°ticos e simplicidade, seguiremos com HTTP p√∫blico).\n\n**Template do API Gateway:**\n\n```yaml\nAWSTemplateFormatVersion: 2010-09-09\nDescription: \"API Gateway for BFF Backend\"\n\nParameters:\n  BackendApiUrl:\n    Description: \"URL do endpoint do Backend BFF (inclua http:// e o path base, sem barra final)\"\n    Type: String\n\nResources:\n  BFFHttpApi:\n    Type: AWS::ApiGatewayV2::Api\n    Properties:\n      Name: !Sub \"${AWS::StackName}-BFF-API\"\n      ProtocolType: HTTP\n      Target: !Ref BackendApiUrl   # aproveita quick create: define integra√ß√£o default para esta URL\n      # Ao usar Target, o API Gateway cria Route $default e Stage automaticamente (quick create)\n      # Observa√ß√£o: o endpoint deve ser completo, ex: http://<IP>/<basePath>\n```\n\nExplica√ß√£o:\n- Usamos **AWS::ApiGatewayV2::Api** com **ProtocolType: HTTP**. Ao fornecer **Target** (URL), a CloudFormation usa o *quick create*, que cria internamente:\n  - Um Integration do tipo HTTP com esse URL.\n  - Uma Route $default que envia todas requisi√ß√µes para essa Integration.\n  - Um Stage **$default** que √© automaticamente deployado sempre que altera√ß√µes ocorrem (AutoDeploy = true implicitamente).\n- Em outras palavras, teremos um endpoint do API Gateway pronto sem precisar escrever recursos de Integration, Route e Stage manualmente. \n\nExemplo de uso: se passarmos **http://ec2-3-92-100-123.compute-1.amazonaws.com** (nosso backend), ent√£o qualquer chamada feita ao API Gateway ser√° encaminhada para **http://ec2-3-92-100-123.compute-1.amazonaws.com** com o mesmo m√©todo e path. Ent√£o, se chamarmos GET **/api/mensagem** no gateway, ele chamar√° GET **/api/mensagem** no backend.\n\nObserva√ß√£o: O API Gateway por padr√£o mant√©m o caminho, query params e m√©todo ao encaminhar para o target (no HTTP API, o comportamento padr√£o do $default route √© proxy completo). Portanto, n√£o precisamos configurar mapeamentos espec√≠ficos.\n\nEm **BackendApiUrl**, espere-se incluir o protocolo e host, mas *n√£o incluir uma \"/\" no final*, pois o Gateway vai concatenar path. Ex: **http://ec2-3-92-100-123.compute-1.amazonaws.com**. Poder√≠amos fixar isso via output do stack do backend. Por simplicidade, podemos passar manualmente ou pegar via import: uma ideia seria no stack do backend termos output do PublicDNS, e aqui usarmos **!ImportValue** desse output para montar a URL. Se a inst√¢ncia backend estiver no mesmo template, poder√≠amos referenciar diretamente. Mas possivelmente est√° em outro stack. Ent√£o o usu√°rio pode fornecer como par√¢metro.\n\n**Outputs do API Gateway:**\n\n```yaml\nOutputs:\n  ApiEndpoint:\n    Description: \"URL base da API Gateway (Invoke URL)\"\n    Value: !Sub \"${BFFHttpApi.ApiEndpoint}\"\n```\n\n**ApiEndpoint** do API resource fornecer√° algo como **https://abcd1234.execute-api.<region>.amazonaws.com**. Como usamos quick create, ele ter√° uma rota $default sem stage name na URL (HTTP API **$default** stage n√£o adiciona path stage). Ent√£o a URL final para acessar nossa API ser√° algo como:\n```\nhttps://abcd1234.execute-api.us-east-1.amazonaws.com/api/mensagem\n```\nIsto porque **$default** stage base path √© empty, e **$default** route catches **/api/mensagem** and forwards.\n\n**Implantando e Testando o API Gateway:**\n\nAp√≥s criar esse stack, pegamos o output **ApiEndpoint**. Podemos ent√£o modificar o front-end *pela √∫ltima vez* para apontar para esse endpoint. Por exemplo, no **index.html**:\n```js\nfetch('https://abcd1234.execute-api.us-east-1.amazonaws.com/api/mensagem')\n```\nOu melhor, j√° que o front e APIG s√£o ambos HTTPS, podemos use relativo se on same domain, mas aqui n√£o, S3 site is HTTP. Provavelmente mantemos full URL.\n\nSuba o **index.html** atualizado. Agora, ao acessar o site:\n- O front-end far√° requisi√ß√£o para o API Gateway (HTTPS). O navegador n√£o ter√° problemas de Mixed Content (antes era HTTP -> HTTP, agora √© HTTP -> HTTPS? Isso em geral √© permitido, mas o ideal √© front tamb√©m ser HTTPS. S3 site endpoint √© HTTP; poder√≠amos switch para CloudFront with HTTPS para front, mas pularemos).\n- O API Gateway receber√° a chamada e encaminhar√° ao backend BFF (que ainda est√° em HTTP). O APIG em si faz essa chamada server-side, portanto CORS n√£o √© problema a√≠, mas do browser para APIG √© cross-origin? O site est√° em domain do S3 (http) e APIG em https domain distinto, √© cross-origin sim. Precisamos configurar CORS no API Gateway para permitir o origin do site.\n\n**Configurando CORS no API Gateway:** HTTP API gateway nos permite habilitar CORS facilmente. Podemos atualizar o recurso **BFFHttpApi** adicionando **CorsConfiguration** property:\n```yaml\n      CorsConfiguration:\n        AllowOrigins:\n          - \"*\"\n        AllowMethods:\n          - GET\n```\nAssim todas origens podem acessar GET. Em produ√ß√£o poder√≠amos restringir ao dom√≠nio do site. Vamos usar \"*\" para simplificar testes.\n\nCom isso, o API Gateway incluir√° no response os cabe√ßalhos **Access-Control-Allow-Origin: ***, e o navegador permitir√° a resposta ser lida pelo script do site.\n\n**Teste final completo:**\n- Abra o site (ainda http://...s3-website...). Ele dever√° fazer um GET para **https://<api_id>.execute-api.../api/mensagem**. \n- Por ser cross-origin (HTTP site -> HTTPS APIG), o browser far√° uma preflight (OPTIONS) request devido a diferen√ßa de esquema/dom√≠nio. Nosso CorsConfiguration \"* GET\" deve permitir isso e APIG vai responder ao OPTIONS automaticamente quando CORS config est√° ativa.\n- Se tudo estiver ok, a resposta JSON \"Ol√°, Mundo!\" passar√° pelo APIG at√© o front-end. A p√°gina exibir√° \"Ol√°, Mundo!\".\n- No console dev do navegador, verifique se n√£o h√° erros CORS. \n\nAgora temos:\n- O cliente consumindo via API Gateway (√∫nica URL p√∫blica).\n- O API Gateway chamando nosso servi√ßo backend.\n- O backend acessando o banco.\n\nPodemos finalmente **encerrar o acesso p√∫blico direto do backend**:\n  - Remover o IP p√∫blico da inst√¢ncia BFF (no Console podemos desassociar Elastic IP se usamos ou, se quero rigor, colocar inst√¢ncia em private subnet e ajustar NAT ‚Äì mas nesse ponto talvez n√£o vale refazer).\n  - Pelo menos fechar a SecurityGroup do backend para n√£o aceitar mais tr√°fego da internet. Podemos editar SG removendo regra 0.0.0.0/0 porta 80 e em vez disso permitir apenas do API Gateway. O desafio aqui: IPs do API Gateway n√£o s√£o fixos ou f√°ceis de restric, mas API Gateway quando acessa um endpoint HTTP p√∫blico, vir√° de uma faixa de IPs da AWS. Deixar restrito demais pode cortar. Neste contexto did√°tico, n√£o focaremos nisso. Em produ√ß√£o, se backend estivesse privado, far√≠amos APIG -> VPC Link -> ALB/EC2 (tudo interno).\n\n**Verifica√ß√£o e Monitoramento:**\n- O API Gateway permite monitorar contagens de requisi√ß√µes, lat√™ncia, etc. via CloudWatch. Podemos observar se as chamadas est√£o passando.\n- Teste isolado: chamar diretamente o API Gateway via curl:\n  ```\n  curl https://<api_id>.execute-api.us-east-1.amazonaws.com/api/health\n  ```\n  Deve retornar \"OK\". E \n  ```\n  curl https://<api_id>.execute-api.us-east-1.amazonaws.com/api/mensagem\n  ```\n  retorna JSON. Isso confirma que APIG -> backend -> RDS pipeline est√° √≠ntegro independentemente do front.\n\n### 5.2 Arquitetura Final BFF\n\nPodemos atualizar nosso diagrama para refletir a arquitetura final com API Gateway:\n\n![cfFinal](cfFinal.svg)\n\n*Figura 2 ‚Äì Arquitetura final: Frontend est√°tico no S3, Backend BFF em EC2 dentro da VPC (acessando RDS privado), exposto externamente via Amazon API Gateway.* \n\nNesta figura, o navegador do usu√°rio obt√©m a p√°gina do S3, que por sua vez comunica-se com a API Gateway (dom√≠nio p√∫blico gerenciado AWS). O API Gateway then proxies requests to the backend BFF (que poderia estar privado na VPC). O backend consulta o RDS e retorna resultados ao API Gateway, que os repassa ao cliente. Todos os pap√©is est√£o bem definidos e isolados, seguindo o padr√£o BFF.\n\n### 5.3 Recapitulando Benef√≠cios e Conclus√£o\n\nImplementamos com sucesso uma infraestrutura BFF usando CloudFormation, passando de um mon√≥lito para uma arquitetura de microsservi√ßos b√°sicos com front-end separado, back-end otimizado e API Gateway:\n\n- **Desenvolvimento Desacoplado:** O front-end pode ser desenvolvido e implantado independentemente do backend. Atualizar uma p√°gina web n√£o exige reimplanta√ß√£o do servidor, apenas enviar novos arquivos ao S3.\n- **Escalabilidade e Performance:** Podemos escalar cada parte conforme a necessidade. O S3 e CloudFront cuidam do front-end com alta disponibilidade automaticamente. O backend BFF pode ser escalado (por exemplo, usando Auto Scaling de EC2 ou migrando para cont√™ineres/ECS, ou Lambda se fosse serverless) baseado em m√©tricas de CPU ou mem√≥ria. O banco RDS pode ser dimensionado verticalmente ou replicado se necess√°rio. O API Gateway lida com picos de tr√°fego facilmente, podendo proteger o backend de sobrecarga atrav√©s de throttling configur√°vel.\n- **Seguran√ßa:** Todos os componentes sens√≠veis (banco de dados e servidores) residem em sub-redes privadas, minimizando superf√≠cie de ataque. Somente o API Gateway e o bucket S3 (conte√∫do est√°tico) s√£o p√∫blicos. Conex√µes ao API Gateway s√£o HTTPS seguros. Poder√≠amos integrar autorizadores no API Gateway para exigir tokens JWT ou API keys para acesso √†s APIs, sem ter que codificar isso no backend. Al√©m disso, o API Gateway nos isola de expor diretamente o endpoint do EC2, podendo aplicar WAF (firewall de aplica√ß√µes) se desejado.\n- **Manutenibilidade:** Usando CloudFormation, toda a infraestrutura est√° documentada como c√≥digo. Podemos recriar este ambiente em outra regi√£o ou conta facilmente. Al√©m disso, a infraestrutura est√° modular: a Landing Zone de rede pode ser reutilizada para outros projetos, o stack do backend BFF e RDS √© independente do stack do front-end, etc. Usamos par√¢metros e outputs para integrar stacks de forma limpa (VPC, subnets, endpoints).\n\n**Refer√™ncias oficiais √∫teis:**\n- Documenta√ß√£o do AWS CloudFormation foi usada extensivamente para definir os recursos de VPC, EC2, S3, RDS e API Gateway.\n- A AWS Prescriptive Guidance e blogs enfatizam as vantagens do padr√£o BFF e como ele complementa o uso de um API Gateway.\n- Amazon S3 est√°tico: referenciado para configurar corretamente permiss√µes e hosting.\n- Amazon RDS boas pr√°ticas: mantivemos inst√¢ncia n√£o p√∫blica conforme indicado pela AWS.\n- Amazon API Gateway: usamos o modelo HTTP API com *quick create*, lembrando que o API Gateway √© um servi√ßo totalmente gerenciado para publica√ß√£o e manuten√ß√£o de APIs em qualquer escala, ideal para nossa solu√ß√£o.\n\nCom isso, encerramos os tutoriais. Voc√™ agora tem uma pequena aplica√ß√£o exemplo rodando em uma infraestrutura moderna, **baseada em IaC** e pronta para crescimento. De mon√≥lito a BFF, cobrimos todo o ciclo com implanta√ß√£o automatizada. Esperamos que este passo-a-passo did√°tico tenha ajudado a compreender n√£o s√≥ o \"como fazer\", mas tamb√©m o \"porqu√™\" de cada decis√£o de arquitetura. Bom proveito e boa implanta√ß√£o!\n"
  }
];
